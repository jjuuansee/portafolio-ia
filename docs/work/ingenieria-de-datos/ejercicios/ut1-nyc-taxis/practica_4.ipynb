{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac68fe60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup completo para análisis multi-fuentes!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurar visualizaciones\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"Setup completo para análisis multi-fuentes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c33039ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos oficiales de NYC Taxi (dataset completo)...\n",
      "   Viajes cargados: 3,066,766 filas, 19 columnas\n",
      "   Columnas: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n",
      "   Período: 2008-12-31 23:01:42 a 2023-02-01 00:56:53\n",
      "   Tamaño en memoria: 565.6 MB\n",
      "\n",
      "Cargando datos oficiales de zonas NYC...\n",
      "   Zonas cargadas: 265 filas, 4 columnas\n",
      "   Columnas: ['LocationID', 'Borough', 'Zone', 'service_zone']\n",
      "   Boroughs únicos: ['EWR' 'Queens' 'Bronx' 'Manhattan' 'Staten Island' 'Brooklyn' 'Unknown'\n",
      " nan]\n",
      "\n",
      "Cargando datos de calendario de eventos...\n",
      "   Eventos calendario: 3 filas\n",
      "   Columnas: ['date', 'name', 'special']\n",
      "\n",
      "VISTA PREVIA DE DATOS:\n",
      "\n",
      "--- TRIPS ---\n",
      "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  2023-01-01 00:32:10   2023-01-01 00:40:36              1.0   \n",
      "1         2  2023-01-01 00:55:08   2023-01-01 01:01:27              1.0   \n",
      "2         2  2023-01-01 00:25:04   2023-01-01 00:37:49              1.0   \n",
      "3         1  2023-01-01 00:03:48   2023-01-01 00:13:25              0.0   \n",
      "4         2  2023-01-01 00:10:29   2023-01-01 00:21:19              1.0   \n",
      "\n",
      "   trip_distance  RatecodeID store_and_fwd_flag  PULocationID  DOLocationID  \\\n",
      "0           0.97         1.0                  N           161           141   \n",
      "1           1.10         1.0                  N            43           237   \n",
      "2           2.51         1.0                  N            48           238   \n",
      "3           1.90         1.0                  N           138             7   \n",
      "4           1.43         1.0                  N           107            79   \n",
      "\n",
      "   payment_type  fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
      "0             2          9.3   1.00      0.5        0.00           0.0   \n",
      "1             1          7.9   1.00      0.5        4.00           0.0   \n",
      "2             1         14.9   1.00      0.5       15.00           0.0   \n",
      "3             1         12.1   7.25      0.5        0.00           0.0   \n",
      "4             1         11.4   1.00      0.5        3.28           0.0   \n",
      "\n",
      "   improvement_surcharge  total_amount  congestion_surcharge  airport_fee  \n",
      "0                    1.0         14.30                   2.5         0.00  \n",
      "1                    1.0         16.90                   2.5         0.00  \n",
      "2                    1.0         34.90                   2.5         0.00  \n",
      "3                    1.0         20.85                   0.0         1.25  \n",
      "4                    1.0         19.68                   2.5         0.00  \n",
      "\n",
      "--- ZONES ---\n",
      "   LocationID        Borough                     Zone service_zone\n",
      "0           1            EWR           Newark Airport          EWR\n",
      "1           2         Queens              Jamaica Bay    Boro Zone\n",
      "2           3          Bronx  Allerton/Pelham Gardens    Boro Zone\n",
      "3           4      Manhattan            Alphabet City  Yellow Zone\n",
      "4           5  Staten Island            Arden Heights    Boro Zone\n",
      "\n",
      "--- CALENDAR ---\n",
      "         date       name  special\n",
      "0  2022-01-01   New Year     True\n",
      "1  2022-01-03  Event Day     True\n",
      "2  2022-01-05  Promo Day     True\n"
     ]
    }
   ],
   "source": [
    "# === CARGAR DATOS DE MÚLTIPLES FUENTES ===\n",
    "\n",
    "# 1. Cargar datos de viajes desde Parquet (Dataset oficial completo NYC)\n",
    "print(\"Cargando datos oficiales de NYC Taxi (dataset completo)...\")\n",
    "trips_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "\n",
    "# Cargar dataset oficial (~3M registros de enero 2023) # función para leer archivos .parquet (más eficiente que CSV)\n",
    "trips = pd.read_parquet(trips_url, engine=\"fastparquet\")\n",
    "\n",
    "print(f\"   Viajes cargados: {trips.shape[0]:,} filas, {trips.shape[1]} columnas\")\n",
    "print(f\"   Columnas: {list(trips.columns)}\")\n",
    "print(f\"   Período: {trips['tpep_pickup_datetime'].min()} a {trips['tpep_pickup_datetime'].max()}\")\n",
    "print(f\"   Tamaño en memoria: {trips.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# 2. Cargar datos de zonas desde CSV (Dataset oficial completo)\n",
    "print(\"\\nCargando datos oficiales de zonas NYC...\")\n",
    "zones_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n",
    "zones = pd.read_csv(zones_url)  # función estándar para archivos CSV\n",
    "\n",
    "print(f\"   Zonas cargadas: {zones.shape[0]} filas, {zones.shape[1]} columnas\")\n",
    "print(f\"   Columnas: {list(zones.columns)}\")\n",
    "print(f\"   Boroughs únicos: {zones['Borough'].unique()}\")\n",
    "\n",
    "# 3. Cargar calendario de eventos desde JSON \n",
    "print(\"\\nCargando datos de calendario de eventos...\")\n",
    "calendar_url = \"https://juanfkurucz.com/ucu-id/ut1/data/calendar.json\"\n",
    "calendar = pd.read_json(calendar_url)  # función para archivos JSON\n",
    "calendar['date'] = pd.to_datetime(calendar['date']).dt.date  # convertir strings a fechas, luego extraer solo la fecha\n",
    "\n",
    "print(f\"   Eventos calendario: {calendar.shape[0]} filas\")\n",
    "print(f\"   Columnas: {list(calendar.columns)}\")\n",
    "\n",
    "# 4. Mostrar primeras filas de cada dataset\n",
    "print(\"\\nVISTA PREVIA DE DATOS:\")\n",
    "print(\"\\n--- TRIPS ---\")\n",
    "print(trips.head())  # método para mostrar primeras filas de un DataFrame\n",
    "print(\"\\n--- ZONES ---\")\n",
    "print(zones.head())  # mismo método para ver estructura de datos\n",
    "print(\"\\n--- CALENDAR ---\")\n",
    "print(calendar.head())  # revisar formato de los eventos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4263d845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando nombres de columnas...\n",
      "   Trips columnas: ['vendorid', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'ratecodeid', 'store_and_fwd_flag', 'pulocationid', 'dolocationid', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'airport_fee']\n",
      "   Zones columnas: ['locationid', 'borough', 'zone', 'service_zone']\n",
      "   Columna pickup_date creada\n",
      "   Rango de fechas: 2008-12-31 a 2023-02-01\n",
      "\n",
      "VERIFICACIÓN DE TIPOS PARA JOINS:\n",
      "   trips['pulocationid'] tipo: int64\n",
      "   zones['locationid'] tipo: int64\n",
      "   trips['pickup_date'] tipo: <class 'datetime.date'>\n",
      "   calendar['date'] tipo: <class 'datetime.date'>\n",
      "\n",
      "OPTIMIZACIÓN PARA DATASETS GRANDES:\n",
      "   Memoria inicial: 682.6 MB\n",
      "   Optimizando tipos de datos para 3M+ registros...\n",
      "   Limpiando valores nulos antes de optimización...\n",
      "   Registros después de limpieza: 3,066,766\n",
      "   Memoria optimizada: 627.0 MB\n",
      "   Ahorro de memoria: 8.1%\n",
      "\n",
      "DATOS FALTANTES ANTES DE JOINS:\n",
      "Trips (top 5 columnas con más nulos):\n",
      "airport_fee             71743\n",
      "congestion_surcharge    71743\n",
      "store_and_fwd_flag      71743\n",
      "ratecodeid              71743\n",
      "passenger_count             0\n",
      "dtype: int64\n",
      "\n",
      "Zones:\n",
      "locationid      0\n",
      "borough         1\n",
      "zone            1\n",
      "service_zone    2\n",
      "dtype: int64\n",
      "     locationid  borough            zone service_zone\n",
      "263         264  Unknown             NaN          NaN\n",
      "264         265      NaN  Outside of NYC          NaN\n",
      "\n",
      "Calendar:\n",
      "date       0\n",
      "name       0\n",
      "special    0\n",
      "dtype: int64\n",
      "\n",
      "ANÁLISIS DE CALIDAD:\n",
      "   Total de viajes: 3,066,766\n",
      "   Viajes sin pickup location: 0\n",
      "   Viajes sin dropoff location: 0\n",
      "   Viajes sin passenger_count: 0\n",
      "\n",
      "ESTRATEGIAS DE LIMPIEZA:\n",
      "   Ubicaciones nulas: Eliminar (crítico para joins)\n",
      "   Passenger_count nulos: Rellenar con valor típico (1)\n",
      "   Tarifas nulas: Revisar caso por caso\n"
     ]
    }
   ],
   "source": [
    "# === NORMALIZAR Y PREPARAR DATOS PARA JOINS ===\n",
    "\n",
    "# 1. Estandarizar nombres de columnas\n",
    "print(\"Normalizando nombres de columnas...\")\n",
    "trips.columns = trips.columns.str.lower()  # convertir todas las columnas a minúsculas\n",
    "zones.columns = zones.columns.str.lower()  # misma transformación para consistencia\n",
    "\n",
    "print(f\"   Trips columnas: {list(trips.columns)}\")\n",
    "print(f\"   Zones columnas: {list(zones.columns)}\")\n",
    "\n",
    "# 2. Crear columna de fecha para el join con calendario\n",
    "trips['pickup_date'] = trips['tpep_pickup_datetime'].dt.date  # extraer solo la fecha (sin hora) de la columna datetime\n",
    "\n",
    "print(f\"   Columna pickup_date creada\")\n",
    "print(f\"   Rango de fechas: {trips['pickup_date'].min()} a {trips['pickup_date'].max()}\")\n",
    "\n",
    "# 3. Verificar tipos de datos para joins\n",
    "print(\"\\nVERIFICACIÓN DE TIPOS PARA JOINS:\")\n",
    "print(f\"   trips['pulocationid'] tipo: {trips['pulocationid'].dtype}\")\n",
    "print(f\"   zones['locationid'] tipo: {zones['locationid'].dtype}\")\n",
    "print(f\"   trips['pickup_date'] tipo: {type(trips['pickup_date'].iloc[0])}\")\n",
    "print(f\"   calendar['date'] tipo: {type(calendar['date'].iloc[0])}\")\n",
    "\n",
    "# 4. Optimización para datasets grandes (~3M registros)\n",
    "print(\"\\nOPTIMIZACIÓN PARA DATASETS GRANDES:\")\n",
    "initial_memory = trips.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"   Memoria inicial: {initial_memory:.1f} MB\")\n",
    "\n",
    "# Optimizar tipos de datos para 3+ millones de registros\n",
    "print(\"   Optimizando tipos de datos para 3M+ registros...\")\n",
    "\n",
    "# Limpiar valores nulos antes de convertir tipos\n",
    "print(\"   Limpiando valores nulos antes de optimización...\")\n",
    "trips['passenger_count'] = trips['passenger_count'].fillna(1)  # método para rellenar valores nulos con un valor específico\n",
    "trips = trips.dropna(subset=['pulocationid', 'dolocationid'])  # eliminar filas críticas sin ubicación (necesarias para joins)\n",
    "\n",
    "# Convertir tipos después de limpiar\n",
    "trips['pulocationid'] = trips['pulocationid'].astype('int16')\n",
    "trips['dolocationid'] = trips['dolocationid'].astype('int16') \n",
    "trips['passenger_count'] = trips['passenger_count'].astype('int8')\n",
    "zones['locationid'] = zones['locationid'].astype('int16')\n",
    "\n",
    "print(f\"   Registros después de limpieza: {len(trips):,}\")\n",
    "\n",
    "optimized_memory = trips.memory_usage(deep=True).sum() / 1024**2\n",
    "savings = ((initial_memory - optimized_memory) / initial_memory * 100)\n",
    "\n",
    "print(f\"   Memoria optimizada: {optimized_memory:.1f} MB\")\n",
    "print(f\"   Ahorro de memoria: {savings:.1f}%\")\n",
    "\n",
    "# 5. Revisar datos faltantes antes de joins\n",
    "print(\"\\nDATOS FALTANTES ANTES DE JOINS:\")\n",
    "print(\"Trips (top 5 columnas con más nulos):\")\n",
    "trips_nulls = trips.isna().sum().sort_values(ascending=False).head()  # método para detectar valores nulos, sumar y ordenar\n",
    "print(trips_nulls)\n",
    "\n",
    "print(\"\\nZones:\")\n",
    "zones_nulls = zones.isna().sum()  # revisar si hay valores faltantes en lookup table\n",
    "print(zones_nulls)\n",
    "print(zones[zones.isna().any(axis=1)])\n",
    "\n",
    "print(\"\\nCalendar:\")\n",
    "calendar_nulls = calendar.isna().sum()  # verificar integridad del calendario de eventos\n",
    "print(calendar_nulls)\n",
    "\n",
    "# Análisis de calidad de datos\n",
    "print(\"\\nANÁLISIS DE CALIDAD:\")\n",
    "total_trips = len(trips)\n",
    "print(f\"   Total de viajes: {total_trips:,}\")\n",
    "print(f\"   Viajes sin pickup location: {trips['pulocationid'].isna().sum():,}\")\n",
    "print(f\"   Viajes sin dropoff location: {trips['dolocationid'].isna().sum():,}\")\n",
    "print(f\"   Viajes sin passenger_count: {trips['passenger_count'].isna().sum():,}\")\n",
    "\n",
    "# Estrategias de limpieza recomendadas\n",
    "print(\"\\nESTRATEGIAS DE LIMPIEZA:\")\n",
    "print(\"   Ubicaciones nulas: Eliminar (crítico para joins)\")\n",
    "print(\"   Passenger_count nulos: Rellenar con valor típico (1)\")\n",
    "print(\"   Tarifas nulas: Revisar caso por caso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8eed1071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes: 3,066,766 registros\n",
      "Después: 2,995,023 registros\n",
      "Eliminados: 71,743 registros\n"
     ]
    }
   ],
   "source": [
    "#Eliminamos los viajes con valores nulos 2,4% del DataFrame\n",
    "# Lista de columnas con nulos\n",
    "cols_con_nulos = [\"airport_fee\", \"congestion_surcharge\", \"store_and_fwd_flag\", \"ratecodeid\"]\n",
    "\n",
    "# Dropear filas donde alguna de esas columnas sea NaN\n",
    "trips_clean = trips.dropna(subset=cols_con_nulos)\n",
    "\n",
    "print(f\"Antes: {len(trips):,} registros\")\n",
    "print(f\"Después: {len(trips_clean):,} registros\")\n",
    "print(f\"Eliminados: {len(trips) - len(trips_clean):,} registros\")\n",
    "\n",
    "# # Completo con 'Unknown' a los LocationID que faltan\n",
    "# zones[['borough','zone','service_zone']] = zones[['borough','zone','service_zone']].fillna(\"Unknown\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f79fcf5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando join: trips + zones...\n",
      "   Registros antes del join: 3066766\n",
      "   Registros después del join: 3066766\n",
      "   Nuevas columnas añadidas: ['locationid', 'borough', 'zone', 'service_zone']\n",
      "\n",
      "VERIFICACIÓN DEL JOIN:\n",
      "Conteo por Borough:\n",
      "borough\n",
      "Manhattan        2715369\n",
      "Queens            286645\n",
      "Unknown            40116\n",
      "Brooklyn           18076\n",
      "Bronx               4162\n",
      "EWR                  410\n",
      "Staten Island        341\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Viajes sin borough asignado: 1647\n",
      "   Algunos viajes no encontraron su zona correspondiente\n",
      "   LocationIDs problemáticos:\n",
      "   [265]\n",
      "\n",
      "MUESTRA DEL DATASET INTEGRADO:\n",
      "   pulocationid    borough               zone  trip_distance  total_amount\n",
      "0           161  Manhattan     Midtown Center           0.97         14.30\n",
      "1            43  Manhattan       Central Park           1.10         16.90\n",
      "2            48  Manhattan       Clinton East           2.51         34.90\n",
      "3           138     Queens  LaGuardia Airport           1.90         20.85\n",
      "4           107  Manhattan           Gramercy           1.43         19.68\n"
     ]
    }
   ],
   "source": [
    "# === PRIMER JOIN: TRIPS + ZONES ===\n",
    "\n",
    "# 1. Hacer join de trips con zones para obtener información geográfica\n",
    "print(\"Realizando join: trips + zones...\")\n",
    "trips_with_zones = trips.merge(zones,   # método principal para unir DataFrames\n",
    "                                left_on='pulocationid',   # columna de trips que contiene ID de zona de pickup\n",
    "                                right_on='locationid',  # columna de zones que contiene ID correspondiente\n",
    "                                how='left')       # tipo de join que mantiene todos los trips\n",
    "\n",
    "print(f\"   Registros antes del join: {len(trips)}\")\n",
    "print(f\"   Registros después del join: {len(trips_with_zones)}\")\n",
    "print(f\"   Nuevas columnas añadidas: {[col for col in trips_with_zones.columns if col not in trips.columns]}\")\n",
    "\n",
    "# 2. Verificar el resultado del join\n",
    "print(\"\\nVERIFICACIÓN DEL JOIN:\")\n",
    "print(\"Conteo por Borough:\")\n",
    "print(trips_with_zones['borough'].value_counts())\n",
    "\n",
    "# 3. Verificar si hay valores nulos después del join\n",
    "null_after_join = trips_with_zones['borough'].isna().sum()  # contar nulos en columna borough\n",
    "print(f\"\\nViajes sin borough asignado: {null_after_join}\")\n",
    "\n",
    "if null_after_join > 0:\n",
    "    print(\"   Algunos viajes no encontraron su zona correspondiente\")\n",
    "    print(\"   LocationIDs problemáticos:\")\n",
    "    problematic_ids = trips_with_zones[trips_with_zones['borough'].isna()]['pulocationid'].unique()  # filtrar filas con nulos\n",
    "    print(f\"   {problematic_ids}\")\n",
    "\n",
    "# 4. Mostrar muestra del resultado\n",
    "print(\"\\nMUESTRA DEL DATASET INTEGRADO:\")\n",
    "print(trips_with_zones[['pulocationid', 'borough', 'zone', 'trip_distance', 'total_amount']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b1e5453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando join: trips_zones + calendar...\n",
      "   Registros antes del join: 3066766\n",
      "   Registros después del join: 3066766\n",
      "\n",
      "DISTRIBUCIÓN DE DÍAS ESPECIALES:\n",
      "is_special_day\n",
      "False    3066766\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Ejemplos de eventos especiales:\n",
      "   No hay eventos especiales en este período\n",
      "\n",
      "DATASET FINAL INTEGRADO:\n",
      "   Total registros: 3066766\n",
      "   Total columnas: 28\n",
      "   Columnas principales: ['borough', 'zone', 'is_special_day', 'trip_distance', 'total_amount']\n",
      "\n",
      "VERIFICACIÓN FINAL:\n",
      "Datos faltantes por columna clave:\n",
      "   borough: 1647 nulos\n",
      "   zone: 40116 nulos\n",
      "   trip_distance: 0 nulos\n",
      "   total_amount: 0 nulos\n",
      "   is_special_day: 0 nulos\n"
     ]
    }
   ],
   "source": [
    "# === SEGUNDO JOIN: TRIPS_ZONES + CALENDAR ===\n",
    "\n",
    "# 1. Hacer join con datos de calendario\n",
    "print(\"Realizando join: trips_zones + calendar...\")\n",
    "trips_complete = trips_with_zones.merge(calendar,   # mismo método de join que antes\n",
    "                                         left_on='pickup_date',   # columna de fecha que creamos en trips\n",
    "                                         right_on='date',  # columna de fecha en calendar\n",
    "                                         how='left')       # tipo que mantiene todos los trips aunque no haya evento especial\n",
    "\n",
    "print(f\"   Registros antes del join: {len(trips_with_zones)}\")\n",
    "print(f\"   Registros después del join: {len(trips_complete)}\")\n",
    "\n",
    "# 2. Crear flag de evento especial\n",
    "trips_complete['is_special_day'] = trips_complete['special'].fillna('False')  # método para rellenar nulos con valor por defecto\n",
    "\n",
    "print(\"\\nDISTRIBUCIÓN DE DÍAS ESPECIALES:\")\n",
    "print(trips_complete['is_special_day'].value_counts())\n",
    "print(\"\\nEjemplos de eventos especiales:\")\n",
    "special_days = trips_complete[trips_complete['is_special_day'] == True]\n",
    "if len(special_days) > 0:\n",
    "    print(special_days[['pickup_date', 'special', 'borough']].drop_duplicates())\n",
    "else:\n",
    "    print(\"   No hay eventos especiales en este período\")\n",
    "\n",
    "# 3. Mostrar dataset final integrado\n",
    "print(\"\\nDATASET FINAL INTEGRADO:\")\n",
    "print(f\"   Total registros: {len(trips_complete)}\")\n",
    "print(f\"   Total columnas: {len(trips_complete.columns)}\")\n",
    "print(f\"   Columnas principales: {['borough', 'zone', 'is_special_day', 'trip_distance', 'total_amount']}\")\n",
    "\n",
    "# 4. Verificar integridad de los datos finales\n",
    "print(\"\\nVERIFICACIÓN FINAL:\")\n",
    "print(\"Datos faltantes por columna clave:\")\n",
    "key_columns = ['borough', 'zone', 'trip_distance', 'total_amount', 'is_special_day']\n",
    "for col in key_columns:\n",
    "    missing = trips_complete[col].isna().sum()  # verificar nulos en cada columna clave final\n",
    "    print(f\"   {col}: {missing} nulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "59a798f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Análisis por Borough (procesando datos grandes)...\n",
      "\n",
      "ANÁLISIS COMPLETO POR BOROUGH:\n",
      "               num_trips  avg_distance  std_distance  median_distance  \\\n",
      "borough                                                                 \n",
      "Manhattan        2715369          2.88        264.53             1.63   \n",
      "Queens            286645         12.32         14.42            11.24   \n",
      "Unknown            40116          7.57        144.96             2.64   \n",
      "Brooklyn           18076          5.68         70.86             3.45   \n",
      "Bronx               4162          5.30          6.34             3.10   \n",
      "EWR                  410          1.59          5.68             0.00   \n",
      "Staten Island        341         11.36         10.21            14.80   \n",
      "\n",
      "               avg_total  std_total  median_total  avg_fare  avg_tip  \\\n",
      "borough                                                                \n",
      "Manhattan          22.49      14.54         19.25     14.78     2.88   \n",
      "Queens             67.27      33.64         70.35     49.98     7.85   \n",
      "Unknown            38.08      30.41         25.38     26.44     4.82   \n",
      "Brooklyn           33.02      22.56         28.64     26.81     2.94   \n",
      "Bronx              34.54      33.26         29.70     30.24     0.78   \n",
      "EWR               104.38      62.75        118.55     87.99    12.44   \n",
      "Staten Island      62.53      44.92         67.80     48.74     1.32   \n",
      "\n",
      "               median_tip  avg_passengers  \n",
      "borough                                    \n",
      "Manhattan            2.66            1.35  \n",
      "Queens               8.18            1.39  \n",
      "Unknown              3.14            1.34  \n",
      "Brooklyn             0.60            1.22  \n",
      "Bronx                0.00            1.09  \n",
      "EWR                 10.00            1.58  \n",
      "Staten Island        0.00            1.13  \n",
      "\n",
      "ANÁLISIS CON MÉTRICAS EMPRESARIALES:\n",
      "               num_trips  market_share  revenue_per_km  tip_rate\n",
      "borough                                                         \n",
      "Manhattan        2715369          88.6            7.81      19.5\n",
      "Queens            286645           9.4            5.46      15.7\n",
      "Unknown            40116           1.3            5.03      18.2\n",
      "Brooklyn           18076           0.6            5.81      11.0\n",
      "Bronx               4162           0.1            6.52       2.6\n",
      "EWR                  410           0.0           65.65      14.1\n",
      "Staten Island        341           0.0            5.50       2.7\n",
      "\n",
      "INSIGHTS PRINCIPALES:\n",
      "   Borough con más viajes: Manhattan\n",
      "   Borough con viajes más largos: Queens\n",
      "   Borough con tarifas más altas: EWR\n",
      "   Mejor revenue por km: EWR\n"
     ]
    }
   ],
   "source": [
    "# === ANÁLISIS AGREGADO POR BOROUGH ===\n",
    "\n",
    "# 1. Análisis básico por borough (con dataset grande)\n",
    "print(\"Análisis por Borough (procesando datos grandes)...\")\n",
    "borough_analysis = trips_complete.groupby(by='borough').agg({   # agrupamos por la columna geográfica\n",
    "    'pulocationid': 'count',           # contar número de viajes\n",
    "    'trip_distance': ['mean', 'std', 'median'],  # promedio + desviación + mediana\n",
    "    'total_amount':  ['mean', 'std', 'median'],  # mismas estadísticas para total\n",
    "    'fare_amount':   'mean',                       # promedio de tarifa base\n",
    "    'tip_amount':    ['mean', 'median'],          # estadísticas de propinas\n",
    "    'passenger_count': 'mean'                     # promedio de pasajeros\n",
    "}).round(2)\n",
    "\n",
    "# Aplanar columnas multi-nivel\n",
    "borough_analysis.columns = ['num_trips', 'avg_distance', 'std_distance', 'median_distance',\n",
    "                           'avg_total', 'std_total', 'median_total', 'avg_fare', \n",
    "                           'avg_tip', 'median_tip', 'avg_passengers']\n",
    "\n",
    "# Ordenar por número de viajes\n",
    "borough_analysis = borough_analysis.sort_values(by='num_trips', ascending=False)\n",
    "\n",
    "print(\"\\nANÁLISIS COMPLETO POR BOROUGH:\")\n",
    "print(borough_analysis)\n",
    "\n",
    "# 2. Calcular métricas adicionales empresariales\n",
    "borough_analysis['revenue_per_km'] = (borough_analysis['avg_total'] / \n",
    "                                     borough_analysis['avg_distance']).round(2)\n",
    "borough_analysis['tip_rate'] = (borough_analysis['avg_tip'] / \n",
    "                               borough_analysis['avg_fare'] * 100).round(1)\n",
    "borough_analysis['market_share'] = (borough_analysis['num_trips'] / \n",
    "                                  borough_analysis['num_trips'].sum() * 100).round(1)\n",
    "\n",
    "print(\"\\nANÁLISIS CON MÉTRICAS EMPRESARIALES:\")\n",
    "print(borough_analysis[['num_trips', 'market_share', 'revenue_per_km', 'tip_rate']])\n",
    "\n",
    "# 3. Encontrar insights\n",
    "print(\"\\nINSIGHTS PRINCIPALES:\")\n",
    "print(f\"   Borough con más viajes: {borough_analysis.index[0]}\")\n",
    "print(f\"   Borough con viajes más largos: {borough_analysis['avg_distance'].idxmax()}\")\n",
    "print(f\"   Borough con tarifas más altas: {borough_analysis['avg_total'].idxmax()}\")\n",
    "print(f\"   Mejor revenue por km: {borough_analysis['revenue_per_km'].idxmax()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c0a41b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   vendorid tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
      "0         2  2023-01-01 00:32:10   2023-01-01 00:40:36                1   \n",
      "1         2  2023-01-01 00:55:08   2023-01-01 01:01:27                1   \n",
      "2         2  2023-01-01 00:25:04   2023-01-01 00:37:49                1   \n",
      "3         1  2023-01-01 00:03:48   2023-01-01 00:13:25                0   \n",
      "4         2  2023-01-01 00:10:29   2023-01-01 00:21:19                1   \n",
      "\n",
      "   trip_distance  ratecodeid store_and_fwd_flag  pulocationid  dolocationid  \\\n",
      "0           0.97         1.0                  N           161           141   \n",
      "1           1.10         1.0                  N            43           237   \n",
      "2           2.51         1.0                  N            48           238   \n",
      "3           1.90         1.0                  N           138             7   \n",
      "4           1.43         1.0                  N           107            79   \n",
      "\n",
      "   payment_type  ...  airport_fee  pickup_date  locationid    borough  \\\n",
      "0             2  ...         0.00   2023-01-01         161  Manhattan   \n",
      "1             1  ...         0.00   2023-01-01          43  Manhattan   \n",
      "2             1  ...         0.00   2023-01-01          48  Manhattan   \n",
      "3             1  ...         1.25   2023-01-01         138     Queens   \n",
      "4             1  ...         0.00   2023-01-01         107  Manhattan   \n",
      "\n",
      "                zone  service_zone  date  name  special is_special_day  \n",
      "0     Midtown Center   Yellow Zone   NaN   NaN      NaN          False  \n",
      "1       Central Park   Yellow Zone   NaN   NaN      NaN          False  \n",
      "2       Clinton East   Yellow Zone   NaN   NaN      NaN          False  \n",
      "3  LaGuardia Airport      Airports   NaN   NaN      NaN          False  \n",
      "4           Gramercy   Yellow Zone   NaN   NaN      NaN          False  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "Análisis: Borough + Día Especial...\n",
      "\n",
      "ANÁLISIS BOROUGH + DÍA ESPECIAL:\n",
      "                              num_trips  avg_distance  avg_total\n",
      "borough       is_special_day                                    \n",
      "Bronx         False                4162          5.30      34.54\n",
      "Brooklyn      False               18076          5.68      33.02\n",
      "EWR           False                 410          1.59     104.38\n",
      "Manhattan     False             2715369          2.88      22.49\n",
      "Queens        False              286645         12.32      67.27\n",
      "Staten Island False                 341         11.36      62.53\n",
      "Unknown       False               40116          7.57      38.08\n",
      "\n",
      "COMPARACIÓN DÍAS NORMALES VS ESPECIALES:\n",
      "            Avg Distance  Avg Amount  Num Trips\n",
      "Día Normal          3.85       27.02    3066766\n",
      "\n",
      "SOLO HAY Día Normal:\n",
      "   Viajes: 3,066,766.0\n",
      "   Distancia promedio: 3.85 millas\n",
      "   Tarifa promedio: $27.02\n",
      "   No hay datos de días especiales para comparar en este período\n"
     ]
    }
   ],
   "source": [
    "# === ANÁLISIS COMPARATIVO: DÍAS NORMALES VS ESPECIALES ===\n",
    "print(trips_complete.head())\n",
    "# 1. Análisis por borough y tipo de día\n",
    "print(\"Análisis: Borough + Día Especial...\")\n",
    "borough_day_analysis = trips_complete.groupby(by=['borough', 'is_special_day']).agg({  # agrupar por DOS columnas: geografía y tipo de día\n",
    "    'pulocationid': 'count',  # función para contar viajes\n",
    "    'trip_distance': 'mean',  # función para promedio de distancia\n",
    "    'total_amount': 'mean'    # función para promedio de tarifa\n",
    "}).round(2)\n",
    "\n",
    "borough_day_analysis.columns = ['num_trips', 'avg_distance', 'avg_total']\n",
    "\n",
    "print(\"\\nANÁLISIS BOROUGH + DÍA ESPECIAL:\")\n",
    "print(borough_day_analysis)\n",
    "\n",
    "# 2. Comparar días normales vs especiales\n",
    "print(\"\\nCOMPARACIÓN DÍAS NORMALES VS ESPECIALES:\")\n",
    "\n",
    "# Pivotear para comparar fácilmente\n",
    "comparison = trips_complete.groupby(by='is_special_day').agg({  # agrupar solo por tipo de día para comparación general\n",
    "    'trip_distance': 'mean',    # promedio de distancia por tipo de día\n",
    "    'total_amount': 'mean',     # promedio de tarifa por tipo de día\n",
    "    'pulocationid': 'count'     # conteo de viajes por tipo de día\n",
    "}).round(2)\n",
    "\n",
    "# Renombrar índices según los valores únicos encontrados\n",
    "unique_day_types = comparison.index.tolist()\n",
    "if len(unique_day_types) == 2:\n",
    "    comparison.index = ['Día Normal', 'Día Especial']\n",
    "elif len(unique_day_types) == 1:\n",
    "    if unique_day_types[0] in ['False', False]:\n",
    "        comparison.index = ['Día Normal']\n",
    "    else:\n",
    "        comparison.index = ['Día Especial']\n",
    "\n",
    "comparison.columns = ['Avg Distance', 'Avg Amount', 'Num Trips']\n",
    "\n",
    "print(comparison)\n",
    "\n",
    "# 3. Calcular diferencias porcentuales\n",
    "if len(comparison) > 1:\n",
    "    # Hay tanto días normales como especiales\n",
    "    if 'Día Normal' in comparison.index and 'Día Especial' in comparison.index:\n",
    "        normal_day = comparison.loc['Día Normal']\n",
    "        special_day = comparison.loc['Día Especial']\n",
    "\n",
    "        print(\"\\nIMPACTO DE DÍAS ESPECIALES:\")\n",
    "        distance_change = ((special_day['Avg Distance'] - normal_day['Avg Distance']) / normal_day['Avg Distance'] * 100)\n",
    "        amount_change = ((special_day['Avg Amount'] - normal_day['Avg Amount']) / normal_day['Avg Amount'] * 100)\n",
    "\n",
    "        print(f\"   Cambio en distancia promedio: {distance_change:+.1f}%\")\n",
    "        print(f\"   Cambio en tarifa promedio: {amount_change:+.1f}%\")\n",
    "    else:\n",
    "        print(\"\\nINFORMACIÓN DE DÍAS:\")\n",
    "        for idx, row in comparison.iterrows():\n",
    "            print(f\"   {idx}: {row['Num Trips']:,} viajes, ${row['Avg Amount']:.2f} promedio\")\n",
    "else:\n",
    "    print(f\"\\nSOLO HAY {comparison.index[0]}:\")\n",
    "    print(f\"   Viajes: {comparison.iloc[0]['Num Trips']:,}\")\n",
    "    print(f\"   Distancia promedio: {comparison.iloc[0]['Avg Distance']:.2f} millas\")\n",
    "    print(f\"   Tarifa promedio: ${comparison.iloc[0]['Avg Amount']:.2f}\")\n",
    "    print(\"   No hay datos de días especiales para comparar en este período\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dbde4fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Aplicando técnicas para datasets grandes...\n",
      "Dataset grande detectado: 3,066,766 registros\n",
      "Creando muestra estratificada para visualizaciones...\n",
      "Muestra creada: 10,000 registros (0.3%)\n",
      "\n",
      "ANÁLISIS DE PERFORMANCE:\n",
      "   total_trips: 3,066,766\n",
      "   matched_zones: 3,065,119\n",
      "   match_rate: 99.9%\n",
      "   unique_zones_used: 255\n",
      "   total_zones_available: 265\n",
      "   zone_coverage: 96.2%\n",
      "\n",
      "ANÁLISIS TEMPORAL AVANZADO:\n",
      "Horas pico por número de viajes:\n",
      "      18:00 - 215,889.0 viajes\n",
      "      17:00 - 209,493.0 viajes\n",
      "      15:00 - 196,424.0 viajes\n"
     ]
    }
   ],
   "source": [
    "# === TÉCNICAS PARA TRABAJAR CON DATASETS GRANDES ===\n",
    "\n",
    "# 1. Sampling estratégico para visualizaciones\n",
    "print(\"⚡ Aplicando técnicas para datasets grandes...\")\n",
    "\n",
    "# Si el dataset es muy grande, usar muestra para visualizaciones\n",
    "if len(trips_complete) > 50000:\n",
    "    print(f\"Dataset grande detectado: {len(trips_complete):,} registros\")\n",
    "    print(\"Creando muestra estratificada para visualizaciones...\")\n",
    "\n",
    "    # Muestra proporcional por borough\n",
    "    sample_size = min(10000, len(trips_complete) // 10)\n",
    "    trips_sample = trips_complete.sample(n=sample_size, random_state=42)  # método para tomar muestra aleatoria de n registros\n",
    "\n",
    "    print(f\"Muestra creada: {len(trips_sample):,} registros ({len(trips_sample)/len(trips_complete)*100:.1f}%)\")\n",
    "else:\n",
    "    trips_sample = trips_complete\n",
    "    print(\"Dataset pequeño, usando datos completos para visualización\")\n",
    "\n",
    "# 2. Análisis de performance de joins\n",
    "print(\"\\nANÁLISIS DE PERFORMANCE:\")\n",
    "join_stats = {\n",
    "    'total_trips': len(trips),\n",
    "    'matched_zones': (trips_complete['borough'].notna()).sum(),\n",
    "    'match_rate': (trips_complete['borough'].notna().sum() / len(trips) * 100),\n",
    "    'unique_zones_used': trips_complete['zone'].nunique(),\n",
    "    'total_zones_available': len(zones),\n",
    "    'zone_coverage': (trips_complete['zone'].nunique() / len(zones) * 100)\n",
    "}\n",
    "\n",
    "for key, value in join_stats.items():\n",
    "    if 'rate' in key or 'coverage' in key:\n",
    "        print(f\"   {key}: {value:.1f}%\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value:,}\")\n",
    "\n",
    "# 3. Análisis temporal avanzado (solo si hay suficientes datos)\n",
    "if len(trips_complete) > 1000:\n",
    "    print(\"\\nANÁLISIS TEMPORAL AVANZADO:\")\n",
    "\n",
    "    # Análisis por hora del día\n",
    "    trips_complete['pickup_hour'] = trips_complete['tpep_pickup_datetime'].dt.hour  # extraer hora de la fecha/hora\n",
    "    hourly_analysis = trips_complete.groupby(by='pickup_hour').agg({  # agrupar por hora del día\n",
    "        'pulocationid': 'count',     # contar viajes por hora\n",
    "        'total_amount': 'mean',      # tarifa promedio por hora\n",
    "        'trip_distance': 'mean'      # distancia promedio por hora\n",
    "    }).round(2)\n",
    "\n",
    "    hourly_analysis.columns = ['trips_count', 'avg_amount', 'avg_distance']\n",
    "\n",
    "    print(\"Horas pico por número de viajes:\")\n",
    "    peak_hours = hourly_analysis.sort_values(by='trips_count', ascending=False).head(3)  # ordenar por más viajes, tomar top 3\n",
    "    for hour, stats in peak_hours.iterrows():\n",
    "        print(f\"      {hour:02d}:00 - {stats['trips_count']:,} viajes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53f2cd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando correlaciones entre variables numéricas...\n",
      "\n",
      "Matriz de Correlación:\n",
      "               trip_distance  total_amount  fare_amount  tip_amount\n",
      "trip_distance          1.000         0.016        0.016       0.011\n",
      "total_amount           0.016         1.000        0.980       0.710\n",
      "fare_amount            0.016         0.980        1.000       0.590\n",
      "tip_amount             0.011         0.710        0.590       1.000\n",
      "\n",
      "Correlaciones más fuertes:\n",
      "   total_amount vs fare_amount: 0.980\n",
      "   total_amount vs tip_amount: 0.710\n",
      "   fare_amount vs tip_amount: 0.590\n",
      "\n",
      "INTERPRETACIÓN DE CORRELACIONES:\n",
      "   > 0.7: Correlación fuerte positiva\n",
      "   0.3-0.7: Correlación moderada positiva\n",
      "   -0.3-0.3: Correlación débil\n",
      "   < -0.7: Correlación fuerte negativa\n"
     ]
    }
   ],
   "source": [
    "# === ANÁLISIS DE CORRELACIONES NUMÉRICAS ===\n",
    "\n",
    "# Calcular correlaciones entre variables numéricas\n",
    "print(\"Calculando correlaciones entre variables numéricas...\")\n",
    "numeric_cols = ['trip_distance', 'total_amount', 'fare_amount', 'tip_amount']\n",
    "corr_matrix = trips_complete[numeric_cols].corr()  # método para calcular matriz de correlación\n",
    "\n",
    "print(\"\\nMatriz de Correlación:\")\n",
    "print(corr_matrix.round(3))\n",
    "\n",
    "print(\"\\nCorrelaciones más fuertes:\")\n",
    "corr_pairs = []\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i+1, len(corr_matrix.columns)):\n",
    "        corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "\n",
    "corr_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
    "for var1, var2, corr in corr_pairs[:3]:\n",
    "    print(f\"   {var1} vs {var2}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\nINTERPRETACIÓN DE CORRELACIONES:\")\n",
    "print(\"   > 0.7: Correlación fuerte positiva\")\n",
    "print(\"   0.3-0.7: Correlación moderada positiva\") \n",
    "print(\"   -0.3-0.3: Correlación débil\")\n",
    "print(\"   < -0.7: Correlación fuerte negativa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4928312",
   "metadata": {},
   "source": [
    "## PREGUNTAS FINALES\n",
    "1. ¿Qué diferencia hay entre un LEFT JOIN y un INNER JOIN? PISTA: Guía visual de joins\n",
    "\n",
    "Usando el left join te quedas con los datos de la tabla, es la que se mantiene, mientras que de la derecha son los valores que se agregan. \n",
    "Usando inner join se hace interseccion de las dos tablas. \n",
    "\n",
    "2. ¿Por qué usamos LEFT JOIN en lugar de INNER JOIN para trips+zones? PISTA: ¿Qué pasaría si algunos viajes no tienen zona asignada?\n",
    "\n",
    "Porque al hacer left te aseguras que vas a mantener toda la informacion de los viajes agregando las zonas correspondientes a los mismos. Si hicieramos inner vamos a perder la informacion de los trips que no tienen zona asignada. \n",
    "\n",
    "3. ¿Qué problemas pueden surgir al hacer joins con datos de fechas? PISTA: Tipos de datos, formatos, zonas horarias\n",
    "\n",
    "• Diferencias en el tipo de dato, por ejemplo: string o datetime.\n",
    "• Formatos de fecha distintos, por ejemplo: YYYY-MM-DD o DD/MM/YYYY.\n",
    "• Valores nulos o fechas faltantes que pueden impedir el join.\n",
    "\n",
    "4. ¿Cuál es la ventaja de integrar múltiples fuentes de datos? PISTA: Análisis más rico, contexto completo, insights cruzados\n",
    "\n",
    "Nos permite realizar un análisis más completo y contextualizado. Además, se pueden cruzar variables de diferentes bases para descubrir patrones que no serían visibles en un solo dataset; esto enriquece la información y habilita conclusiones más profundos.\n",
    "\n",
    "5. ¿Qué insights de negocio obtuviste del análisis integrado? PISTA: Patrones por zona, impacto de eventos especiales, oportunidades\n",
    "\n",
    "Manhattan concentra la mayoría de los viajes, los viajes en Queens son más largos y costosos en promedio y el mejor revenue por km son de EWR. \n",
    "Hay diferencias claras en el revenue por kilómetro y en la tasa de propinas entre boroughs y los días especiales pueden tener impacto en la distancia y tarifa promedio.\n",
    "\n",
    "## BONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ddc262b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prefect instalado y configurado\n",
      "   Versión: 3.4.14\n"
     ]
    }
   ],
   "source": [
    "import prefect\n",
    "from prefect import task, flow, get_run_logger\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "os.environ[\"PREFECT_LOGGING_SERVER_ENABLED\"] = \"false\"\n",
    "\n",
    "print(\"Prefect instalado y configurado\")\n",
    "print(f\"   Versión: {prefect.__version__}\")\n",
    "\n",
    "# === TASKS SIMPLES PARA APRENDER PREFECT ===\n",
    "\n",
    "@task(retries=3, retry_delay_seconds=10, name=\"Cargar Datos\")\n",
    "def cargar_datos(url: str, tipo: str) -> pd.DataFrame:\n",
    "    logger = get_run_logger()\n",
    "    logger.info(f\"Cargando {tipo} desde: {url}\")\n",
    "\n",
    "    tipo = tipo.lower().strip()\n",
    "    if tipo == \"parquet\":\n",
    "        # Requiere pyarrow instalado\n",
    "        return pd.read_parquet(url, engine=\"pyarrow\")\n",
    "    elif tipo == \"csv\":\n",
    "        # Maneja compresión (gzip) automáticamente\n",
    "        return pd.read_csv(url, encoding=\"utf-8\", low_memory=False, compression=\"infer\")\n",
    "    else:\n",
    "        raise ValueError(f\"Tipo no soportado: {tipo} (usa 'csv' o 'parquet')\")\n",
    "\n",
    "\n",
    "@task(name=\"Hacer Join Simple\")\n",
    "def hacer_join_simple(trips: pd.DataFrame, zones: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Task para hacer join básico de trips + zones\"\"\"\n",
    "    logger = get_run_logger()\n",
    "    logger.info(\"Haciendo join simple...\")\n",
    "\n",
    "    # Normalizar columnas\n",
    "    trips.columns = trips.columns.str.lower()  # convertir a minúsculas\n",
    "    zones.columns = zones.columns.str.lower()  # misma transformación\n",
    "\n",
    "    # Join básico\n",
    "    resultado = trips.merge(zones,   # método para unir DataFrames\n",
    "                             left_on='pickup_date',   # columna de pickup location en trips\n",
    "                             right_on='locationid',  # columna de location en zones\n",
    "                             how='left')       # tipo de join que mantiene todos los trips\n",
    "\n",
    "    logger.info(f\"Join completado: {len(resultado)} registros\")\n",
    "    return resultado\n",
    "\n",
    "@task(name=\"Análisis Rápido\")\n",
    "\n",
    "def analisis_rapido(data: pd.DataFrame) -> dict:\n",
    "    \"\"\"Task para análisis básico\"\"\"\n",
    "    logger = get_run_logger()\n",
    "    logger.info(\"Haciendo análisis básico...\")\n",
    "\n",
    "    # Stats simples\n",
    "    stats = {\n",
    "        'total_registros': len(data),\n",
    "        'boroughs': data['borough'].count().head(3).to_dict(),  # método para contar valores\n",
    "        'distancia_promedio': round(data['trip_distance'].mean(), 2),  # método para promedio\n",
    "        'tarifa_promedio': round(data['total_amount'].mean(), 2)  # método para promedio\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Análisis completado: {stats['total_registros']} registros\")\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c58f71a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FLOW PRINCIPAL (EL PIPELINE COMPLETO) ===\n",
    "\n",
    "@flow(name=\"Pipeline Simple NYC Taxi\")\n",
    "def pipeline_taxi_simple():\n",
    "    \"\"\"\n",
    "    Flow simple que conecta todos los tasks\n",
    "    \"\"\"\n",
    "    logger = get_run_logger()\n",
    "    logger.info(\"Iniciando pipeline simple...\")\n",
    "\n",
    "    # URLs de datos\n",
    "    trips_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\"\n",
    "    zones_url = \"https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\"\n",
    "\n",
    "    # PASO 1: Cargar datos (con retry automático si falla)\n",
    "    logger.info(\"Paso 1: Cargando datos...\")\n",
    "    trips = cargar_datos(trips_url, \"parquet\")  # tipo de datos trips\n",
    "    zones = cargar_datos(zones_url, \"csv\")  # tipo de datos zones\n",
    "\n",
    "    # PASO 2: Hacer join\n",
    "    logger.info(\"Paso 2: Haciendo join...\")\n",
    "    data_unida = hacer_join_simple(trips, zones)\n",
    "\n",
    "    # PASO 3: Análisis básico\n",
    "    logger.info(\"Paso 3: Analizando...\")\n",
    "    resultados = analisis_rapido(data_unida)\n",
    "\n",
    "    # PASO 4: Mostrar resultados\n",
    "    logger.info(\"Pipeline completado!\")\n",
    "    logger.info(f\"Resultados: {resultados}\")\n",
    "\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d72e7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejecutando pipeline simple...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:08:42.590 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | prefect - Starting temporary server on <span style=\"color: #0000ff; text-decoration-color: #0000ff\">http://127.0.0.1:8275</span>\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://docs.prefect.io/v3/concepts/server#how-to-guides</span> for more information on running a dedicated Prefect server.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:08:42.590 | \u001b[36mINFO\u001b[0m    | prefect - Starting temporary server on \u001b[94mhttp://127.0.0.1:8275\u001b[0m\n",
       "See \u001b[94mhttps://docs.prefect.io/v3/concepts/server#how-to-guides\u001b[0m for more information on running a dedicated Prefect server.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:09:09.335 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'xanthic-giraffe'</span> - Beginning flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'xanthic-giraffe'</span> for flow<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> 'Pipeline Simple NYC Taxi'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:09:09.335 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'xanthic-giraffe'\u001b[0m - Beginning flow run\u001b[35m 'xanthic-giraffe'\u001b[0m for flow\u001b[1;35m 'Pipeline Simple NYC Taxi'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:09:09.344 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'xanthic-giraffe'</span> - Iniciando pipeline simple...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:09:09.344 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'xanthic-giraffe'\u001b[0m - Iniciando pipeline simple...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:09:09.347 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'xanthic-giraffe'</span> - Paso 1: Cargando datos...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:09:09.347 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'xanthic-giraffe'\u001b[0m - Paso 1: Cargando datos...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:09:10.256 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'Cargar Datos-8bf' - Cargando parquet desde: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:09:10.256 | \u001b[36mINFO\u001b[0m    | Task run 'Cargar Datos-8bf' - Cargando parquet desde: \u001b[94mhttps://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:09:15.320 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'Cargar Datos-8bf' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:09:15.320 | \u001b[36mINFO\u001b[0m    | Task run 'Cargar Datos-8bf' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:09:16.078 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'Cargar Datos-c9b' - Cargando csv desde: <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:09:16.078 | \u001b[36mINFO\u001b[0m    | Task run 'Cargar Datos-c9b' - Cargando csv desde: \u001b[94mhttps://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:09:16.543 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'Cargar Datos-c9b' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:09:16.543 | \u001b[36mINFO\u001b[0m    | Task run 'Cargar Datos-c9b' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:09:16.550 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'xanthic-giraffe'</span> - Paso 2: Haciendo join...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:09:16.550 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'xanthic-giraffe'\u001b[0m - Paso 2: Haciendo join...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:09:19.845 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'Hacer Join Simple-fd4' - Haciendo join simple...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:09:19.845 | \u001b[36mINFO\u001b[0m    | Task run 'Hacer Join Simple-fd4' - Haciendo join simple...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:09:19.849 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Task run 'Hacer Join Simple-fd4' - Task run failed with exception: KeyError('pickup_date') - No retries configured for this task.\n",
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 869, in run_context\n",
       "    yield self\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 1505, in run_task_sync\n",
       "    engine.call_task_fn(txn)\n",
       "    ~~~~~~~~~~~~~~~~~~~^^^^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 886, in call_task_fn\n",
       "    result = call_with_parameters(self.task.fn, parameters)\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "  File \"C:\\Users\\juanp\\AppData\\Local\\Temp\\ipykernel_23228\\1823903112.py\", line 40, in hacer_join_simple\n",
       "    resultado = trips.merge(zones,   # método para unir DataFrames\n",
       "                             left_on='pickup_date',   # columna de pickup location en trips\n",
       "                             right_on='locationid',  # columna de location en zones\n",
       "                             how='left')       # tipo de join que mantiene todos los trips\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\frame.py\", line 10839, in merge\n",
       "    return merge(\n",
       "        self,\n",
       "    ...&lt;11 lines&gt;...\n",
       "        validate=validate,\n",
       "    )\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 170, in merge\n",
       "    op = _MergeOperation(\n",
       "        left_df,\n",
       "    ...&lt;10 lines&gt;...\n",
       "        validate=validate,\n",
       "    )\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 794, in __init__\n",
       "    ) = self._get_merge_keys()\n",
       "        ~~~~~~~~~~~~~~~~~~~~^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 1311, in _get_merge_keys\n",
       "    left_keys.append(left._get_label_or_level_values(lk))\n",
       "                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\generic.py\", line 1911, in _get_label_or_level_values\n",
       "    raise KeyError(key)\n",
       "KeyError: 'pickup_date'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:09:19.849 | \u001b[38;5;160mERROR\u001b[0m   | Task run 'Hacer Join Simple-fd4' - Task run failed with exception: KeyError('pickup_date') - No retries configured for this task.\n",
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 869, in run_context\n",
       "    yield self\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 1505, in run_task_sync\n",
       "    engine.call_task_fn(txn)\n",
       "    ~~~~~~~~~~~~~~~~~~~^^^^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 886, in call_task_fn\n",
       "    result = call_with_parameters(self.task.fn, parameters)\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "  File \"C:\\Users\\juanp\\AppData\\Local\\Temp\\ipykernel_23228\\1823903112.py\", line 40, in hacer_join_simple\n",
       "    resultado = trips.merge(zones,   # método para unir DataFrames\n",
       "                             left_on='pickup_date',   # columna de pickup location en trips\n",
       "                             right_on='locationid',  # columna de location en zones\n",
       "                             how='left')       # tipo de join que mantiene todos los trips\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\frame.py\", line 10839, in merge\n",
       "    return merge(\n",
       "        self,\n",
       "    ...<11 lines>...\n",
       "        validate=validate,\n",
       "    )\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 170, in merge\n",
       "    op = _MergeOperation(\n",
       "        left_df,\n",
       "    ...<10 lines>...\n",
       "        validate=validate,\n",
       "    )\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 794, in __init__\n",
       "    ) = self._get_merge_keys()\n",
       "        ~~~~~~~~~~~~~~~~~~~~^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 1311, in _get_merge_keys\n",
       "    left_keys.append(left._get_label_or_level_values(lk))\n",
       "                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\generic.py\", line 1911, in _get_label_or_level_values\n",
       "    raise KeyError(key)\n",
       "KeyError: 'pickup_date'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:09:20.024 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Task run 'Hacer Join Simple-fd4' - Finished in state <span style=\"color: #d70000; text-decoration-color: #d70000\">Failed</span>(\"Task run encountered an exception KeyError: 'pickup_date'\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:09:20.024 | \u001b[38;5;160mERROR\u001b[0m   | Task run 'Hacer Join Simple-fd4' - Finished in state \u001b[38;5;160mFailed\u001b[0m(\"Task run encountered an exception KeyError: 'pickup_date'\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:09:20.030 | <span style=\"color: #d70000; text-decoration-color: #d70000\">ERROR</span>   | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'xanthic-giraffe'</span> - Encountered exception during execution: KeyError('pickup_date')\n",
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\flow_engine.py\", line 782, in run_context\n",
       "    yield self\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\flow_engine.py\", line 1397, in run_flow_sync\n",
       "    engine.call_flow_fn()\n",
       "    ~~~~~~~~~~~~~~~~~~~^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\flow_engine.py\", line 802, in call_flow_fn\n",
       "    result = call_with_parameters(self.flow.fn, self.parameters)\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "  File \"C:\\Users\\juanp\\AppData\\Local\\Temp\\ipykernel_23228\\3170529249.py\", line 22, in pipeline_taxi_simple\n",
       "    data_unida = hacer_join_simple(trips, zones)\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\tasks.py\", line 1139, in __call__\n",
       "    return run_task(\n",
       "        task=self,\n",
       "    ...&lt;2 lines&gt;...\n",
       "        return_type=return_type,\n",
       "    )\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 1732, in run_task\n",
       "    return run_task_sync(**kwargs)\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 1507, in run_task_sync\n",
       "    return engine.state if return_type == \"state\" else engine.result()\n",
       "                                                       ~~~~~~~~~~~~~^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 494, in result\n",
       "    raise self._raised\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 869, in run_context\n",
       "    yield self\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 1505, in run_task_sync\n",
       "    engine.call_task_fn(txn)\n",
       "    ~~~~~~~~~~~~~~~~~~~^^^^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 886, in call_task_fn\n",
       "    result = call_with_parameters(self.task.fn, parameters)\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "  File \"C:\\Users\\juanp\\AppData\\Local\\Temp\\ipykernel_23228\\1823903112.py\", line 40, in hacer_join_simple\n",
       "    resultado = trips.merge(zones,   # método para unir DataFrames\n",
       "                             left_on='pickup_date',   # columna de pickup location en trips\n",
       "                             right_on='locationid',  # columna de location en zones\n",
       "                             how='left')       # tipo de join que mantiene todos los trips\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\frame.py\", line 10839, in merge\n",
       "    return merge(\n",
       "        self,\n",
       "    ...&lt;11 lines&gt;...\n",
       "        validate=validate,\n",
       "    )\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 170, in merge\n",
       "    op = _MergeOperation(\n",
       "        left_df,\n",
       "    ...&lt;10 lines&gt;...\n",
       "        validate=validate,\n",
       "    )\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 794, in __init__\n",
       "    ) = self._get_merge_keys()\n",
       "        ~~~~~~~~~~~~~~~~~~~~^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 1311, in _get_merge_keys\n",
       "    left_keys.append(left._get_label_or_level_values(lk))\n",
       "                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\generic.py\", line 1911, in _get_label_or_level_values\n",
       "    raise KeyError(key)\n",
       "KeyError: 'pickup_date'\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:09:20.030 | \u001b[38;5;160mERROR\u001b[0m   | Flow run\u001b[35m 'xanthic-giraffe'\u001b[0m - Encountered exception during execution: KeyError('pickup_date')\n",
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\flow_engine.py\", line 782, in run_context\n",
       "    yield self\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\flow_engine.py\", line 1397, in run_flow_sync\n",
       "    engine.call_flow_fn()\n",
       "    ~~~~~~~~~~~~~~~~~~~^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\flow_engine.py\", line 802, in call_flow_fn\n",
       "    result = call_with_parameters(self.flow.fn, self.parameters)\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "  File \"C:\\Users\\juanp\\AppData\\Local\\Temp\\ipykernel_23228\\3170529249.py\", line 22, in pipeline_taxi_simple\n",
       "    data_unida = hacer_join_simple(trips, zones)\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\tasks.py\", line 1139, in __call__\n",
       "    return run_task(\n",
       "        task=self,\n",
       "    ...<2 lines>...\n",
       "        return_type=return_type,\n",
       "    )\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 1732, in run_task\n",
       "    return run_task_sync(**kwargs)\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 1507, in run_task_sync\n",
       "    return engine.state if return_type == \"state\" else engine.result()\n",
       "                                                       ~~~~~~~~~~~~~^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 494, in result\n",
       "    raise self._raised\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 869, in run_context\n",
       "    yield self\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 1505, in run_task_sync\n",
       "    engine.call_task_fn(txn)\n",
       "    ~~~~~~~~~~~~~~~~~~~^^^^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\", line 886, in call_task_fn\n",
       "    result = call_with_parameters(self.task.fn, parameters)\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\utilities\\callables.py\", line 210, in call_with_parameters\n",
       "    return fn(*args, **kwargs)\n",
       "  File \"C:\\Users\\juanp\\AppData\\Local\\Temp\\ipykernel_23228\\1823903112.py\", line 40, in hacer_join_simple\n",
       "    resultado = trips.merge(zones,   # método para unir DataFrames\n",
       "                             left_on='pickup_date',   # columna de pickup location en trips\n",
       "                             right_on='locationid',  # columna de location en zones\n",
       "                             how='left')       # tipo de join que mantiene todos los trips\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\frame.py\", line 10839, in merge\n",
       "    return merge(\n",
       "        self,\n",
       "    ...<11 lines>...\n",
       "        validate=validate,\n",
       "    )\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 170, in merge\n",
       "    op = _MergeOperation(\n",
       "        left_df,\n",
       "    ...<10 lines>...\n",
       "        validate=validate,\n",
       "    )\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 794, in __init__\n",
       "    ) = self._get_merge_keys()\n",
       "        ~~~~~~~~~~~~~~~~~~~~^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\", line 1311, in _get_merge_keys\n",
       "    left_keys.append(left._get_label_or_level_values(lk))\n",
       "                     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^\n",
       "  File \"c:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\generic.py\", line 1911, in _get_label_or_level_values\n",
       "    raise KeyError(key)\n",
       "KeyError: 'pickup_date'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">20:09:20.096 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'xanthic-giraffe'</span> - Finished in state <span style=\"color: #d70000; text-decoration-color: #d70000\">Failed</span>(\"Flow run encountered an exception: KeyError: 'pickup_date'\")\n",
       "</pre>\n"
      ],
      "text/plain": [
       "20:09:20.096 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'xanthic-giraffe'\u001b[0m - Finished in state \u001b[38;5;160mFailed\u001b[0m(\"Flow run encountered an exception: KeyError: 'pickup_date'\")\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'pickup_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_23228\\3991190509.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m __name__ == \u001b[33m\"__main__\"\u001b[39m:\n\u001b[32m      4\u001b[39m     print(\u001b[33m\"Ejecutando pipeline simple...\"\u001b[39m)\n\u001b[32m      5\u001b[39m \n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Ejecutar el flow\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     resultado = pipeline_taxi_simple()  \u001b[38;5;66;03m# nombre de la función del flow\u001b[39;00m\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m     print(\u001b[33m\"\\nRESULTADOS FINALES:\"\u001b[39m)\n\u001b[32m     10\u001b[39m     print(f\"   Total registros: {resultado[\u001b[33m'total_registros'\u001b[39m]:,}\")\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\flows.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, return_state, wait_for, *args, **kwargs)\u001b[39m\n\u001b[32m   1698\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m track_viz_task(self.isasync, self.name, parameters)\n\u001b[32m   1699\u001b[39m \n\u001b[32m   1700\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m prefect.flow_engine \u001b[38;5;28;01mimport\u001b[39;00m run_flow\n\u001b[32m   1701\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1702\u001b[39m         return run_flow(\n\u001b[32m   1703\u001b[39m             flow=self,\n\u001b[32m   1704\u001b[39m             parameters=parameters,\n\u001b[32m   1705\u001b[39m             wait_for=wait_for,\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\flow_engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type, error_logger, context)\u001b[39m\n\u001b[32m   1558\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m error_logger:\n\u001b[32m   1559\u001b[39m             error_logger.error(\n\u001b[32m   1560\u001b[39m                 \u001b[33m\"Engine execution exited with unexpected exception\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1561\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   1563\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret_val\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\flow_engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type, context)\u001b[39m\n\u001b[32m   1395\u001b[39m         \u001b[38;5;28;01mwhile\u001b[39;00m engine.is_running():\n\u001b[32m   1396\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m engine.run_context():\n\u001b[32m   1397\u001b[39m                 engine.call_flow_fn()\n\u001b[32m   1398\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"state\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m engine.result()\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\flow_engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, raise_on_failure)\u001b[39m\n\u001b[32m    357\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m    358\u001b[39m \n\u001b[32m    359\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self._raised \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m NotSet:\n\u001b[32m    360\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m raise_on_failure:\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m self._raised\n\u001b[32m    362\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._raised\n\u001b[32m    363\u001b[39m \n\u001b[32m    364\u001b[39m         \u001b[38;5;66;03m# This is a fall through case which leans on the existing state result mechanics to get the\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\flow_engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    769\u001b[39m     @contextmanager\n\u001b[32m    770\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m run_context(self):\n\u001b[32m    771\u001b[39m         timeout_context = timeout_async \u001b[38;5;28;01mif\u001b[39;00m self.flow.isasync \u001b[38;5;28;01melse\u001b[39;00m timeout\n\u001b[32m    772\u001b[39m         \u001b[38;5;66;03m# reenter the run context to ensure it is up to date for every run\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m773\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m self.setup_run_context():\n\u001b[32m    774\u001b[39m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    775\u001b[39m                 with timeout_context(\n\u001b[32m    776\u001b[39m                     seconds=self.flow.timeout_seconds,\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\flow_engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(flow, flow_run, parameters, wait_for, return_type, context)\u001b[39m\n\u001b[32m   1395\u001b[39m         \u001b[38;5;28;01mwhile\u001b[39;00m engine.is_running():\n\u001b[32m   1396\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m engine.run_context():\n\u001b[32m   1397\u001b[39m                 engine.call_flow_fn()\n\u001b[32m   1398\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1399\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"state\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m engine.result()\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\flow_engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    798\u001b[39m                 self.handle_success(result)\n\u001b[32m    799\u001b[39m \n\u001b[32m    800\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m _call_flow_fn()\n\u001b[32m    801\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m802\u001b[39m             result = call_with_parameters(self.flow.fn, self.parameters)\n\u001b[32m    803\u001b[39m             self.handle_success(result)\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\utilities\\callables.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(fn, parameters)\u001b[39m\n\u001b[32m    206\u001b[39m     will fail. If you need to send to a function \u001b[38;5;28;01mwith\u001b[39;00m a different signature, extract\n\u001b[32m    207\u001b[39m     the args/kwargs using `parameters_to_positional_and_keyword` directly\n\u001b[32m    208\u001b[39m     \"\"\"\n\u001b[32m    209\u001b[39m     args, kwargs = parameters_to_args_kwargs(fn, parameters)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_23228\\3170529249.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     18\u001b[39m     zones = cargar_datos(zones_url, \u001b[33m\"csv\"\u001b[39m)  \u001b[38;5;66;03m# tipo de datos zones\u001b[39;00m\n\u001b[32m     19\u001b[39m \n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# PASO 2: Hacer join\u001b[39;00m\n\u001b[32m     21\u001b[39m     logger.info(\u001b[33m\"Paso 2: Haciendo join...\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     data_unida = hacer_join_simple(trips, zones)\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m     \u001b[38;5;66;03m# PASO 3: Análisis básico\u001b[39;00m\n\u001b[32m     25\u001b[39m     logger.info(\u001b[33m\"Paso 3: Analizando...\"\u001b[39m)\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\tasks.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, return_state, wait_for, *args, **kwargs)\u001b[39m\n\u001b[32m   1135\u001b[39m             )\n\u001b[32m   1136\u001b[39m \n\u001b[32m   1137\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m prefect.task_engine \u001b[38;5;28;01mimport\u001b[39;00m run_task\n\u001b[32m   1138\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1139\u001b[39m         return run_task(\n\u001b[32m   1140\u001b[39m             task=self,\n\u001b[32m   1141\u001b[39m             parameters=parameters,\n\u001b[32m   1142\u001b[39m             wait_for=wait_for,\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[39m\n\u001b[32m   1728\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m run_generator_task_sync(**kwargs)\n\u001b[32m   1729\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m task.isasync:\n\u001b[32m   1730\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m run_task_async(**kwargs)\n\u001b[32m   1731\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1732\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m run_task_sync(**kwargs)\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[39m\n\u001b[32m   1503\u001b[39m                 engine.transaction_context() \u001b[38;5;28;01mas\u001b[39;00m txn,\n\u001b[32m   1504\u001b[39m             ):\n\u001b[32m   1505\u001b[39m                 engine.call_task_fn(txn)\n\u001b[32m   1506\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"state\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m engine.result()\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, raise_on_failure)\u001b[39m\n\u001b[32m    490\u001b[39m \n\u001b[32m    491\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m self._raised \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m NotSet:\n\u001b[32m    492\u001b[39m             \u001b[38;5;66;03m# if the task raised an exception, raise it\u001b[39;00m\n\u001b[32m    493\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m raise_on_failure:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m self._raised\n\u001b[32m    495\u001b[39m \n\u001b[32m    496\u001b[39m             \u001b[38;5;66;03m# otherwise, return the exception\u001b[39;00m\n\u001b[32m    497\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self._raised\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    854\u001b[39m     @contextmanager\n\u001b[32m    855\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m run_context(self):\n\u001b[32m    856\u001b[39m         \u001b[38;5;66;03m# reenter the run context to ensure it is up to date for every run\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m857\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m self.setup_run_context():\n\u001b[32m    858\u001b[39m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    859\u001b[39m                 with timeout(\n\u001b[32m    860\u001b[39m                     seconds=self.task.timeout_seconds,\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(task, task_run_id, task_run, parameters, wait_for, return_type, dependencies, context)\u001b[39m\n\u001b[32m   1503\u001b[39m                 engine.transaction_context() \u001b[38;5;28;01mas\u001b[39;00m txn,\n\u001b[32m   1504\u001b[39m             ):\n\u001b[32m   1505\u001b[39m                 engine.call_task_fn(txn)\n\u001b[32m   1506\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m engine.state \u001b[38;5;28;01mif\u001b[39;00m return_type == \u001b[33m\"state\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m engine.result()\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\task_engine.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, transaction)\u001b[39m\n\u001b[32m    882\u001b[39m         parameters = self.parameters \u001b[38;5;28;01mor\u001b[39;00m {}\n\u001b[32m    883\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m transaction.is_committed():\n\u001b[32m    884\u001b[39m             result = transaction.read()\n\u001b[32m    885\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m886\u001b[39m             result = call_with_parameters(self.task.fn, parameters)\n\u001b[32m    887\u001b[39m         self.handle_success(result, transaction=transaction)\n\u001b[32m    888\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\prefect\\utilities\\callables.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(fn, parameters)\u001b[39m\n\u001b[32m    206\u001b[39m     will fail. If you need to send to a function \u001b[38;5;28;01mwith\u001b[39;00m a different signature, extract\n\u001b[32m    207\u001b[39m     the args/kwargs using `parameters_to_positional_and_keyword` directly\n\u001b[32m    208\u001b[39m     \"\"\"\n\u001b[32m    209\u001b[39m     args, kwargs = parameters_to_args_kwargs(fn, parameters)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_23228\\1823903112.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(trips, zones)\u001b[39m\n\u001b[32m     36\u001b[39m     trips.columns = trips.columns.str.lower()  \u001b[38;5;66;03m# convertir a minúsculas\u001b[39;00m\n\u001b[32m     37\u001b[39m     zones.columns = zones.columns.str.lower()  \u001b[38;5;66;03m# misma transformación\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# Join básico\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     resultado = trips.merge(zones,   # método para unir DataFrames\n\u001b[32m     41\u001b[39m                              left_on=\u001b[33m'pickup_date'\u001b[39m,   \u001b[38;5;66;03m# columna de pickup location en trips\u001b[39;00m\n\u001b[32m     42\u001b[39m                              right_on=\u001b[33m'locationid'\u001b[39m,  \u001b[38;5;66;03m# columna de location en zones\u001b[39;00m\n\u001b[32m     43\u001b[39m                              how=\u001b[33m'left'\u001b[39m)       \u001b[38;5;66;03m# tipo de join que mantiene todos los trips\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m  10835\u001b[39m         validate: MergeValidate | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m  10836\u001b[39m     ) -> DataFrame:\n\u001b[32m  10837\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m pandas.core.reshape.merge \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m  10838\u001b[39m \n\u001b[32m> \u001b[39m\u001b[32m10839\u001b[39m         return merge(\n\u001b[32m  10840\u001b[39m             self,\n\u001b[32m  10841\u001b[39m             right,\n\u001b[32m  10842\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    166\u001b[39m             validate=validate,\n\u001b[32m    167\u001b[39m             copy=copy,\n\u001b[32m    168\u001b[39m         )\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         op = _MergeOperation(\n\u001b[32m    171\u001b[39m             left_df,\n\u001b[32m    172\u001b[39m             right_df,\n\u001b[32m    173\u001b[39m             how=how,\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    790\u001b[39m             self.right_join_keys,\n\u001b[32m    791\u001b[39m             self.join_names,\n\u001b[32m    792\u001b[39m             left_drop,\n\u001b[32m    793\u001b[39m             right_drop,\n\u001b[32m--> \u001b[39m\u001b[32m794\u001b[39m         ) = self._get_merge_keys()\n\u001b[32m    795\u001b[39m \n\u001b[32m    796\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m left_drop:\n\u001b[32m    797\u001b[39m             self.left = self.left._drop_labels_or_levels(left_drop)\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1307\u001b[39m                     \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1308\u001b[39m                         \u001b[38;5;66;03m# Then we're either Hashable or a wrong-length arraylike,\u001b[39;00m\n\u001b[32m   1309\u001b[39m                         \u001b[38;5;66;03m#  the latter of which will raise\u001b[39;00m\n\u001b[32m   1310\u001b[39m                         lk = cast(Hashable, lk)\n\u001b[32m-> \u001b[39m\u001b[32m1311\u001b[39m                         left_keys.append(left._get_label_or_level_values(lk))\n\u001b[32m   1312\u001b[39m                         join_names.append(lk)\n\u001b[32m   1313\u001b[39m                     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1314\u001b[39m                         \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "\u001b[32mc:\\Users\\juanp\\OneDrive - Universidad Católica del Uruguay\\UCU\\Semestre 4\\Ingeniería de Datos\\portafolio-ia\\.ven\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1907\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1908\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1909\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1910\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1912\u001b[39m \n\u001b[32m   1913\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1914\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'pickup_date'"
     ]
    }
   ],
   "source": [
    "# === EJECUTAR EL PIPELINE ===\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Ejecutando pipeline simple...\")\n",
    "\n",
    "    # Ejecutar el flow\n",
    "    resultado = pipeline_taxi_simple()  # nombre de la función del flow\n",
    "\n",
    "    print(\"\\nRESULTADOS FINALES:\")\n",
    "    print(f\"   Total registros: {resultado['total_registros']:,}\")\n",
    "    print(f\"   Distancia promedio: {resultado['distancia_promedio']} millas\")\n",
    "    print(f\"   Tarifa promedio: ${resultado['tarifa_promedio']}\")\n",
    "    print(\"\\nTop 3 Boroughs:\")\n",
    "    for borough, count in resultado['top_boroughs'].items():  # clave del diccionario que contiene boroughs\n",
    "        print(f\"   {borough}: {count:,} viajes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c31bdbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANTES - Código normal:\n",
    "def cargar_datos(url):\n",
    "    return pd.read_csv(url)  # Si falla, todo se rompe\n",
    "\n",
    "# DESPUÉS - Con Prefect:\n",
    "@task(retries=2)\n",
    "def cargar_datos(url):\n",
    "    return pd.read_csv(url)  # Si falla, lo intenta 2 veces más"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5e2306",
   "metadata": {},
   "source": [
    "# PREGUNTAS BONUS\n",
    "\n",
    "1. ¿Qué ventaja tiene usar @task en lugar de una función normal? PISTA: ¿Qué pasa si la carga de datos falla temporalmente?\n",
    "\n",
    "    Una función normal en Python se ejecuta sin ningún control extra.\n",
    "\n",
    "    Cuando usas @task:\n",
    "\n",
    "    - Podés definir reintentos automáticos si falla por algo temporal (ej: red caída al bajar datos).\n",
    "\n",
    "    - Podés definir cortes por tiempo (si un paso tarda demasiado, Prefect lo corta).\n",
    "\n",
    "    - Tenés logs integrados en la interfaz de Prefect.\n",
    "\n",
    "    - Cada task queda orquestado y monitoreado: podés ver qué falló y reintentar sólo ese paso.\n",
    "\n",
    "2. ¿Para qué sirve el @flow decorator? PISTA: ¿Cómo conecta y organiza los tasks?\n",
    "    El @flow define un pipeline completo, que organiza y conecta varios @task.\n",
    "\n",
    "    Le dice a Prefect: “Esto no es sólo un script de Python, es un flujo de trabajo con dependencias y monitoreo”.\n",
    "\n",
    "    Permite:\n",
    "\n",
    "    - Ejecutar tasks en orden y pasar resultados de uno a otro.\n",
    "\n",
    "    - Monitorear el flow run entero.\n",
    "\n",
    "3. ¿En qué casos reales usarías esto? PISTA: Reportes diarios, análisis automáticos, pipelines de ML\n",
    "\n",
    "    Reportes diarios: Automatizar que cada mañana se bajen datos de ventas, se limpien y se envíen reportes a un dashboard.\n",
    "\n",
    "    Análisis automáticos: Procesar logs de usuarios, detectar anomalías o generar alertas de fraude sin intervención manual.\n",
    "\n",
    "    Pipelines de ML: cargar dataset - limpiar/preprocesar - entrenar modelo - guardar métricas "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".ven",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
