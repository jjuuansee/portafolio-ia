---
title: "Web Scraping de Datos con Python"
date: 2025-12-07
author: "Juan Paroli"
---

# üï∑Ô∏è Web Scraping de Datos con Python

## Contexto

En este ejercicio se implementaron t√©cnicas de **web scraping** para extraer datos de sitios web usando **BeautifulSoup** y **requests**. El web scraping es una habilidad fundamental en ingenier√≠a de datos para obtener informaci√≥n de fuentes p√∫blicas que no tienen APIs disponibles.

## üéØ Objetivos

- [x] Entender la estructura HTML de p√°ginas web
- [x] Usar requests para obtener contenido de p√°ginas
- [x] Parsear HTML con BeautifulSoup
- [x] Extraer datos estructurados (tablas, listas, texto)
- [x] Manejar paginaci√≥n y m√∫ltiples p√°ginas
- [x] Exportar datos a CSV/DataFrame
- [x] Implementar buenas pr√°cticas (delays, headers, robots.txt)

## Desarrollo

### 1. Instalaci√≥n

```bash
pip install requests beautifulsoup4 lxml pandas
```

### 2. Estructura b√°sica de scraping

```python
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

# Headers para simular navegador
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

def scrape_page(url):
    """Extrae datos de una p√°gina web."""
    response = requests.get(url, headers=headers)
    
    if response.status_code == 200:
        soup = BeautifulSoup(response.content, 'lxml')
        return soup
    else:
        print(f"Error: {response.status_code}")
        return None

# Ejemplo de uso
url = "https://example.com/data"
soup = scrape_page(url)

# Extraer elementos
titles = soup.find_all('h2', class_='title')
prices = soup.find_all('span', class_='price')

# Construir DataFrame
data = []
for title, price in zip(titles, prices):
    data.append({
        'titulo': title.text.strip(),
        'precio': price.text.strip()
    })

df = pd.DataFrame(data)
```

### 3. Selectores CSS y m√©todos principales

| M√©todo | Uso |
|--------|-----|
| `soup.find()` | Primer elemento que coincide |
| `soup.find_all()` | Todos los elementos que coinciden |
| `soup.select()` | Selector CSS (m√°s flexible) |
| `element.text` | Texto del elemento |
| `element['href']` | Atributo del elemento |
| `element.get('class')` | Obtener atributo (None si no existe) |

### 4. Scraping de tablas HTML

```python
def scrape_table(soup):
    """Extrae datos de una tabla HTML."""
    table = soup.find('table')
    rows = table.find_all('tr')
    
    data = []
    headers = [th.text.strip() for th in rows[0].find_all('th')]
    
    for row in rows[1:]:
        cols = row.find_all('td')
        data.append([col.text.strip() for col in cols])
    
    return pd.DataFrame(data, columns=headers)
```

### 5. Manejo de paginaci√≥n

```python
def scrape_all_pages(base_url, max_pages=10):
    """Scraping con paginaci√≥n."""
    all_data = []
    
    for page in range(1, max_pages + 1):
        url = f"{base_url}?page={page}"
        soup = scrape_page(url)
        
        if soup is None:
            break
            
        # Extraer datos de la p√°gina
        page_data = extract_data(soup)
        all_data.extend(page_data)
        
        # Delay entre requests (respeto al servidor)
        time.sleep(1)
        
        print(f"P√°gina {page} completada")
    
    return pd.DataFrame(all_data)
```

---

## Buenas Pr√°cticas

| Pr√°ctica | Descripci√≥n |
|----------|-------------|
| **Respetar robots.txt** | Verificar qu√© p√°ginas se pueden scrapear |
| **Delays entre requests** | Usar `time.sleep()` para no sobrecargar servidores |
| **User-Agent** | Identificarse correctamente |
| **Manejo de errores** | Try-except para conexiones fallidas |
| **Cach√©** | Guardar respuestas para no repetir requests |
| **Rate limiting** | No hacer m√°s de X requests por minuto |

---

## Herramientas Alternativas

| Herramienta | Uso |
|-------------|-----|
| **Selenium** | P√°ginas con JavaScript din√°mico |
| **Scrapy** | Framework completo para scraping a escala |
| **Playwright** | Automatizaci√≥n de navegador moderna |
| **requests-html** | requests + renderizado JS |

---

## Evidencias

- **Notebook**: [scraping.ipynb](scraping.ipynb)
- **Dataset exportado**: `books_scraped.csv`
- **Sitio scrapeado**: [books.toscrape.com](https://books.toscrape.com/)

---

## Reflexi√≥n

El web scraping es una herramienta poderosa pero requiere **responsabilidad**:

- Siempre verificar los t√©rminos de servicio del sitio
- No sobrecargar servidores con requests excesivos
- Preferir APIs oficiales cuando est√©n disponibles
- Considerar aspectos legales seg√∫n jurisdicci√≥n

### Casos de uso v√°lidos

- Agregaci√≥n de datos p√∫blicos
- Monitoreo de precios
- Investigaci√≥n acad√©mica
- Datos sin API disponible

---

## Conclusi√≥n

Se implement√≥ un flujo completo de web scraping:

1. ‚úÖ Setup de requests y BeautifulSoup
2. ‚úÖ Parsing de HTML
3. ‚úÖ Extracci√≥n de datos estructurados
4. ‚úÖ Manejo de paginaci√≥n
5. ‚úÖ Exportaci√≥n a DataFrame/CSV
6. ‚úÖ Buenas pr√°cticas implementadas

---

## Referencias

- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Requests Library](https://docs.python-requests.org/)
- [Web Scraping Best Practices](https://www.scrapehero.com/how-to-prevent-getting-blacklisted-while-scraping/)
- [Scrapy Framework](https://scrapy.org/)

