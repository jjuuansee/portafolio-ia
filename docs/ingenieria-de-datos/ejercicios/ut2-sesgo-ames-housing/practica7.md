```python
!pip install fairlearn

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, r2_score, mean_squared_error

# Fairlearn - La estrella del show
from fairlearn.metrics import (
    MetricFrame, 
    demographic_parity_difference, 
    equalized_odds_difference,
    selection_rate
)
from fairlearn.reductions import ExponentiatedGradient, DemographicParity

import warnings
warnings.filterwarnings('ignore')

plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
np.random.seed(42)

print("âš–ï¸ PRÃCTICA 7: Detectar y Corregir Sesgo con Fairlearn")
print("ğŸ“Š Parte I: Boston Housing (sesgo racial histÃ³rico)")
print("ğŸš¢ Parte II: Titanic (sesgo gÃ©nero + clase)")
print("ğŸ”§ Parte III: Pipeline automÃ¡tico producciÃ³n")
```

    Requirement already satisfied: fairlearn in /Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages (0.12.0)
    Requirement already satisfied: numpy>=1.24.4 in /Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from fairlearn) (2.2.6)
    Requirement already satisfied: pandas>=2.0.3 in /Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from fairlearn) (2.2.3)
    Requirement already satisfied: scikit-learn>=1.2.1 in /Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from fairlearn) (1.5.2)
    Requirement already satisfied: scipy>=1.9.3 in /Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from fairlearn) (1.15.3)
    Requirement already satisfied: python-dateutil>=2.8.2 in /Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pandas>=2.0.3->fairlearn) (2.9.0.post0)
    Requirement already satisfied: pytz>=2020.1 in /Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pandas>=2.0.3->fairlearn) (2025.2)
    Requirement already satisfied: tzdata>=2022.7 in /Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from pandas>=2.0.3->fairlearn) (2025.2)
    Requirement already satisfied: six>=1.5 in /Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=2.0.3->fairlearn) (1.17.0)
    Requirement already satisfied: joblib>=1.2.0 in /Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from scikit-learn>=1.2.1->fairlearn) (1.4.2)
    Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/merbarrutia/.pyenv/versions/3.10.13/lib/python3.10/site-packages (from scikit-learn>=1.2.1->fairlearn) (3.6.0)
    âš–ï¸ PRÃCTICA 7: Detectar y Corregir Sesgo con Fairlearn
    ğŸ“Š Parte I: Boston Housing (sesgo racial histÃ³rico)
    ğŸš¢ Parte II: Titanic (sesgo gÃ©nero + clase)
    ğŸ”§ Parte III: Pipeline automÃ¡tico producciÃ³n
    


```python
# Cargar desde fuente original (CMU)

data_url = "http://lib.stat.cmu.edu/datasets/boston"
raw_df = pd.read_csv(data_url, sep="\s+", skiprows=22, header=None)

# Restructurar formato especial
data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
target = raw_df.values[1::2, 2]

print(f"âœ… Boston Housing cargado: {data.shape}")

feature_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 
                'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']

boston_df = pd.DataFrame(data, columns=feature_names)
boston_df['MEDV'] = target

# Decodificar variable B problemÃ¡tica
# B = 1000(Bk - 0.63)Â² â†’ Bk = sqrt(B/1000) + 0.63
boston_df['Bk_racial'] = np.sqrt(boston_df['B'] / 1000) + 0.63

print(f"ğŸš¨ Variable B (racial): correlaciÃ³n = {boston_df['B'].corr(boston_df['MEDV']):.3f}")
print(f"ğŸ“Š ProporciÃ³n racial media: {boston_df['Bk_racial'].mean():.3f}")
```

    âœ… Boston Housing cargado: (506, 13)
    ğŸš¨ Variable B (racial): correlaciÃ³n = 0.333
    ğŸ“Š ProporciÃ³n racial media: 1.216
    


```python
# Preparar features con y sin variable racial
X_with_bias = boston_df.drop(['MEDV', 'Bk_racial'], axis=1)
X_without_bias = X_with_bias.drop(['B'], axis=1)
y_boston = boston_df['MEDV']

# Train modelo con sesgo
X_train, X_test, y_train, y_test = train_test_split(
    X_with_bias, y_boston, test_size=0.3, random_state=42
)

boston_biased_model = LinearRegression()
boston_biased_model.fit(X_train, y_train)
boston_biased_pred = boston_biased_model.predict(X_test)

boston_biased_r2 = r2_score(y_test, boston_biased_pred)
print(f"ğŸ”´ Boston CON sesgo: RÂ² = {boston_biased_r2:.4f}")
```

    ğŸ”´ Boston CON sesgo: RÂ² = 0.7112
    


```python
# PASO 4A: Crear grupos por proporciÃ³n racial  
racial_threshold = boston_df['Bk_racial'].median()  # mediana
boston_df['grupo_racial'] = (boston_df['Bk_racial'] > racial_threshold).map({
    True: 'Alta_prop_afroam', 
    False: 'Baja_prop_afroam'
})

print(f"ğŸ‘¥ GRUPOS POR PROPORCIÃ“N RACIAL:")
print(boston_df['grupo_racial'].value_counts())

# PASO 4B: AnÃ¡lisis de distribuciÃ³n de precios por grupo
print(f"\nğŸ’° DISTRIBUCIÃ“N DE PRECIOS POR GRUPO RACIAL:")
price_by_group = boston_df.groupby('grupo_racial')['MEDV'].agg(['mean', 'median', 'std', 'count'])
print(price_by_group)

# PASO 4C: Calcular brecha de precios
price_gap = price_by_group.loc['Baja_prop_afroam', 'mean'] - price_by_group.loc['Alta_prop_afroam', 'mean']
price_gap_pct = (price_gap / price_by_group.loc['Alta_prop_afroam', 'mean']) * 100

print(f"\nğŸš¨ BRECHA DE PRECIOS POR SESGO RACIAL:")
print(f"Diferencia promedio: ${price_gap:.2f}k ({price_gap_pct:.1f}%)")
print(f"Baja prop. afroam: ${price_by_group.loc['Baja_prop_afroam', 'mean']:.2f}k")
print(f"Alta prop. afroam: ${price_by_group.loc['Alta_prop_afroam', 'mean']:.2f}k")

# PASO 4D: Visualizar el sesgo
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Histograma de precios por grupo
for group in boston_df['grupo_racial'].unique():
    data = boston_df[boston_df['grupo_racial'] == group]['MEDV']
    axes[0].hist(data, alpha=0.7, label=group, bins=20)
axes[0].set_xlabel('Precio (miles $)')
axes[0].set_ylabel('Frecuencia')
axes[0].set_title('DistribuciÃ³n de Precios por Grupo Racial')
axes[0].legend()

# Boxplot comparativo
boston_df.boxplot(column='MEDV', by='grupo_racial', ax=axes[1])
axes[1].set_title('Precios por Grupo Racial')
axes[1].set_xlabel('Grupo Racial')
axes[1].set_ylabel('Precio (miles $)')

plt.tight_layout()
plt.show()

print(f"ğŸ“Š VISUALIZACIÃ“N: Â¿Se observa sesgo sistemÃ¡tico en las distribuciones?")
```

    ğŸ‘¥ GRUPOS POR PROPORCIÃ“N RACIAL:
    grupo_racial
    Alta_prop_afroam    253
    Baja_prop_afroam    253
    Name: count, dtype: int64
    
    ğŸ’° DISTRIBUCIÃ“N DE PRECIOS POR GRUPO RACIAL:
                           mean  median        std  count
    grupo_racial                                         
    Alta_prop_afroam  22.810672    22.0   7.994651    253
    Baja_prop_afroam  22.254941    20.4  10.268380    253
    
    ğŸš¨ BRECHA DE PRECIOS POR SESGO RACIAL:
    Diferencia promedio: $-0.56k (-2.4%)
    Baja prop. afroam: $22.25k
    Alta prop. afroam: $22.81k
    


    
![png](practica7_files/practica7_3_1.png)
    


    ğŸ“Š VISUALIZACIÃ“N: Â¿Se observa sesgo sistemÃ¡tico en las distribuciones?
    


```python
# PASO 5A: ReflexiÃ³n guiada sobre el uso Ã©tico de variable B
print("âš–ï¸ REFLEXIÃ“N Ã‰TICA SOBRE VARIABLE B")
print("="*50)

print(f"\nğŸ¤” PREGUNTAS PARA REFLEXIONAR:")

print(f"\n1. CONTEXTO HISTÃ“RICO:")
print(f"   - La variable B fue diseÃ±ada en 1978")
print(f"   - Codifica proporciÃ³n de poblaciÃ³n afroamericana") 
print(f"   - CorrelaciÃ³n con precios: {boston_df['B'].corr(boston_df['MEDV']):.3f}")
print(f"   â“ Â¿Es Ã©tico usar esta variable en 2025?")
print("NO")

print(f"\n2. DILEMA DE UTILIDAD:")
print(f"   - La variable B es predictiva (mejora el modelo)")
print(f"   - Pero perpetÃºa sesgos raciales histÃ³ricos")
print(f"   â“ Â¿CuÃ¡ndo la utilidad justifica el sesgo?")
print("Depende del uso que se le quiera dar, ejemplo: se entrena con datos de determinada raza y los resultados " \
"solo se van a usar con esa raza determinada. Igualmente, es muy raro que el sesgo este justificado, siempre debemos evitarlo.")

print(f"\n3. RESPONSABILIDAD PROFESIONAL:")
print(f"   - Sklearn removiÃ³ este dataset por razones Ã©ticas")
print(f"   - Nosotros lo usamos para APRENDER sobre sesgo")
print(f"   â“ Â¿CuÃ¡l es nuestra responsabilidad como data scientists?")
print("No utilizar estos datos por fuera de fines academicos ni compartir el trabajo.")

print(f"\n4. ALTERNATIVAS Ã‰TICAS:")
print(f"   - Podemos eliminar la variable B")
print(f"   - Podemos documentar sus limitaciones") 
print(f"   - Podemos buscar proxies menos problemÃ¡ticos")
print(f"   â“ Â¿QuÃ© harÃ­as en un contexto real?")
print("Se prohibe el uso de la base y se evita sesgo Ã©tico")

# PASO 5B: AnÃ¡lisis de correlaciones alternativas
print(f"\nğŸ“Š ANÃLISIS DE VARIABLES ALTERNATIVAS:")
print(f"Variables que podrÃ­an ser menos problemÃ¡ticas:")

alternative_vars = ['LSTAT', 'RM', 'CRIM', 'TAX', 'PTRATIO']
for var in alternative_vars:
    corr = boston_df[var].corr(boston_df['MEDV'])
    print(f"  {var}: correlaciÃ³n = {corr:.3f}")

print(f"\nğŸ’¡ OBSERVACIÃ“N:")
print(f"Algunas variables tienen correlaciones altas sin sesgo racial explÃ­cito")

# PASO 5C: Marco de decisiÃ³n Ã©tica
print(f"\nğŸ¯ MARCO DE DECISIÃ“N PARA VARIABLE PROBLEMÃTICA:")
print(f"="*50)

print(f"\nâœ… USAR variable B SI:")
print(f"  - Contexto es puramente acadÃ©mico/educativo")
print(f"  - Se documenta explÃ­citamente su naturaleza problemÃ¡tica") 
print(f"  - El objetivo es estudiar/detectar sesgo histÃ³rico")

print(f"\nâŒ NO USAR variable B SI:")
print(f"  - El modelo se usarÃ¡ en producciÃ³n")
print(f"  - AfectarÃ¡ decisiones sobre personas reales")
print(f"  - Existe riesgo de perpetuar discriminaciÃ³n")

print(f"\nâš–ï¸ TU DECISIÃ“N Ã‰TICA:")
print(f"Basado en el anÃ¡lisis, Â¿usarÃ­as la variable B en tu modelo?")
print(f"Â¿Por quÃ©? Â¿QuÃ© consideraciones Ã©ticas son mÃ¡s importantes?")

# PASO 5D: Documentar la decisiÃ³n
boston_ethical_decision = "USAR SOLO PARA EDUCACIÃ“N - NO PARA PRODUCCIÃ“N"
print(f"\nğŸ“‹ DECISIÃ“N DOCUMENTADA: {boston_ethical_decision}")
print(f"ğŸ“ JustificaciÃ³n: Variable histÃ³ricamente sesgada, Ãºtil para detectar sesgo pero inapropiada para modelos de producciÃ³n")
```

    âš–ï¸ REFLEXIÃ“N Ã‰TICA SOBRE VARIABLE B
    ==================================================
    
    ğŸ¤” PREGUNTAS PARA REFLEXIONAR:
    
    1. CONTEXTO HISTÃ“RICO:
       - La variable B fue diseÃ±ada en 1978
       - Codifica proporciÃ³n de poblaciÃ³n afroamericana
       - CorrelaciÃ³n con precios: 0.333
       â“ Â¿Es Ã©tico usar esta variable en 2025?
    NO
    
    2. DILEMA DE UTILIDAD:
       - La variable B es predictiva (mejora el modelo)
       - Pero perpetÃºa sesgos raciales histÃ³ricos
       â“ Â¿CuÃ¡ndo la utilidad justifica el sesgo?
    Depende del uso que se le quiera dar, ejemplo: se entrena con datos de determinada raza y los resultados solo se van a usar con esa raza determinada. Igualmente, es muy raro que el sesgo este justificado, siempre debemos evitarlo.
    
    3. RESPONSABILIDAD PROFESIONAL:
       - Sklearn removiÃ³ este dataset por razones Ã©ticas
       - Nosotros lo usamos para APRENDER sobre sesgo
       â“ Â¿CuÃ¡l es nuestra responsabilidad como data scientists?
    No utilizar estos datos por fuera de fines academicos ni compartir el trabajo.
    
    4. ALTERNATIVAS Ã‰TICAS:
       - Podemos eliminar la variable B
       - Podemos documentar sus limitaciones
       - Podemos buscar proxies menos problemÃ¡ticos
       â“ Â¿QuÃ© harÃ­as en un contexto real?
    Se prohibe el uso de la base y se evita sesgo Ã©tico
    
    ğŸ“Š ANÃLISIS DE VARIABLES ALTERNATIVAS:
    Variables que podrÃ­an ser menos problemÃ¡ticas:
      LSTAT: correlaciÃ³n = -0.738
      RM: correlaciÃ³n = 0.695
      CRIM: correlaciÃ³n = -0.388
      TAX: correlaciÃ³n = -0.469
      PTRATIO: correlaciÃ³n = -0.508
    
    ğŸ’¡ OBSERVACIÃ“N:
    Algunas variables tienen correlaciones altas sin sesgo racial explÃ­cito
    
    ğŸ¯ MARCO DE DECISIÃ“N PARA VARIABLE PROBLEMÃTICA:
    ==================================================
    
    âœ… USAR variable B SI:
      - Contexto es puramente acadÃ©mico/educativo
      - Se documenta explÃ­citamente su naturaleza problemÃ¡tica
      - El objetivo es estudiar/detectar sesgo histÃ³rico
    
    âŒ NO USAR variable B SI:
      - El modelo se usarÃ¡ en producciÃ³n
      - AfectarÃ¡ decisiones sobre personas reales
      - Existe riesgo de perpetuar discriminaciÃ³n
    
    âš–ï¸ TU DECISIÃ“N Ã‰TICA:
    Basado en el anÃ¡lisis, Â¿usarÃ­as la variable B en tu modelo?
    Â¿Por quÃ©? Â¿QuÃ© consideraciones Ã©ticas son mÃ¡s importantes?
    
    ğŸ“‹ DECISIÃ“N DOCUMENTADA: USAR SOLO PARA EDUCACIÃ“N - NO PARA PRODUCCIÃ“N
    ğŸ“ JustificaciÃ³n: Variable histÃ³ricamente sesgada, Ãºtil para detectar sesgo pero inapropiada para modelos de producciÃ³n
    


```python
# Cargar Titanic
try:
    titanic = sns.load_dataset("titanic")  # load_dataset
except:
    titanic = pd.read_csv("https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv")

titanic_clean = titanic.dropna(subset=['age', 'embarked'])  # dropna

# AnÃ¡lisis rÃ¡pido de sesgo
gender_survival = titanic_clean.groupby('sex')['survived'].mean()  # mean
class_survival = titanic_clean.groupby('pclass')['survived'].mean()

print(f"ğŸš¨ TITANIC BIAS ANALYSIS:")
print(f"Gender gap: {gender_survival['female'] - gender_survival['male']:.1%}")
print(f"Class gap: {class_survival[1] - class_survival[3]:.1%}")
print("âœ… Ambos tipos de sesgo significativos!")
```

    ğŸš¨ TITANIC BIAS ANALYSIS:
    Gender gap: 54.8%
    Class gap: 41.3%
    âœ… Ambos tipos de sesgo significativos!
    


```python
# Preparar datos Titanic
features_titanic = ['pclass', 'age', 'sibsp', 'parch', 'fare']
X_titanic = titanic_clean[features_titanic].copy()
X_titanic['fare'].fillna(X_titanic['fare'].median(), inplace=True)  # fillna
y_titanic = titanic_clean['survived']
sensitive_titanic = titanic_clean['sex']

# Train baseline
X_train_t, X_test_t, y_train_t, y_test_t, A_train_t, A_test_t = train_test_split(
    X_titanic, y_titanic, sensitive_titanic, test_size=0.3, random_state=42, stratify=y_titanic
)

titanic_baseline = RandomForestClassifier(n_estimators=100, random_state=42)
titanic_baseline.fit(X_train_t, y_train_t)
titanic_baseline_pred = titanic_baseline.predict(X_test_t)

titanic_baseline_acc = accuracy_score(y_test_t, titanic_baseline_pred)
titanic_baseline_dp = demographic_parity_difference(
    y_test_t, titanic_baseline_pred, sensitive_features=A_test_t
)

print(f"ğŸ”´ Titanic BASELINE: Accuracy = {titanic_baseline_acc:.3f}")
print(f"ğŸš¨ Demographic Parity Diff: {titanic_baseline_dp:.3f}")
```

    ğŸ”´ Titanic BASELINE: Accuracy = 0.673
    ğŸš¨ Demographic Parity Diff: 0.113
    


```python
# Aplicar ExponentiatedGradient a Titanic
titanic_fair = ExponentiatedGradient(
    RandomForestClassifier(n_estimators=100, random_state=42),
    constraints=DemographicParity()
)

print("ğŸ”§ Aplicando Fairlearn a Titanic...")
titanic_fair.fit(X_train_t, y_train_t, sensitive_features=A_train_t)
titanic_fair_pred = titanic_fair.predict(X_test_t)

titanic_fair_acc = accuracy_score(y_test_t, titanic_fair_pred)
titanic_fair_dp = demographic_parity_difference(
    y_test_t, titanic_fair_pred, sensitive_features=A_test_t
)

print(f"ğŸŸ¢ Titanic FAIR: Accuracy = {titanic_fair_acc:.3f}")
print(f"âš–ï¸ Demographic Parity Diff: {titanic_fair_dp:.3f}")
```

    ğŸ”§ Aplicando Fairlearn a Titanic...
    ğŸŸ¢ Titanic FAIR: Accuracy = 0.617
    âš–ï¸ Demographic Parity Diff: 0.035
    


```python
titanic_performance_loss = (titanic_baseline_acc - titanic_fair_acc) / titanic_baseline_acc * 100
titanic_fairness_gain = abs(titanic_baseline_dp) - abs(titanic_fair_dp)

print(f"ğŸ“Š TITANIC TRADE-OFF:")
print(f"Performance loss: {titanic_performance_loss:.1f}%")  
print(f"Fairness gain: {titanic_fairness_gain:.3f}")

if titanic_performance_loss < 5 and titanic_fairness_gain > 0.1:
    titanic_recommendation = "âœ… Usar modelo FAIR - excelente trade-off"
else:
    titanic_recommendation = "âš ï¸ Evaluar caso por caso"

print(f"ğŸ“‹ RecomendaciÃ³n Titanic: {titanic_recommendation}")
```

    ğŸ“Š TITANIC TRADE-OFF:
    Performance loss: 8.3%
    Fairness gain: 0.079
    ğŸ“‹ RecomendaciÃ³n Titanic: âš ï¸ Evaluar caso por caso
    

Â¿QuÃ© has aprendido sobre DETECTAR sesgo y decidir estrategias Ã©ticas?

ğŸ“Š Resultados por Dataset:Â¶

AnÃ¡lisis de DetecciÃ³n de Sesgo:

Dataset 1 (RegresiÃ³n): Brecha detectada = -2.4%
Impacto en modelo: 
DecisiÃ³n Ã©tica: Tiene sesgo racial, por lo que no debemos usarlo. Solo con fines acadÃ©micos para trabajar con sesgo.

Dataset 2 (ClasificaciÃ³n): Brecha detectada = Gender gap: 54.8% y Class gap: 41.3%
TÃ©cnica aplicada: Performance loss =  8.3% despuÃ©s de correcciÃ³n y Fairness gain: 0.079
DecisiÃ³n Ã©tica: No deberiamos filtrar por sexo y clase, estas tienen una brecha muy grande y sesgada.

DetecciÃ³n vs CorrecciÃ³n: Cada estrategia apropiada para diferentes contextos
Sesgo histÃ³rico: MÃ¡s complejo de corregir que sesgo sistemÃ¡tico
Context matters: Dominio determina tolerancia al sesgo
Frameworks universales: Posibles pero requieren adaptaciÃ³n por caso
Reflexiones Ã‰ticas CrÃ­ticas: 

Â¿CuÃ¡ndo es mÃ¡s valioso detectar que corregir automÃ¡ticamente?
* Detectar es mÃ¡s valioso cuando el sesgo es por factores histÃ³ricos que no se pueden arreglar, y necesitamos transparencia para comprender cÃ³mo se manifiesta. TambiÃ©n cuando se trata de aplicaciones sensibles donde una correcciÃ³n automÃ¡tica puede generar problemas.

Â¿CÃ³mo balancear transparencia vs utilidad cuando persiste el sesgo?
* Transparencia se garantiza documentando el sesgo y sus efectos, utilidad implica decidir si el modelo aÃºn puede ser aplicado bajo ciertos lÃ­mites y el balance se logra mediante explicabilidad. 


Â¿QuÃ© responsabilidades Ã©ticas tenemos con sesgos histÃ³ricos no corregibles?
* Reconocerlos, no ocultarlos y evitar reforzarlos.

CRÃTICO: Â¿Es mejor un modelo con sesgo conocido y documentado vs uno "corregido" pero impredecible?
* En la mayorÃ­a de los casos es preferible un modelo con sesgo conocido y documentado porque permite tomar decisiones conscientes. El otro nos servirÃ­a para trabajar eficientemente en lugar de analizando correlaciÃ³n e influencia de variables.

# Entrega extendida


```python
"""
MATI: tenes que Aplicar los conocimientos de esta entrega en la base de ames housing (la de las entregas 4 y 5)
Luego de hacer eso, deberÃ­as tener estos valores: 

Brecha geogrÃ¡fica: {completar con tu resultado}% entre barrios mÃ¡s y menos caros
Brecha temporal: {completar}% diferencia entre casas nuevas vs antiguas
DecisiÃ³n Ã©tica: {tu evaluaciÃ³n sobre uso en decisiones hipotecarias}"""





```
