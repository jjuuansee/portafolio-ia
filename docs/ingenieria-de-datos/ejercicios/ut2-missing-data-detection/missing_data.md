---
title: "An√°lisis de calidad de datos e imputaci√≥n en Ames Housing"
date: 2025-10-10
author: "Juan Paroli"
categories: ["Data Cleaning", "Machine Learning", "EDA"]
tags: ["Missing Data", "Outliers", "Pipelines", "Ames Housing"]
---

# An√°lisis de calidad de datos e imputaci√≥n en Ames Housing

## Contexto
Este an√°lisis se centr√≥ en evaluar y tratar los problemas de **calidad de datos** del dataset *Ames Housing*, una base de datos ampliamente usada en aprendizaje autom√°tico para predecir precios de vivienda.  
Se abordaron los ejes clave de **valores faltantes**, **outliers** y **preprocesamiento reproducible**, generando visualizaciones y estrategias sistem√°ticas de imputaci√≥n y limpieza.

> El trabajo replica un flujo de preprocesamiento profesional, desde la exploraci√≥n inicial hasta la implementaci√≥n de pipelines reproducibles en `scikit-learn`.

---

## Objetivos
- [x] Analizar la estructura del dataset e identificar patrones de valores faltantes.
- [x] Clasificar los tipos de *missing data* (MCAR, MAR, MNAR) presentes.
- [x] Implementar y comparar estrategias de imputaci√≥n simples y ‚Äúsmart‚Äù.
- [x] Detectar y evaluar outliers mediante m√©todos IQR y Z-score.
- [x] Crear un pipeline reproducible de limpieza para modelos predictivos.

---

## Desarrollo

### 1. Exploraci√≥n inicial del dataset
El dataset contiene **2930 registros y 82 columnas**, con una mezcla de variables num√©ricas (‚âà 38) y categ√≥ricas (‚âà 43).  
El an√°lisis de memoria revel√≥ un uso total de aproximadamente **6.9 MB**, con algunas variables categ√≥ricas.

Se detectaron **29 columnas con valores faltantes**, con proporciones variables:
- Variables con >90% de missing: `Pool QC`, `Misc Feature`, `Alley`.
- Variables con missing moderado (5‚Äì10%): `Year Built`, `Garage Type`, `Garage Area`.
- Variables num√©ricas cr√≠ticas como `SalePrice` (‚âà12% faltante) fueron parcialmente eliminadas para simular *MNAR* (faltantes no aleatorios).}

Se elaboraron gr√°ficos de:
- **Top 10 columnas con mayor porcentaje de missing**, destacando `Pool QC` y `Misc Feature` (>95%).
- **Distribuci√≥n de missing por fila**, que mostr√≥ que la mayor√≠a de las filas tienen entre 0 y 5 valores faltantes, con pocos casos extremos.

Estas visualizaciones confirmaron que el problema no est√° concentrado en pocas observaciones, sino distribuido entre distintas columnas.

![](results/visualizaciones/top-10-porcentaje-missing.png)

![](results/visualizaciones/missing_patterns.png)

---

### 2. Clasificaci√≥n del missing data
Se introdujo missing sint√©tico para replicar escenarios reales de p√©rdida de informaci√≥n:
- **MCAR (Missing Completely At Random)** ‚Üí `Year Built`: los faltantes no dependen de otras variables ni del propio valor.
- **MAR (Missing At Random)** ‚Üí `Garage Area`: los faltantes dependen de una variable observable (`Garage Type`).
- **MNAR (Missing Not At Random)** ‚Üí `SalePrice`: los faltantes se asocian con precios altos, reflejando sesgo en la no respuesta.

Esta diferenciaci√≥n fue clave para definir **estrategias de imputaci√≥n adecuadas**.

---

### 4. An√°lisis de outliers
Se aplicaron dos enfoques complementarios:

| M√©todo | Criterio | Contexto de uso |
|--------|-----------|----------------|
| IQR | 1.5√órango intercuart√≠lico | Distribuciones sesgadas o con colas pesadas |
| Z-Score | ¬±3 desviaciones est√°ndar | Distribuciones aproximadamente normales |

**Resultados:**
- En promedio, un **2.9% de los registros** fueron considerados outliers por IQR.  
- Las variables con m√°s outliers fueron `Enclosed Porch`, `BsmtFin SF 2` y `Overall Cond`.
- La comparaci√≥n mostr√≥ que IQR detecta m√°s casos extremos que Z-Score, especialmente en variables sesgadas como `Lot Area` y `SalePrice`.

Se generaron boxplots con l√≠mites IQR y visualizaci√≥n logar√≠tmica para `Lot Area` y `SalePrice`, revelando alta asimetr√≠a.

![](results/visualizaciones/outliers_analysis.png)

---

### 5. Estrategias de imputaci√≥n

Se ensayaron tres estrategias simples (media, mediana, moda), las tres logrando **0 valores faltantes restantes**, pero con implicancias diferentes en la distribuci√≥n.

Posteriormente, se implement√≥ una **imputaci√≥n inteligente (smart)** con reglas espec√≠ficas por tipo de variable:
- `Year Built`: mediana por *Neighborhood* y *House Style* (MAR).  
- `Garage Area`: imputaci√≥n condicional y creaci√≥n de *flag* binario para MNAR.  
- `SalePrice`: mediana por *Neighborhood*.  
- `Garage Type`: moda global.

Esta imputaci√≥n redujo la p√©rdida de informaci√≥n estructural, aunque persistieron **~15.591 valores imputados** en variables no cr√≠ticas.

---

### 6. Comparaci√≥n de distribuciones y correlaciones
Se compararon histogramas y conteos categ√≥ricos entre datos originales e imputados con la estrategia *SMART*.  
Las diferencias m√°s notorias se observaron en las variables num√©ricas sesgadas (`Lot Area`, `SalePrice`), aunque la estructura general se mantuvo.

![](results/visualizaciones/distribution_comparison.png)

El an√°lisis de correlaciones mostr√≥ **cambios menores (<0.2)** tras la imputaci√≥n, indicando que las relaciones entre variables clave (ej. `Overall Qual` ‚Üî `SalePrice`) se conservaron.

![](results/visualizaciones/correlation_comparison.png)

---

### 7. Pipeline reproducible
Se desarroll√≥ un **pipeline modular** con `scikit-learn` que integra:
- Imputaci√≥n (`SimpleImputer`),
- Escalamiento (`StandardScaler`),
- Codificaci√≥n categ√≥rica (`OneHotEncoder`).

```python
def create_cleaning_pipeline():
    """Crear pipeline de limpieza reproducible"""

    # Definir columnas num√©ricas y categ√≥ricas
    numeric_features = ['SalePrice', 'Lot Area', 'Year Built', 'Garage Area']
    categorical_features = ['Neighborhood', 'House Style', 'Garage Type']

    # Transformadores
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),  # estrategia de imputaci√≥n
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),  # estrategia de imputaci√≥n
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    # Combinar transformadores
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ]
    )

    return preprocessor
```

El resultado final fue una **matriz de 46 columnas transformadas**, lista para usarse en modelos predictivos sin intervenci√≥n manual.

---

## Reflexi√≥n

Este an√°lisis me demostr√≥ que el tratamiento del *missing data* no puede resolverse de forma gen√©rica; cada variable requiere entender su origen, relaci√≥n con otras y posible sesgo.

### Aprendizajes clave:

- Las imputaciones basadas en agrupaciones (por barrio o estilo de casa) preservan mejor la estructura interna de los datos.
- Los m√©todos robustos como IQR son preferibles frente a Z-Score cuando las distribuciones son asim√©tricas.

### Limitaciones:

- Algunas variables con >90% de missing (ej. `Pool QC`) son pr√°cticamente inutilizables.

---

## üìö Referencias
- Little, R. J. A., & Rubin, D. B. (2019). *Statistical Analysis with Missing Data*. Wiley.  
- Kaggle: [Ames Housing Dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)  
- Documentation: *scikit-learn Imputer & ColumnTransformer APIs*.  
- van Buuren, S. (2018). *Flexible Imputation of Missing Data*.
- Accede [aqu√≠](ames_housing.ipynb) al notebook con todo el c√≥digo

---
