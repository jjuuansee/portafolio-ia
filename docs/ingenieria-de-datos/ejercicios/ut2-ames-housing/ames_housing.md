---

title: "Feature Scaling y Leakage en Ames Housing"
date: 2025-10-12
author: "Juan Paroli"
categories: ["Feature Engineering", "Modeling", "Best Practices"]
tags: ["Scaling", "Outliers", "Pipelines", "Ames Housing", "PowerTransformer", "Data Leakage"]

---

# Feature Scaling y Leakage en Ames Housing

## Contexto

Esta pr√°ctica extiende el trabajo previo de calidad de datos en [Ames Housing](../ut2-missing-data-detection/missing_data.md) para centrarse en **escalado de features**, **detecci√≥n/tratamiento de outliers** y **prevenci√≥n de data leakage**.
Adem√°s, incluye una **investigaci√≥n avanzada** sobre `PowerTransformer (Yeo‚ÄìJohnson)` y su comparaci√≥n con los scalers cl√°sicos.

> Objetivo: construir un **pipeline honesto (anti-leakage)**, seleccionar transformaciones adecuadas seg√∫n la **distribuci√≥n** de cada variable y demostrar su impacto en la **performance**.

---

## Objetivos

* [x] Diagnosticar **escalas dispares** y **outliers** que afecten algoritmos sensibles a distancia.
* [x] Comparar `StandardScaler`, `MinMaxScaler`, `RobustScaler` vs `PowerTransformer`.
* [x] Demostrar **data leakage** con tres estrategias (incorrecta/correcta/pipeline).
* [x] Validar con **cross-validation** y baseline para medir valor real.

---

## Desarrollo

* Dataset: **Ames Housing** (2930 filas, 82 columnas).
* *Missing* sint√©tico agregado en la [pr√°ctica previa](../ut2-missing-data-detection/missing_data.md) (MCAR/MAR/MNAR) y luego guardado en `df_imputed` (cero NaN restantes con reglas simples + ‚Äúsmart‚Äù por vecindario/estilo/garage).

### 1. Exploraci√≥n inicial

Comenzamos analizando las escalas de las variables mediante un boxplot que apliga la transformacion *log1p*.

![](results/top5-escalas.png)

Estas escalas se dan ya que estas variables tienen rangos enormes comparadas con otras.

- `PID`: va de 5.26e+08 a 1.00e+09 (escala de cientos de millones).

- `Lot Area`: va de 1300 a 215,245 (rango muy grande).

- `Mas Vnr Area`: de 0 a 1600, mientras que muchas otras est√°n entre 1‚Äì10.

- `Year Built` y `Year Remod/Add`: rangos de ~100 a√±os (1872‚Äì2010), mucho mayores que escalas ordinales (1‚Äì10).

- `Order`: de 1 a 2930, tambi√©n m√°s grande que calificaciones como Overall Qual (1‚Äì10).

---

### 2. Outliers

Para detectar los outlier se utilizaron dos enfoques:

| M√©todo            | Cu√°ndo usar                          |
| ----------------- | ------------------------------------ |
| **IQR (1.5√óIQR)** | Distribuciones sesgadas/colas largas |
| **Z-Score (¬±3œÉ)** | Distribuciones ~normales             |

**Resultados clave**

* Por IQR, `Lot Area` tuvo **127** outliers (‚âà4.3%).
* Por Z-Score, `Lot Area` tuvo **29** outliers (‚âà1.0%).
* En el barrido completo, el % promedio de outliers por IQR fue ‚âà **2.94%**; variables como `Enclosed Porch` y `Screen Porch` concentran muchos ceros (l√≠mites IQR en 0), elevando conteos.

Entre las aplicaciones de `StandardScaler`, `MinMaxScaler` y `RobustScaler` no cambi√≥ la detecci√≥n de outliers ya que dieron la misma cantidad para los 3 m√©todos como se ve en la siguiente secci√≥n.

---

### 3. Escalado

Se prob√≥ el efecto del escalado en la detecci√≥n de outliers (con `Lot Area`):

| Escaler            | IQR (conteo) | Z-Score (conteo) |
| ------------------ | -----------: | ---------------: |
| **StandardScaler** |          127 |               29 |
| **MinMaxScaler**   |          127 |               29 |
| **RobustScaler**   |          127 |               29 |

---

### PowerTransformer (Yeo‚ÄìJohnson)

PowerTransformer corrige la asimetr√≠a de las distribuciones. Siendo √∫til en el dataset actual.

**Antes (skew | kurtosis)**

* `SalePrice`: 1.44 | 6.18
* `Lot Area`: 12.82 | 265.02
* `Misc Val`: 22.00 | 566.20
* `Total Bsmt SF`: 1.16 | 9.14

**Despu√©s con PowerTransformer (YJ, standardize=True)**

* `SalePrice__PT`: 0.08 | 2.21
* `Lot Area__PT`: 0.10 | 5.22
* `Misc Val__PT`: 5.05 | 23.53 
* `Total Bsmt SF__PT`: 0.11 | 4.09

![](results/comparativa_skew.png)

---

### 5. Data leakage

Tres m√©todos con `KNeighborsRegressor (k=5)`:

| M√©todo                                         | ¬øHay leakage? |         R¬≤ |  MAE (USD) |
| ---------------------------------------------- | ------------- | ---------: | ---------: |
| **1. Escalar todo y luego split**              | **S√≠**        |     0.1846 |     36,914 |
| **2. Split ‚Üí fit scaler en train ‚Üí transform** | No            | **0.1957** | **36,443** |
| **3. Pipeline (Scaler‚ÜíModelo)**                | No            | **0.1957** | **36,443** |

El m√©todo 1 ‚Äúfiltra‚Äù informaci√≥n del test al train (medias/desv√≠os).
El **Pipeline** (3) automatiza el orden correcto y es el est√°ndar para **evitar errores** y usar **cross-validation** sin fugas.

Baseline (Dummy median, test): R¬≤ = ‚àí0.0443; MAE ‚âà 39,416.

---

### 6. Validaci√≥n final (CV=5)

**Pipeline ganador:** `PowerTransformer(YJ) ‚Üí KNN (k=5)`

* R¬≤ (folds): `[0.0340, 0.1525, 0.1490, 0.0223, 0.2254]` ‚Üí **0.1166 ¬± 0.0773**
* MAE (folds): `[38,486; 33,607; 30,843; 34,818; 32,192]` ‚Üí **33,989 ¬± 2,615**

**Baseline (Dummy median, CV=5)**

* R¬≤: **‚àí0.0248 ¬± 0.0248**
* MAE: **35,194 ¬± 2,027**

> El pipeline con `PowerTransformer` brinda **R¬≤ positivo** sostenido y mejora el MAE vs. baseline (~**1,200 USD** menos en media), aunque el problema con s√≥lo 3 features sigue siendo desafiante (resultado *modesto pero real*).

---

## Reflexi√≥n

**Aprendizajes clave**

* **Detectar y tratar outliers antes** del escalado evita distorsionar medias/desv√≠os y rangos; luego aplicar escalado/transformaci√≥n.
* Los **scalers lineales** (Standard/MinMax/Robust) **no corrigen** asimetr√≠a; `PowerTransformer` s√≠.
* **Pipeline + CV** es **obligatorio** para evitar leakage y obtener m√©tricas honestas.
* En variables como `Lot Area` o `SalePrice`, las **colas largas** justifican transformaciones no lineales.

**Limitaciones**

* `Misc Val` presenta **masa en 0** por lo tanto persiste asimetr√≠a aun con Yeo-Johnson.
* El experimento de modelado us√≥ **pocas features** (demostraci√≥n). Un modelo final deber√≠a incorporar m√°s se√±ales (calidad, metros cubiertos, barrio, interacci√≥n, etc.).

---

## üìö Referencias

* Documentaci√≥n `scikit-learn`: *Preprocessing (scalers, PowerTransformer), Pipeline, Model Selection*.
* Box-Cox & Yeo‚ÄìJohnson: papers originales y notas de sklearn (para f√≥rmulas y supuestos).
* Dataset: *Ames Housing* (Kaggle).

---
