::::::::::::::::::::::::::: {.md-main role="main" md-component="main"}
:::::::::::::::::::::::::: {.md-main__inner .md-grid}
::::: {.md-sidebar .md-sidebar--primary md-component="sidebar" md-type="navigation" style="top: 48px;"}
:::: {.md-sidebar__scrollwrap style="height: 468px;"}
::: md-sidebar__inner
[![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjAgMCAyNCAyNCI+PHBhdGggZD0iTTEyIDhhMyAzIDAgMCAwIDMtMyAzIDMgMCAwIDAtMy0zIDMgMyAwIDAgMC0zIDMgMyAzIDAgMCAwIDMgM20wIDMuNTRDOS42NCA5LjM1IDYuNSA4IDMgOHYxMWMzLjUgMCA2LjY0IDEuMzUgOSAzLjU0IDIuMzYtMi4xOSA1LjUtMy41NCA5LTMuNTRWOGMtMy41IDAtNi42NCAxLjM1LTkgMy41NCIgLz48L3N2Zz4=)](https://juanfkurucz.com/ucu-id/ "IngenierÃ­a de Datos - Universidad CatÃ³lica del Uruguay"){.md-nav__button
.md-logo
aria-label="IngenierÃ­a de Datos - Universidad CatÃ³lica del Uruguay"
md-component="logo"} IngenierÃ­a de Datos - Universidad CatÃ³lica del
Uruguay

- [[ Inicio
  ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/){.md-nav__link}
- [[ UT1
  ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut1/){.md-nav__link}
- [[ UT2
  ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/){.md-nav__link}
- [[ Recursos
  ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/recursos/){.md-nav__link}
:::
::::
:::::

::::: {.md-sidebar .md-sidebar--secondary md-component="sidebar" md-type="toc" style="top: 48px;"}
:::: {.md-sidebar__scrollwrap style="height: 468px;"}
::: md-sidebar__inner
[]{.md-nav__icon .md-icon} Tabla de contenidos

- [[ ğŸ¯ Objetivos BÃ¡sicos
  ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#objetivos-basicos){.md-nav__link
  .md-nav__link--passed}
- [[ ğŸ“‹ Lo que necesitas saber ANTES de empezar
  ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#lo-que-necesitas-saber-antes-de-empezar){.md-nav__link
  .md-nav__link--passed}
  - [[ ğŸ” Parte 1: Setup y Carga de Datos
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#parte-1-setup-y-carga-de-datos){.md-nav__link
    .md-nav__link--passed}
  - [[ ğŸ  Paso 2: Cargar y Crear Missing Data SintÃ©tico
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-2-cargar-y-crear-missing-data-sintetico){.md-nav__link
    .md-nav__link--passed}
  - [[ ğŸ“Š Paso 3: AnÃ¡lisis Inicial del Dataset
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-3-analisis-inicial-del-dataset){.md-nav__link
    .md-nav__link--passed}
  - [[ ğŸ” Paso 4: DetecciÃ³n de Patrones de Missing Data
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-4-deteccion-de-patrones-de-missing-data){.md-nav__link
    .md-nav__link--passed}
  - [[ ğŸ§  Paso 5: Clasificar Tipos de Missing Data
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-5-clasificar-tipos-de-missing-data){.md-nav__link
    .md-nav__link--passed}
  - [[ ğŸš¨ Paso 6: DetecciÃ³n de Outliers
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-6-deteccion-de-outliers){.md-nav__link
    .md-nav__link--passed}
  - [[ ğŸ” Paso 7: DetecciÃ³n de Outliers con Z-Score
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-7-deteccion-de-outliers-con-z-score){.md-nav__link
    .md-nav__link--passed}
  - [[ ğŸ“Š Paso 8: VisualizaciÃ³n de Outliers
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-8-visualizacion-de-outliers){.md-nav__link
    .md-nav__link--passed}
  - [[ ğŸ”§ Paso 9: Estrategias de ImputaciÃ³n
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-9-estrategias-de-imputacion){.md-nav__link
    .md-nav__link--passed}
  - [[ ğŸ§  Paso 10: ImputaciÃ³n Inteligente por Tipo de Missing
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-10-imputacion-inteligente-por-tipo-de-missing){.md-nav__link
    .md-nav__link--passed}
  - [[ ğŸš« Paso 11: Anti-Leakage BÃ¡sico
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-11-anti-leakage-basico){.md-nav__link
    .md-nav__link--passed}
  - [[ ğŸ“Š Paso 12: AnÃ¡lisis de Impacto de la ImputaciÃ³n
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-12-analisis-de-impacto-de-la-imputacion){.md-nav__link
    .md-nav__link--passed}
  - [[ ğŸ”§ Paso 13: Pipeline de Limpieza Reproducible
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-13-pipeline-de-limpieza-reproducible){.md-nav__link
    .md-nav__link--passed}
- [[ ğŸ¯ Parte B: AnÃ¡lisis CrÃ­tico
  ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#parte-b-analisis-critico){.md-nav__link
  .md-nav__link--passed}
  - [[ ğŸ“ Preguntas de ReflexiÃ³n
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#preguntas-de-reflexion){.md-nav__link
    .md-nav__link--passed}
- [[ ğŸš€ Sugerencias para Explorar MÃ¡s
  ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#sugerencias-para-explorar-mas){.md-nav__link
  .md-nav__link--passed}
  - [[ ğŸ“Š Otros Datasets Interesantes
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#otros-datasets-interesantes){.md-nav__link
    .md-nav__link--passed .md-nav__link--active}
  - [[ ğŸ”§ Algoritmos Avanzados de ImputaciÃ³n
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#algoritmos-avanzados-de-imputacion){.md-nav__link}
  - [[ ğŸ¯ TÃ©cnicas de DetecciÃ³n de Outliers Avanzadas
    ]{.md-ellipsis}](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#tecnicas-de-deteccion-de-outliers-avanzadas){.md-nav__link}
:::
::::
:::::

::::::::::::::::::: {.md-content md-component="content"}
# ğŸ” PrÃ¡ctica 5: Missing Data Detective - Fill in the Blanks[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#practica-5-missing-data-detective-fill-in-the-blanks "Permanent link"){.headerlink} {#practica-5-missing-data-detective-fill-in-the-blanks}

**UT2: Calidad & Ã‰tica** \| **PrÃ¡ctica Guiada**

## ğŸ¯ Objetivos BÃ¡sicos[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#objetivos-basicos "Permanent link"){.headerlink} {#objetivos-basicos}

- Aprender a detectar y analizar datos faltantes (MCAR, MAR, MNAR)
- Identificar outliers usando mÃ©todos estadÃ­sticos
- Implementar estrategias de imputaciÃ³n apropiadas
- Crear pipelines de limpieza reproducibles
- Considerar aspectos Ã©ticos en el tratamiento de datos

## ğŸ“‹ Lo que necesitas saber ANTES de empezar[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#lo-que-necesitas-saber-antes-de-empezar "Permanent link"){.headerlink}

- Conceptos bÃ¡sicos de pandas y visualizaciÃ³n
- Idea general de quÃ© son los datos faltantes
- Curiosidad por entender patrones en la calidad de datos

------------------------------------------------------------------------

### ğŸ” Parte 1: Setup y Carga de Datos[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#parte-1-setup-y-carga-de-datos "Permanent link"){.headerlink}

**ğŸ“‹ CONTEXTO DE NEGOCIO (CRISP-DM: Business Understanding)**

ğŸ”— **Referencias oficiales:**

- [Kaggle Data Cleaning - Handling Missing
  Values](https://www.kaggle.com/learn/data-cleaning/lesson/1)
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [Matplotlib
  Documentation](https://matplotlib.org/stable/contents.html)
- [Seaborn Documentation](https://seaborn.pydata.org/)
- [Scikit-learn Documentation](https://scikit-learn.org/stable/)

**ğŸ  Caso de negocio:**

- **Problema**: El dataset Ames Housing tiene datos faltantes y outliers
  que afectan las predicciones de precios
- **Objetivo**: Detectar patrones de missing data y outliers para
  limpiar el dataset
- **Variables**: SalePrice, LotArea, YearBuilt, GarageArea,
  Neighborhood, HouseStyle, etc.
- **Valor para el negocio**: Datos mÃ¡s limpios = predicciones de precios
  mÃ¡s confiables

::: highlight
## 0. Setup bÃ¡sico
### Modulos necesarios
```python
# === SETUP DEL ENTORNO ===

# 1. Importar librerÃ­as necesarias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import warnings
from pandas.api.types import is_numeric_dtype
warnings.filterwarnings('ignore')

print("âœ… Todas las librerÃ­as importadas correctamente")

# 3. Configurar visualizaciones
plt.style.use('seaborn-v0_8')  # estilo visual (ej: 'seaborn-v0_8', 'default', 'classic')
sns.set_palette("husl")  # paleta de colores (ej: 'husl', 'Set1', 'viridis')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12

print("ğŸ¨ ConfiguraciÃ³n de visualizaciones lista!")
```
:::

**ğŸ’¡ PISTAS:**

- ğŸ¨ [Estilos de
  matplotlib](https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html)
- ğŸŒˆ [Paletas de
  seaborn](https://seaborn.pydata.org/tutorial/color_palettes.html)
- ğŸ’­ Â¿QuÃ© estilo se ve mÃ¡s profesional para anÃ¡lisis de datos?
### Dataset
```python
# === CARGAR DATASET AMES HOUSING ===

# 1. Cargar dataset base
!curl -L -o ames-housing-dataset.zip https://www.kaggle.com/api/v1/datasets/download/shashanknecrothapa/ames-housing-dataset
!unzip ames-housing-dataset.zip
df = pd.read_csv('AmesHousing.csv')


print("ğŸ  DATASET: Ames Housing")
print(f"   ğŸ“Š Forma original: {df.shape}")
print(f"   ğŸ“‹ Columnas: {list(df.columns)}")

# 2. Crear missing data sintÃ©tico para prÃ¡ctica
np.random.seed(42)  # para reproducibilidad

# Simular MCAR en Year Built (8% missing aleatorio)
# "Los valores faltan al azar: que falte un Year Built no depende de la edad ni del propio Year Built"
missing_year = np.random.random(len(df)) < 0.08
df.loc[missing_year, 'Year Built'] = np.nan

# Simular MAR en Garage Area (missing relacionado con Garage Type)
# "Los faltantes de Garage Area se concentran en ciertos tipos de garaje (variable observada)"
df.loc[df['Garage Type'] == 'None', 'Garage Area'] = df.loc[df['Garage Type'] == 'None', 'Garage Area'].sample(frac=0.7, random_state=42)

# Simular MNAR en SalePrice (missing relacionado con precio alto)
# "Los faltantes dependen del propio valor: quienes tienen precios altos no reportan precio"
high_price = df['SalePrice'] > df['SalePrice'].quantile(0.85)
df.loc[high_price, 'SalePrice'] = df.loc[high_price, 'SalePrice'].sample(frac=0.2, random_state=42)

print("\nğŸ” Missing data sintÃ©tico creado:")
print(df._____().sum())  # mÃ©todo para contar valores faltantes por columna
```
:::

**ğŸ’¡ PISTAS:**

- â“ Â¿QuÃ© valor se usa para representar datos faltantes en pandas?
  (np.nan, None, \'missing\')
- ğŸ”¢ Â¿QuÃ© mÃ©todo cuenta valores faltantes?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isnull.html)

## 1. AnÃ¡lisis exploratorio bÃ¡sico

```python
# === EXPLORACIÃ“N BÃSICA ===

# 1. InformaciÃ³n general del dataset
print("=== INFORMACIÃ“N GENERAL ===")
print(df.info())  # mÃ©todo que muestra tipos de datos, memoria y valores no nulos

# 2. EstadÃ­sticas descriptivas
print("\n=== ESTADÃSTICAS DESCRIPTIVAS ===")
print(df.describe())  # mÃ©todo que calcula estadÃ­sticas descriptivas

# 3. Tipos de datos
print("\n=== TIPOS DE DATOS ===")
print(df.dtypes)  # atributo que muestra tipos de datos por columna

# 4. Verificar missing data
print("\n=== MISSING DATA POR COLUMNA ===")
missing_count = df.isnull().sum()  # contar valores faltantes
missing_pct = (missing_count / len(df)) * 100  # calcular porcentaje

missing_stats = pd.DataFrame({
    'Column': df.columns,
    'Missing_Count': missing_count,
    'Missing_Percentage': missing_pct
})
print(missing_stats[missing_stats['Missing_Count'] > 0])

# 5. AnÃ¡lisis de memoria
print("\n=== ANÃLISIS DE MEMORIA ===")
total_bytes = df.memory_usage(deep=True).sum()  # mÃ©todo para memoria en bytes
print(f"Memoria total del DataFrame: {total_bytes / (1024**2):.2f} MB")
print(f"Memoria por columna:")
for col in df.columns:
    memory_usage = df[col].memory_usage(deep=True)  # mÃ©todo para memoria de una columna
    print(f"  {col}: {memory_usage / 1024:.2f} KB")

# 6. AnÃ¡lisis de duplicados
print("\n=== ANÃLISIS DE DUPLICADOS ===")
duplicates = df.duplicated()  # mÃ©todo para detectar filas duplicadas
print(f"NÃºmero de filas duplicadas: {duplicates.sum()}")
if duplicates.sum() > 0:
    print("Primeras 5 filas duplicadas:")
    print(df[df.filter()].head())  # mÃ©todo para filtrar duplicados
```
:::

**ğŸ’¡ PISTAS:**

- â„¹ï¸ Â¿QuÃ© mÃ©todo da informaciÃ³n sobre tipos y memoria?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html)
- ğŸ“Š Â¿QuÃ© mÃ©todo calcula estadÃ­sticas descriptivas?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html)
- ğŸ”¢ Â¿QuÃ© atributo muestra tipos de datos?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.dtypes.html)
- â“ Â¿QuÃ© mÃ©todo cuenta valores faltantes?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isnull.html)
- ğŸ’¾ Â¿QuÃ© mÃ©todo calcula uso de memoria?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.memory_usage.html)
- ğŸ” Â¿QuÃ© mÃ©todo detecta filas duplicadas?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html)

------------------------------------------------------------------------

### ğŸ” **Paso 4: DetecciÃ³n de Patrones de Missing Data**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-4-deteccion-de-patrones-de-missing-data "Permanent link"){.headerlink} {#paso-4-deteccion-de-patrones-de-missing-data}

::: highlight
```python
# === ANÃLISIS DE PATRONES DE MISSING DATA ===

# 1. Filtrar solo columnas con missing data para visualizaciÃ³n
missing_columns = df.columns[df.isnull().any()].tolist()  # mÃ©todo para detectar missing
print(f"Columnas con missing data: {len(missing_columns)}")
print(f"Columnas: {missing_columns}")

# 2. VisualizaciÃ³n mejorada sin missingno
plt.subplot(1, 1, 1)
if len(missing_columns) > 0:
    # Crear estadÃ­sticas de missing solo para columnas con missing data
    missing_count = df[missing_columns].isnull().sum()  # mÃ©todo para contar missing
    missing_pct = (missing_count / len(df)) * 100  # calcular porcentaje

    missing_stats_filtered = pd.DataFrame({
        'Column': missing_columns,
        'Missing_Count': missing_count,
        'Missing_Percentage': missing_pct
    }).sort_values('Missing_Percentage', ascending=False).head(10)

    # Crear grÃ¡fico de barras mÃ¡s limpio
    bars = plt.bar(range(len(missing_stats_filtered)), missing_stats_filtered['Missing_Percentage'], 
                   color='steelblue', alpha=0.7, edgecolor='black', linewidth=0.5)  # funciÃ³n para barras
    plt.title('Top 10: Porcentaje de Missing por Columna', fontsize=14, fontweight='bold')
    plt.xticks(range(len(missing_stats_filtered)), missing_stats_filtered['Column'], 
               rotation=45, ha='right')  # funciÃ³n para etiquetas del eje X

    plt.ylabel('Porcentaje de Missing (%)')
    plt.grid(True, alpha=0.3, axis='y')

    # Agregar valores en las barras
    for i, bar in enumerate(bars):
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,
                f'{height:.1f}%', ha='center', va='bottom', fontsize=10)
else:
    plt.text(0.5, 0.5, 'No hay missing data', ha='center', va='center', fontsize=16)
    plt.title('Porcentaje de Missing por Columna', fontsize=14, fontweight='bold')

# DistribuciÃ³n de missing por fila
plt.show()
plt.subplot(1, 1, 1)
missing_per_row = df.isnull().sum(axis=1)  # contar missing por fila
plt.hist(missing_per_row, bins=range(0, missing_per_row.max()+2), alpha=0.7, 
         edgecolor='black', color='lightcoral')  # funciÃ³n para histograma
plt.title('DistribuciÃ³n de Missing por Fila', fontsize=14, fontweight='bold')
plt.xlabel('NÃºmero de valores faltantes por fila')
plt.ylabel('Frecuencia')
plt.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
!mkdir -p results/visualizaciones
plt.savefig('results/visualizaciones/missing_patterns.png', dpi=300, bbox_inches='tight')
plt.show()
```
:::

**ğŸ’¡ PISTAS:**

- â“ Â¿QuÃ© mÃ©todo detecta valores faltantes?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isnull.html)
- ğŸ“Š Â¿QuÃ© funciÃ³n de matplotlib crea barras?
  [DocumentaciÃ³n](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.bar.html)
- ğŸ·ï¸ Â¿QuÃ© funciÃ³n establece etiquetas del eje X?
  [DocumentaciÃ³n](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xticks.html)
- ğŸ“ˆ Â¿QuÃ© funciÃ³n crea histogramas?
  [DocumentaciÃ³n](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html)
- ğŸ”¢ Â¿CÃ³mo contar missing por fila? (axis=1 para filas, axis=0 para
  columnas)

### ğŸ§  **Paso 5: Clasificar Tipos de Missing Data**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-5-clasificar-tipos-de-missing-data "Permanent link"){.headerlink}

::: highlight
```python
# === CLASIFICACIÃ“N MCAR/MAR/MNAR ===

print("=== ANÃLISIS DE TIPOS DE MISSING ===")

# 1. Year Built: Â¿MCAR o MAR?
print("\n1. YEAR BUILT - AnÃ¡lisis de patrones:")
year_missing = df['Year Built'].isnull()  # mÃ©todo para detectar missing
print("Missing Year Built por Neighborhood:")
print(df.groupby('Neighborhood')['Year Built'].apply(lambda x: x.isnull().sum()))  # contar missing por grupo

print("Missing Year Built por House Style:")
print(df.groupby('House Style')['Year Built'].apply(lambda x: x.isnull().sum()))

# 2. Garage Area: Â¿MAR?
print("\n2. GARAGE AREA - AnÃ¡lisis de patrones:")
print("Missing Garage Area por Garage Type:")
print(df.groupby('Garage Type')['Garage Area'].apply(lambda x: x.isnull().sum()))

# 3. SalePrice: Â¿MNAR?
print("\n3. SALEPRICE - AnÃ¡lisis de patrones:")
price_missing = df['SalePrice'].isnull()
print("Valores de SalePrice en registros con missing:")
print(df[price_missing]['SalePrice'].describe())  # estadÃ­sticas descriptivas
```
### ğŸš¨ **Paso 6: DetecciÃ³n de Outliers**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-6-deteccion-de-outliers "Permanent link"){.headerlink} {#paso-6-deteccion-de-outliers}

::: highlight
```python
# === DETECCIÃ“N DE OUTLIERS CON IQR ===
# "Detectar extremos usando mediana y cuartiles"
# "CuÃ¡ndo usar: distribuciones asimÃ©tricas / colas pesadas / presencia de outliers"
if "Year Built" in df.columns:
    df["Year Built"] = pd.to_numeric(df["Year Built"], errors="coerce")

# === DETECCIÃ“N DE OUTLIERS: IQR y Z-SCORE (robustas) ===
def detect_outliers_iqr(df, column, factor=1.5):
    """Outliers por IQR. Devuelve (df_outliers, lower, upper)."""
    x = pd.to_numeric(df[column], errors="coerce")
    x_no_na = x.dropna().astype(float).values
    if x_no_na.size == 0:
        # sin datos vÃ¡lidos
        return df.iloc[[]], np.nan, np.nan
    q1 = np.percentile(x_no_na, 25)
    q3 = np.percentile(x_no_na, 75)
    iqr = q3 - q1
    lower = q1 - factor * iqr
    upper = q3 + factor * iqr
    mask = (pd.to_numeric(df[column], errors="coerce") < lower) | (pd.to_numeric(df[column], errors="coerce") > upper)
    return df[mask], lower, upper

# Analizar outliers en columnas numÃ©ricas
numeric_columns = df.select_dtypes(include=[np.number]).columns  # mÃ©todo para seleccionar columnas numÃ©ricas
outlier_analysis = {}

for col in numeric_columns:
    if not df[col].isnull().all():  # mÃ©todo para verificar si hay missing data
        outliers, lower, upper = detect_outliers_iqr(df, col)
        outlier_analysis[col] = {
            'count': len(outliers),
            'percentage': (len(outliers) / len(df)) * 100,
            'lower_bound': lower,
            'upper_bound': upper
        }

outlier_df = pd.DataFrame(outlier_analysis).T
print("=== ANÃLISIS DE OUTLIERS (IQR) ===")
print("Ãštil cuando la distribuciÃ³n estÃ¡ chueca o con colas largas")
print(outlier_df)

# AnÃ¡lisis adicional de outliers
print("\n=== RESUMEN DE OUTLIERS ===")
total_outliers = outlier_df['count'].sum()  # mÃ©todo para sumar outliers
print(f"Total de outliers detectados: {total_outliers}")
print(f"Porcentaje promedio de outliers: {outlier_df['percentage'].mean():.2f}%")  # mÃ©todo para calcular media
print(f"Columna con mÃ¡s outliers: {outlier_df['count'].max()}")  # mÃ©todo para encontrar mÃ¡ximo
```
:::

### ğŸ” **Paso 7: DetecciÃ³n de Outliers con Z-Score**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-7-deteccion-de-outliers-con-z-score "Permanent link"){.headerlink} {#paso-7-deteccion-de-outliers-con-z-score}

::: highlight
```python
# === DETECCIÃ“N DE OUTLIERS CON Z-SCORE ===
# "CuÃ¡ndo usar: distribuciÃ³n aprox. campana y sin colas raras"
# "Regla: 3 pasos (desvios) desde el promedio = raro"

def detect_outliers_zscore(df, column, threshold=3):
    """Detectar outliers usando Z-Score - Regla: 3 desvios desde el promedio = raro"""
    from scipy import stats
    z_scores = np.abs(stats.zscore(df[column].dropna()))
    outlier_indices = df[column].dropna().index[z_scores > threshold]
    return df.loc[outlier_indices]

# Comparar mÃ©todos de detecciÃ³n
print("\n=== COMPARACIÃ“N DE MÃ‰TODOS DE DETECCIÃ“N ===")
for col in ['SalePrice', 'Lot Area', 'Year Built', 'Garage Area']:
    if col in df.columns and not df[col].isnull().all():
        iqr_outliers = detect_outliers_iqr(df, col)
        zscore_outliers = detect_outliers_zscore(df, col)

        print(f"\n{col}:")
        print(f"  IQR outliers: {len(iqr_outliers[0])} ({len(iqr_outliers[0])/len(df)*100:.1f}%)")
        print(f"  Z-Score outliers: {len(zscore_outliers)} ({len(zscore_outliers)/len(df)*100:.1f}%)")
```
:::

**ğŸ’¡ PISTAS:**

- ğŸ“Š **IQR**: Interquartile Range = Q3 - Q1
- ğŸš¨ **Outliers**: Valores fuera de \[Q1-1.5*IQR, Q3+1.5*IQR\]
- ğŸ”¢ Â¿QuÃ© mÃ©todo selecciona columnas numÃ©ricas?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html)
- â“ Â¿QuÃ© mÃ©todo verifica missing data?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.isnull.html)
- â• Â¿QuÃ© mÃ©todo suma valores?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.sum.html)
- ğŸ“Š Â¿QuÃ© mÃ©todo calcula la media?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.mean.html)
- ğŸ” Â¿QuÃ© mÃ©todo encuentra el mÃ¡ximo?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.idxmax.html)

### ğŸ“Š **Paso 8: VisualizaciÃ³n de Outliers**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-8-visualizacion-de-outliers "Permanent link"){.headerlink} {#paso-8-visualizacion-de-outliers}

::: highlight
```python
# === VISUALIZAR OUTLIERS ===
os.makedirs('results/visualizaciones', exist_ok=True)

cols = ['SalePrice', 'Lot Area', 'Year Built', 'Garage Area']

fig, axes = plt.subplots(2, 2, figsize=(15, 12))  # funciÃ³n para crear subplots
axes = axes.ravel()  # mÃ©todo para aplanar array

for i, col in enumerate(cols):
    if col not in df.columns:
        axes[i].set_visible(False)
        continue

    # convertir a numÃ©rico de forma segura
    y = pd.to_numeric(df[col], errors='coerce').dropna()

    if y.empty:
        axes[i].axis('off')
        axes[i].text(0.5, 0.5, f"{col}: sin datos numÃ©ricos", ha='center', va='center', fontsize=11)
        continue

    # Boxplot usando el vector numÃ©rico (evita inferencias de dtype de seaborn)
    sns.boxplot(y=y, ax=axes[i])  # funciÃ³n para boxplot
    axes[i].set_title(f'Outliers en {col}', fontweight='bold')
    axes[i].set_ylabel(col)

    # Outliers por IQR y bandas
    iqr_df, lower, upper = detect_outliers_iqr(df, col)
    out_vals = pd.to_numeric(iqr_df[col], errors='coerce').dropna()

    if np.isfinite(lower):
        axes[i].axhline(lower, linestyle='--', linewidth=1, label='LÃ­mite IQR')
    if np.isfinite(upper):
        axes[i].axhline(upper, linestyle='--', linewidth=1)

    # Marcar outliers con un leve jitter en X para que se vean
    if len(out_vals) > 0:
        jitter_x = np.random.normal(loc=0, scale=0.02, size=len(out_vals))
        axes[i].scatter(jitter_x, out_vals, alpha=0.6, s=50, label=f'Outliers ({len(out_vals)})') # funciÃ³n para scatter
        axes[i].legend()  # mÃ©todo para mostrar leyenda

    # Opcional: si la variable es muy sesgada, usar escala log
    if col in ['Lot Area', 'SalePrice'] and y.skew() > 1:
        axes[i].set_yscale('log')
        axes[i].set_title(f'Outliers en {col} (escala log)', fontweight='bold')

plt.tight_layout() # funciÃ³n para ajustar layout
plt.savefig('results/visualizaciones/outliers_analysis.png', dpi=300, bbox_inches='tight') # funciÃ³n para guardar
plt.show() # funciÃ³n para mostrar grÃ¡fico
```
:::

**ğŸ’¡ PISTAS:**

- ğŸ“ Â¿CÃ³mo crear directorios?
  [DocumentaciÃ³n](https://docs.python.org/3/library/os.html)
- ğŸ“Š Â¿QuÃ© funciÃ³n crea subplots?
  [DocumentaciÃ³n](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html)
- ğŸ”„ Â¿QuÃ© mÃ©todo aplana arrays?
  [DocumentaciÃ³n](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.ravel.html)
- ğŸ“¦ Â¿QuÃ© funciÃ³n crea boxplots?
  [DocumentaciÃ³n](https://seaborn.pydata.org/generated/seaborn.boxplot.html)
- ğŸ¯ Â¿QuÃ© funciÃ³n crea scatter plots?
  [DocumentaciÃ³n](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html)
- ğŸ·ï¸ Â¿QuÃ© mÃ©todo muestra leyendas?
  [DocumentaciÃ³n](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html)
- ğŸ“ Â¿QuÃ© funciÃ³n ajusta layout?
  [DocumentaciÃ³n](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.tight_layout.html)
- ğŸ’¾ Â¿QuÃ© funciÃ³n guarda grÃ¡ficos?
  [DocumentaciÃ³n](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.savefig.html)
- ğŸ‘ï¸ Â¿QuÃ© funciÃ³n muestra grÃ¡ficos?
  [DocumentaciÃ³n](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.show.html)
- ğŸ”´ Los puntos rojos son outliers detectados

------------------------------------------------------------------------

### ğŸ”§ **Paso 9: Estrategias de ImputaciÃ³n**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-9-estrategias-de-imputacion "Permanent link"){.headerlink} {#paso-9-estrategias-de-imputacion}

::: highlight
```python
# === IMPLEMENTAR ESTRATEGIAS DE IMPUTACIÃ“N ===
# "Rellenar no es gratis; hacelo columna a columna y documentÃ¡"
# "Num: mediana (si cola pesada) / media (si ~normal)"
# "Cat: moda o 'Unknown' (+ flag si sospecha MNAR)"

def impute_missing_data(df, strategy='median'):
    """Implementar diferentes estrategias de imputaciÃ³n - Reglas simples de la clase"""
    df_imputed = df.copy()

    for col in df.columns:
        if df[col].isnull().any():
            if df[col].dtype in ['int64', 'float64']:
                if strategy == 'mean':
                    df_imputed[col].fillna(df[col].mean(), inplace=True)  # imputar con media
                elif strategy == 'median':
                    df_imputed[col].fillna(df[col].median(), inplace=True)  # imputar con mediana
                elif strategy == 'mode':
                    df_imputed[col].fillna(df[col].mode()[0], inplace=True)  # imputar con moda
            else:
                # Para variables categÃ³ricas
                df_imputed[col].fillna(df[col].mode()[0], inplace=True)  # imputar con moda

    return df_imputed

# Probar diferentes estrategias
strategies = ['mean', 'median', 'mode']
imputed_datasets = {}

for strategy in strategies:
    imputed_datasets[strategy] = impute_missing_data(df, strategy)
    print(f"Estrategia {strategy}: {imputed_datasets[strategy].isnull().sum().sum()} missing values restantes")
```
:::

**ğŸ’¡ PISTAS:**

- ğŸ“Š Â¿QuÃ© mÃ©todo calcula la media?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.mean.html)
- ğŸ“Š Â¿QuÃ© mÃ©todo calcula la mediana?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.median.html)
- ğŸ“Š Â¿QuÃ© mÃ©todo calcula la moda?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.mode.html)
- ğŸ”§ Â¿QuÃ© mÃ©todo imputa valores faltantes?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html)

### ğŸ§  **Paso 10: ImputaciÃ³n Inteligente por Tipo de Missing**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-10-imputacion-inteligente-por-tipo-de-missing "Permanent link"){.headerlink} {#paso-10-imputacion-inteligente-por-tipo-de-missing}

::: highlight
``` {#__code_9}
def smart_imputation(df, *, impute_saleprice=True):
    """ImputaciÃ³n inteligente robusta a dtypes y NaN."""
    df_imputed = df.copy()

    # --- 0) Asegurar dtypes numÃ©ricos donde corresponde ---
    for c in ["Year Built", "Garage Area", "SalePrice"]:
        if c in df_imputed.columns:
            df_imputed[c] = pd._____(df_imputed[c], errors="coerce")  # to_numeric

    # --- 1) Year Built: mediana por (Neighborhood, House Style) -> Neighborhood -> global ---
    if {"Neighborhood", "House Style", "Year Built"}.issubset(df_imputed.columns):
        grp_med = df_imputed._____(["Neighborhood", "House Style"])["Year Built"]._____("median")  # groupby, transform
        df_imputed["Year Built"] = df_imputed["Year Built"]._____(grp_med)  # fillna

        nb_med = df_imputed.groupby("Neighborhood")["Year Built"].transform("median")
        df_imputed["Year Built"] = df_imputed["Year Built"].fillna(nb_med)

        df_imputed["Year Built"] = df_imputed["Year Built"].fillna(df_imputed["Year Built"]._____( ))  # median
        # AÃ±o entero nullable
        df_imputed["Year Built"] = df_imputed["Year Built"]._____( ).astype("Int64")  # round

    # --- 2) Garage Area: MNAR â†’ indicador + 0; resto por mediana del barrio ---
    if "Garage Area" in df_imputed.columns:
        df_imputed["GarageArea_was_na"] = df_imputed["Garage Area"]._____( ).astype("Int8")  # isna

        # Si hay "Garage Cars", usarlo para inferir "sin garaje" (0 Ã¡rea)
        if "Garage Cars" in df_imputed.columns:
            no_garage_mask = (df_imputed["Garage Cars"].fillna(0) == 0) & df_imputed["Garage Area"].isna()
            df_imputed.loc[no_garage_mask, "Garage Area"] = 0.0

        # Para los NaN restantes: mediana por Neighborhood, luego global
        if "Neighborhood" in df_imputed.columns:
            med_gar = df_imputed.groupby("Neighborhood")["Garage Area"].transform("median")
            df_imputed["Garage Area"] = df_imputed["Garage Area"].fillna(med_gar)

        df_imputed["Garage Area"] = df_imputed["Garage Area"].fillna(df_imputed["Garage Area"].median())

    # --- 3) SalePrice: mediana por Neighborhood (opcional) ---
    if impute_saleprice and {"Neighborhood", "SalePrice"}.issubset(df_imputed.columns):
        nb_price = df_imputed.groupby("Neighborhood")["SalePrice"].transform("median")
        df_imputed["SalePrice"] = df_imputed["SalePrice"]._____(nb_price)  # fillna
        df_imputed["SalePrice"] = df_imputed["SalePrice"].fillna(df_imputed["SalePrice"].median())

    # --- 4) Garage Type: moda global (MCAR); manejar categorÃ­as ---
    if "Garage Type" in df_imputed.columns:
        # evitar problemas si es category
        if pd.api.types._____(df_imputed["Garage Type"]):  # is_categorical_dtype
            df_imputed["Garage Type"] = df_imputed["Garage Type"].astype("object")
        mode_val = df_imputed["Garage Type"].dropna()._____( )  # mode
        fill_val = mode_val.iloc[0] if not mode_val.empty else "Unknown"
        df_imputed["Garage Type"] = df_imputed["Garage Type"]._____(fill_val)  # fillna

    return df_imputed

# Aplicar
df_smart_imputed = smart_imputation(df)
print("=== IMPUTACIÃ“N INTELIGENTE ===")
print(f"Missing restantes: {int(df_smart_imputed.isnull().sum().sum())}")
```
:::

**ğŸ’¡ PISTAS:**

- ğŸ”¢ Â¿QuÃ© funciÃ³n convierte una columna a numÃ©rico manejando errores?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html)
- ğŸ§© Â¿CÃ³mo agrupÃ¡s por columnas para calcular estadÃ­sticas por grupo?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)
- ğŸ”„ Â¿CÃ³mo propagÃ¡s el resultado de una agregaciÃ³n (p. ej., mediana) a
  cada fila del grupo?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.GroupBy.transform.html)
- ğŸ§ª Â¿QuÃ© mÃ©todo usÃ¡s para rellenar valores faltantes con un valor o
  serie?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html)
- ğŸ“Š Â¿QuÃ© mÃ©todo calcula la mediana de una serie?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.median.html)
- ğŸ§® Â¿CÃ³mo redondeÃ¡s valores en una serie antes de castear a entero
  anulable?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.round.html)
- â“ Â¿QuÃ© mÃ©todo detecta valores faltantes en una serie?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.isna.html)
- ğŸ“ˆ Â¿QuÃ© mÃ©todo obtiene la moda de una serie?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.mode.html)
- ğŸ·ï¸ Â¿CÃ³mo verificÃ¡s si una serie es de tipo categÃ³rico?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.api.types.is_categorical_dtype.html)

------------------------------------------------------------------------

### ğŸš« **Paso 11: Anti-Leakage BÃ¡sico**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-11-anti-leakage-basico "Permanent link"){.headerlink} {#paso-11-anti-leakage-basico}

::: highlight
``` {#__code_10}
# === ANTI-LEAKAGE BÃSICO ===
# "No espiÃ©s el examen: fit en TRAIN, transform en VALID/TEST"
# "Split: X_train / X_valid / X_test"
# "imputer.fit(X_train) â†’ transform al resto"

from sklearn.model_selection import train_test_split

# 1. Split de datos (ANTES de imputar)
X = df.drop('SalePrice', axis=1)  # features
y = df['SalePrice']  # target

X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

print("=== SPLIT DE DATOS ===")
print(f"Train: {X_train.shape[0]} registros")
print(f"Valid: {X_valid.shape[0]} registros") 
print(f"Test: {X_test.shape[0]} registros")

# 2. Imputar SOLO en train, luego transformar
from sklearn.impute import SimpleImputer

# Separar columnas numÃ©ricas y categÃ³ricas
numeric_columns = X_train.select_dtypes(include=[np.number]).columns.tolist()
categorical_columns = X_train.select_dtypes(include=['object']).columns.tolist()

print(f"Columnas numÃ©ricas: {len(numeric_columns)}")
print(f"Columnas categÃ³ricas: {len(categorical_columns)}")

# Crear imputers para cada tipo de dato
numeric_imputer = SimpleImputer(strategy='_____')  # estrategia para numÃ©ricas
categorical_imputer = SimpleImputer(strategy='_____')  # estrategia para categÃ³ricas

# Ajustar imputers SOLO con train
numeric_imputer._____(X_train[numeric_columns])  # ajustar numÃ©ricas
categorical_imputer._____(X_train[categorical_columns])  # ajustar categÃ³ricas

# Transformar todos los conjuntos
X_train_numeric = numeric_imputer._____(X_train[numeric_columns])  # transformar numÃ©ricas
X_train_categorical = categorical_imputer._____(X_train[categorical_columns])  # transformar categÃ³ricas

X_valid_numeric = numeric_imputer._____(X_valid[numeric_columns])
X_valid_categorical = categorical_imputer._____(X_valid[categorical_columns])

X_test_numeric = numeric_imputer._____(X_test[numeric_columns])
X_test_categorical = categorical_imputer._____(X_test[categorical_columns])

print("\nâœ… Anti-leakage aplicado: fit solo en train, transform en todo")
```
:::

**ğŸ’¡ PISTAS:**

- ğŸ”¢ Â¿QuÃ© estrategia usar para numÃ©ricas? (\'median\', \'mean\',
  \'most_frequent\')
- ğŸ“ Â¿QuÃ© estrategia usar para categÃ³ricas? (\'most_frequent\',
  \'constant\')
- ğŸ¯ Â¿QuÃ© mÃ©todo ajusta el imputer?
  [DocumentaciÃ³n](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)
- ğŸ”„ Â¿QuÃ© mÃ©todo aplica las transformaciones?
  [DocumentaciÃ³n](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html)
- ğŸ“Š Â¿QuÃ© mÃ©todo separa tipos de datos?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.select_dtypes.html)

------------------------------------------------------------------------

### ğŸ“Š **Paso 12: AnÃ¡lisis de Impacto de la ImputaciÃ³n**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-12-analisis-de-impacto-de-la-imputacion "Permanent link"){.headerlink} {#paso-12-analisis-de-impacto-de-la-imputacion}

::: highlight
``` {#__code_11}
# 1) Crear df_imputed con imputaciÃ³n simple, robusta a dtypes
df_imputed = df.copy()

for col in df.columns:
    s = df[col]
    # Si es numÃ©rica o puede convertirse a numÃ©rica, imputar mediana
    if is_numeric_dtype(s) or (s.dtype == "object"):
        s_num = pd.to_numeric(s, errors="coerce")
        if s_num.notna().any():
            df_imputed[col] = s_num.fillna(s_num._____()) # imputar numÃ©ricas con mediana
            continue
    # Caso categÃ³rico: imputar con moda (si existe), sino "Unknown"
    moda = s.dropna()._____() # imputar categÃ³ricas con moda
    fill_val = moda.iloc[0] if not moda.empty else "Unknown"
    df_imputed[col] = s.fillna(fill_val)

# 2) Comparar distribuciones (hist para numÃ©ricas, barras para categÃ³ricas)
fig, axes = plt.subplots(2, 3, figsize=(18, 12))
axes = axes.ravel()

cols_to_plot = ['SalePrice', 'Lot Area', 'Year Built', 'Garage Area', 'Neighborhood', 'House Style']
for i, col in enumerate(cols_to_plot):
    if col not in df.columns:
        axes[i].axis('off')
        axes[i].set_title(f'{col} no existe', fontweight='bold')
        continue

    s_orig = df[col]
    s_imp = df_imputed[col]

    # Intentar tratar como numÃ©rico (coerce) para decidir el tipo de grÃ¡fico
    s_orig_num = pd.to_numeric(s_orig, errors='coerce')
    s_imp_num = pd.to_numeric(s_imp, errors='coerce')

    if s_orig_num.notna().any() and s_imp_num.notna().any():
        # NUMÃ‰RICAS â†’ hist
        # Mismo rango/bins para una comparaciÃ³n justa
        data_combined = pd.concat([s_orig_num.dropna(), s_imp_num.dropna()])
        bins = np.histogram_bin_edges(data_combined, bins=20)

        axes[i].hist(s_orig_num.dropna(), bins=bins, alpha=0.9, label='Original',
                     color='steelblue', edgecolor='black')
        axes[i].hist(s_imp_num.dropna(), bins=bins, alpha=0.3, label='Imputado',
                     color='orange', edgecolor='black')

        # Si estÃ¡ muy sesgada, te puede servir escala log
        if col in ['Lot Area', 'SalePrice'] and s_orig_num.dropna().skew() > 1:
            axes[i].set_yscale('log')

    else:
        # CATEGÃ“RICAS â†’ barras (top-K categorÃ­as para legibilidad)
        K = 12
        vc_orig = s_orig.astype('object').fillna('Missing').value_counts().head(K)
        vc_imp  = s_imp.astype('object').fillna('Missing').value_counts().head(K)
        cats = list(dict.fromkeys(list(vc_orig.index) + list(vc_imp.index)))[:K]  # uniÃ³n ordenada

        vc_orig = vc_orig.reindex(cats, fill_value=0)
        vc_imp  = vc_imp.reindex(cats, fill_value=0)

        x = np.arange(len(cats))
        w = 0.4
        axes[i].bar(x - w/2, vc_orig.values, width=w, label='Original')
        axes[i].bar(x + w/2, vc_imp.values,  width=w, label='Imputado')
        axes[i].set_xticks(x)
        axes[i].set_xticklabels(cats, rotation=30, ha='right')

    axes[i].set_title(f'DistribuciÃ³n de {col}', fontweight='bold')
    axes[i].legend()
    axes[i].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('results/visualizaciones/distribution_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

# 3) Correlaciones (solo numÃ©ricas y con coerciÃ³n segura)
important_cols = ['SalePrice', 'Lot Area', 'Year Built', 'Garage Area', 'Overall Qual', 'Gr Liv Area', 'Total Bsmt SF']
available_cols = [c for c in important_cols if c in df.columns]
print(f"Columnas seleccionadas para correlaciones: {available_cols}")

df_num_original = df[available_cols].apply(pd.to_numeric, errors='coerce')
df_num_imputed  = df_imputed[available_cols].apply(pd.to_numeric, errors='coerce')

fig, axes = plt.subplots(1, 2, figsize=(16, 8))

corr_original = df_num_original._____(numeric_only=True)  # mÃ©todo para matriz de correlaciÃ³n
sns.heatmap(corr_original, annot=True, cmap='coolwarm', center=0, ax=axes[0],
            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})
axes[0].set_title('Correlaciones - Original', fontweight='bold', fontsize=14)

corr_imputed = df_num_imputed._____(numeric_only=True)  # mÃ©todo para matriz de correlaciÃ³n
sns.heatmap(corr_imputed, annot=True, cmap='coolwarm', center=0, ax=axes[1],
            square=True, fmt='.2f', cbar_kws={'shrink': 0.8})
axes[1].set_title('Correlaciones - Imputado', fontweight='bold', fontsize=14)

plt.tight_layout()
plt.savefig('results/visualizaciones/correlation_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

# 4) Diferencias en correlaciones
print("\n=== DIFERENCIAS EN CORRELACIONES ===")
corr_diff = corr_imputed - corr_original
print("Cambios en correlaciones (Imputado - Original):")
print(corr_diff.round(3))
```
:::

**ğŸ’¡ PISTAS:**

- ğŸ“Š Â¿QuÃ© mÃ©todo calcula la mediana?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.median.html)
- ğŸ“Š Â¿QuÃ© mÃ©todo calcula la moda?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.Series.mode.html)
- ğŸ“Š Â¿QuÃ© mÃ©todo calcula matriz de correlaciÃ³n?
  [DocumentaciÃ³n](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)
- ğŸ“ˆ Â¿QuÃ© funciÃ³n de seaborn crea heatmaps?
  [DocumentaciÃ³n](https://seaborn.pydata.org/generated/seaborn.heatmap.html)

------------------------------------------------------------------------

### ğŸ”§ **Paso 13: Pipeline de Limpieza Reproducible**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#paso-13-pipeline-de-limpieza-reproducible "Permanent link"){.headerlink}

::: highlight
``` {#__code_12}
# === CREAR PIPELINE CON SKLEARN ===

from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

def create_cleaning_pipeline():
    """Crear pipeline de limpieza reproducible"""

    # Definir columnas numÃ©ricas y categÃ³ricas
    numeric_features = ['SalePrice', 'Lot Area', 'Year Built', 'Garage Area']
    categorical_features = ['Neighborhood', 'House Style', 'Garage Type']

    # Transformadores
    numeric_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='_____')),  # estrategia de imputaciÃ³n
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='_____')),  # estrategia de imputaciÃ³n
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    # Combinar transformadores
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_transformer, numeric_features),
            ('cat', categorical_transformer, categorical_features)
        ]
    )

    return preprocessor

# Crear y probar pipeline
preprocessor = create_cleaning_pipeline()

# Aplicar pipeline
X_cleaned = preprocessor._____(df)  # mÃ©todo para aplicar transformaciones
print(f"Shape despuÃ©s del pipeline: {X_cleaned.shape}")
print(f"Tipo de datos: {type(X_cleaned)}")
```
:::

**ğŸ’¡ PISTAS:**

- ğŸ”§ Â¿QuÃ© estrategia usar para numÃ©ricas? (\'median\', \'mean\',
  \'most_frequent\')
- ğŸ”§ Â¿QuÃ© estrategia usar para categÃ³ricas? (\'most_frequent\',
  \'constant\')
- ğŸ”§ Â¿QuÃ© mÃ©todo aplica las transformaciones?
  [DocumentaciÃ³n](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)

------------------------------------------------------------------------

## ğŸ¯ **Parte B: AnÃ¡lisis CrÃ­tico**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#parte-b-analisis-critico "Permanent link"){.headerlink} {#parte-b-analisis-critico}

### ğŸ“ **Preguntas de ReflexiÃ³n**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#preguntas-de-reflexion "Permanent link"){.headerlink} {#preguntas-de-reflexion}

Responde estas preguntas en tu notebook:

1.  **Â¿QuÃ© tipo de missing data identificaste en cada columna?**
    Justifica tu clasificaciÃ³n.

2.  **Â¿Por quÃ© elegiste esas estrategias de imputaciÃ³n especÃ­ficas?**
    Â¿QuÃ© alternativas consideraste?

3.  **Â¿CÃ³mo podrÃ­an las decisiones de imputaciÃ³n afectar a diferentes
    grupos demogrÃ¡ficos?** (ej: barrios, tipos de vivienda)

4.  **Â¿QuÃ© informaciÃ³n adicional necesitarÃ­as para tomar mejores
    decisiones sobre los outliers?**

5.  **Â¿CÃ³mo garantizas que tu pipeline sea reproducible y
    transparente?**

------------------------------------------------------------------------

## ğŸš€ **Sugerencias para Explorar MÃ¡s**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#sugerencias-para-explorar-mas "Permanent link"){.headerlink} {#sugerencias-para-explorar-mas}

### ğŸ“Š **Otros Datasets Interesantes**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#otros-datasets-interesantes "Permanent link"){.headerlink}

**ğŸ¥ Datasets de Salud:** - [Heart Disease
Dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset) -
Missing data en variables mÃ©dicas - [COVID-19
Dataset](https://www.kaggle.com/datasets/sudalairajkumar/novel-corona-virus-2019-dataset) -
Datos temporales con missing values - [Medical Cost
Dataset](https://www.kaggle.com/datasets/mirichoi0218/insurance) -
Missing data en costos mÃ©dicos

**ğŸ’° Datasets Financieros:** - [Credit Card Fraud
Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud) -
Missing data en transacciones - [Loan Prediction
Dataset](https://www.kaggle.com/datasets/ninzaami/loan-predication) -
Missing data en informaciÃ³n crediticia - [Stock Market
Data](https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks) -
Missing data en precios histÃ³ricos

**ğŸŒ Datasets Ambientales:** - [Air Quality
Dataset](https://www.kaggle.com/datasets/rohanrao/air-quality-data-in-india) -
Missing data en mediciones ambientales - [Weather
Dataset](https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data) -
Missing data en temperaturas histÃ³ricas

### ğŸ”§ **Algoritmos Avanzados de ImputaciÃ³n**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#algoritmos-avanzados-de-imputacion "Permanent link"){.headerlink} {#algoritmos-avanzados-de-imputacion}

**ğŸ“ˆ ImputaciÃ³n EstadÃ­stica Avanzada:**

::: highlight
``` {#__code_13}
# KNN Imputation
from sklearn.impute import KNNImputer
knn_imputer = KNNImputer(n_neighbors=5)

# Iterative Imputation
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
iterative_imputer = IterativeImputer(random_state=42)

# Matrix Factorization
from sklearn.decomposition import TruncatedSVD
# Para datos categÃ³ricos con missing
```
:::

### ğŸ¯ **TÃ©cnicas de DetecciÃ³n de Outliers Avanzadas**[Â¶](https://juanfkurucz.com/ucu-id/ut2/05-missing-data-detective/#tecnicas-de-deteccion-de-outliers-avanzadas "Permanent link"){.headerlink} {#tecnicas-de-deteccion-de-outliers-avanzadas}

**ğŸ” MÃ©todos EstadÃ­sticos:**

::: highlight
``` {#__code_14}
# Isolation Forest
from sklearn.ensemble import IsolationForest
isolation_forest = IsolationForest(contamination=0.1)

# Local Outlier Factor (LOF)
from sklearn.neighbors import LocalOutlierFactor
lof = LocalOutlierFactor(n_neighbors=20)

# One-Class SVM
from sklearn.svm import OneClassSVM
one_class_svm = OneClassSVM(nu=0.1)
```
:::

**ğŸ“Š MÃ©todos de VisualizaciÃ³n:**

::: highlight
``` {#__code_15}
# DBSCAN Clustering
from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps=0.5, min_samples=5)

# PCA para detecciÃ³n de outliers
from sklearn.decomposition import PCA
pca = PCA(n_components=2)
```
:::

------------------------------------------------------------------------

*\"Los datos faltantes no son un problema tÃ©cnico, son una oportunidad
para entender mejor el mundo que representan.\"*

[ [
![](data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdib3g9IjAgMCAyNCAyNCI+PHBhdGggZD0iTTIxIDEzLjFjLS4xIDAtLjMuMS0uNC4ybC0xIDEgMi4xIDIuMSAxLTFjLjItLjIuMi0uNiAwLS44bC0xLjMtMS4zYy0uMS0uMS0uMi0uMi0uNC0uMm0tMS45IDEuOC02LjEgNlYyM2gyLjFsNi4xLTYuMXpNMTIuNSA3djUuMmw0IDIuNC0xIDFMMTEgMTNWN3pNMTEgMjEuOWMtNS4xLS41LTktNC44LTktOS45QzIgNi41IDYuNSAyIDEyIDJjNS4zIDAgOS42IDQuMSAxMCA5LjMtLjMtLjEtLjYtLjItMS0uMnMtLjcuMS0xIC4yQzE5LjYgNy4yIDE2LjIgNCAxMiA0Yy00LjQgMC04IDMuNi04IDggMCA0LjEgMy4xIDcuNSA3LjEgNy45bC0uMS4yeiIgLz48L3N2Zz4=)
]{.md-icon title="Ãšltima actualizaciÃ³n"} [10 de septiembre de
2025]{.git-revision-date-localized-plugin
.git-revision-date-localized-plugin-date
title="10 de septiembre de 2025 13:33:43 UTC"} ]{.md-source-file__fact}
:::::::::::::::::::
::::::::::::::::::::::::::
:::::::::::::::::::::::::::
