---
title: "Resumen Reflexivo UT1 â€” EDA & Fuentes"
date: 2025-12-02
author: "Juan Paroli"
---

# ğŸ“Š ReflexiÃ³n sobre UT1: EDA & Fuentes de Datos

## Â¿De quÃ© tratÃ³ esta unidad y quÃ© problemas buscaba resolver?

La **UT1** se centrÃ³ en desarrollar habilidades fundamentales para **entender datos antes de modelar**. En el mundo real, los datasets no vienen limpios ni organizados: hay nulos, duplicados, outliers, formatos inconsistentes y, muchas veces, necesitÃ¡s combinar informaciÃ³n de mÃºltiples fuentes. El problema central que esta unidad busca resolver es: **Â¿cÃ³mo convertir datos crudos y dispersos en informaciÃ³n Ãºtil para tomar decisiones?**

El **AnÃ¡lisis Exploratorio de Datos (EDA)** es esa primera conversaciÃ³n con el dataset: preguntarle quÃ© contiene, quÃ© le falta, quÃ© patrones esconde. No se trata solo de aplicar `.describe()` y hacer grÃ¡ficos, sino de desarrollar una **mirada crÃ­tica** sobre la calidad de los datos, las limitaciones y las oportunidades de anÃ¡lisis.

En mis palabras, esta unidad me enseÃ±Ã³ a **ser detective antes que cientÃ­fico**: primero investigar, cuestionar, limpiar y documentar, para despuÃ©s poder construir sobre bases sÃ³lidas. AprendÃ­ que el EDA no es un paso opcional, es el **cimiento** de cualquier proyecto de datos serio.

---

## Conceptos y tÃ©cnicas clave que incorporÃ©

### 1. **Trazabilidad de calidad de datos (Data Quality Checks)**

Uno de los aprendizajes mÃ¡s importantes fue entender que **no podÃ©s confiar en un dataset a primera vista**. Antes de cualquier anÃ¡lisis, necesitÃ¡s documentar:

- **Valores faltantes**: Â¿cuÃ¡ntos hay?, Â¿en quÃ© columnas?, Â¿por quÃ© faltan?
- **Duplicados**: Â¿hay registros repetidos que sesgan los conteos?
- **Outliers**: Â¿hay valores extremos legÃ­timos o errores de carga?

**Ejemplo del portafolio**: En el [anÃ¡lisis de Netflix](../ejercicios/ut1-netflix-data/netflix-data.md), encontrÃ© que `director` tenÃ­a **31.6%** de datos faltantes y habÃ­a **57 tÃ­tulos duplicados**. Esto impacta directamente en cualquier anÃ¡lisis de "directores mÃ¡s prolÃ­ficos" o "conteo de tÃ­tulos Ãºnicos". Documentar esto no solo me ayudÃ³ a entender las limitaciones del dataset, sino que tambiÃ©n me obligÃ³ a tomar decisiones conscientes sobre cÃ³mo manejar esos casos.

```python
# CÃ³digo que apliquÃ© consistentemente:
missing = df.isna().sum()
missing_pct = (df.isna().sum() / len(df) * 100)
duplicados = df.duplicated().sum()
```

Este chequeo sistemÃ¡tico se convirtiÃ³ en mi **checklist de entrada** para cualquier dataset nuevo.

---

### 2. **IntegraciÃ³n de fuentes heterogÃ©neas (Joins inteligentes)**

Otro concepto clave fue entender que **los datos interesantes rara vez vienen en una sola tabla**. En el mundo real, necesitÃ¡s combinar informaciÃ³n de CSVs, JSONs, APIs, bases de datos, etc. Y cada join tiene implicancias en tu anÃ¡lisis.

**Ejemplo del portafolio**: En la [prÃ¡ctica de NYC Taxis](../ejercicios/ut1-nyc-taxis/practica_4.md), integrÃ© **tres fuentes distintas**:

- **Trips** (3M registros en `.parquet`)
- **Zones** (CSV con informaciÃ³n geogrÃ¡fica)
- **Calendar** (JSON con dÃ­as especiales)

La decisiÃ³n de usar **LEFT JOIN** vs **INNER JOIN** fue crucial:

```python
# LEFT JOIN: mantengo todos los viajes aunque algunos no tengan zona asignada
trips_with_zones = trips.merge(zones, 
                                left_on='pulocationid', 
                                right_on='locationid', 
                                how='left')
```

Si hubiera usado `INNER JOIN`, habrÃ­a perdido **1,647 viajes** sin borough asignado. Esto me enseÃ±Ã³ que **cada tipo de join cuenta una historia diferente**, y tenÃ©s que elegir conscientemente cuÃ¡l querÃ©s contar.

---

### 3. **VisualizaciÃ³n con propÃ³sito (Data Storytelling)**

Hacer grÃ¡ficos es fÃ¡cil; hacer grÃ¡ficos que **comuniquen algo Ãºtil** es otra historia. AprendÃ­ a elegir el tipo de visualizaciÃ³n segÃºn lo que quiero mostrar:

- **Line plots** â†’ tendencias temporales (ej.: publicaciones de Netflix a lo largo del tiempo)
- **Bar plots** â†’ comparaciones categÃ³ricas (ej.: distribuciÃ³n de viajes por borough)
- **Heatmaps** â†’ correlaciones entre variables numÃ©ricas
- **Box/Violin plots** â†’ distribuciones y outliers

**Ejemplo del portafolio**: En Netflix, usÃ© un **line plot** para mostrar el pico de publicaciones en **2018 (1,063 tÃ­tulos)** y la caÃ­da abrupta en 2020. Este grÃ¡fico **cuenta una historia** que una tabla no podrÃ­a transmitir con la misma claridad.

```python
# VisualizaciÃ³n temporal con propÃ³sito:
sns.lineplot(data=netflix_por_aÃ±o, x='year', y='count')
plt.title("Tendencia de publicaciones en Netflix")
```

AdemÃ¡s, aprendÃ­ a **evitar grÃ¡ficos redundantes** (como pie + donut del mismo dato) y a priorizar la legibilidad sobre la complejidad.

---

## Â¿QuÃ© fue lo que mÃ¡s me costÃ³ y cÃ³mo lo destrabÃ©?

Lo que mÃ¡s me costÃ³ fue **manejar datasets grandes sin que se trabe todo**. En la prÃ¡ctica de NYC Taxis, trabajar con **3 millones de registros** hizo que mi primera versiÃ³n del notebook fuera extremadamente lenta. Cargar los datos tomaba minutos, y cada operaciÃ³n de groupby era una eternidad.

### Â¿CÃ³mo lo destrabÃ©?

1. **OptimizaciÃ³n de tipos de datos**: Me di cuenta de que pandas por defecto carga todo como `int64` o `float64`, incluso cuando no hace falta tanta precisiÃ³n. ConvertÃ­ columnas categÃ³ricas a `category` y reduje el tamaÃ±o en memoria:

```python
# Antes: 400MB en memoria
# DespuÃ©s de optimizar:
trips['borough'] = trips['borough'].astype('category')
trips['passenger_count'] = trips['passenger_count'].astype('int8')
# Ahorro ~8% de memoria
```

2. **Limpieza temprana**: En lugar de trabajar con 3M registros todo el tiempo, limpiÃ© datos faltantes crÃ­ticos al principio (~71K registros eliminados). Esto hizo que las operaciones posteriores fueran mÃ¡s rÃ¡pidas.

3. **Uso de `.parquet`**: AprendÃ­ que `.parquet` es mucho mÃ¡s eficiente que CSV para datasets grandes (compresiÃ³n + lectura rÃ¡pida con `fastparquet`).

4. **Google Colab con GPU**: Para anÃ¡lisis pesados, migrÃ© a Colab, que tiene mÃ¡s RAM que mi laptop.

**LecciÃ³n clave**: No se trata solo de *quÃ©* analizÃ¡s, sino de *cÃ³mo* lo analizÃ¡s. La eficiencia importa cuando escalÃ¡s.

---

## Una tarea en detalle: NYC Taxis â€” IntegraciÃ³n de mÃºltiples fuentes

### Â¿QuÃ© hice?

En esta prÃ¡ctica, integrÃ© **tres fuentes de datos distintas** para analizar el sistema de taxis de NYC:

1. **Trips** (~3M registros en `.parquet`): viajes con timestamps, distancias, tarifas, propinas.
2. **Zones** (CSV): mapeo de `location_id` â†’ `borough` + `zone`.
3. **Calendar** (JSON): dÃ­as especiales (eventos, feriados).

El flujo fue:

```
trips + zones (LEFT JOIN) â†’ trips_with_zones
trips_with_zones + calendar (LEFT JOIN) â†’ trips_complete
```

Luego realicÃ© un **anÃ¡lisis exploratorio enriquecido** para responder preguntas de negocio:

- Â¿QuÃ© borough concentra mÃ¡s viajes? â†’ **Manhattan (88%)**
- Â¿DÃ³nde son mÃ¡s largos los viajes? â†’ **Queens**
- Â¿CuÃ¡les son las horas pico? â†’ **18:00 (215K viajes)**
- Â¿Hay correlaciÃ³n entre tarifa y propina? â†’ **SÃ­, fuerte (r=0.71)**

### Â¿QuÃ© aprendÃ­?

1. **Joins robustos**: Entender que LEFT JOIN mantiene todos los registros de la tabla izquierda, mientras que INNER JOIN solo conserva matches. La elecciÃ³n depende del anÃ¡lisis que querÃ©s hacer.

2. **VerificaciÃ³n post-join**: Siempre chequear cuÃ¡ntos registros se perdieron o cuÃ¡ntos quedaron sin match. En este caso, **1,647 viajes** no tenÃ­an borough asignado (location_id problemÃ¡tico: 265).

3. **Enriquecimiento contextual**: Integrar mÃºltiples fuentes permite anÃ¡lisis mÃ¡s profundos. Sin la tabla `zones`, solo tendrÃ­a IDs numÃ©ricos sin significado. Con ella, puedo agrupar por borough y extraer insights de negocio.

4. **DocumentaciÃ³n del proceso**: RegistrÃ© cada paso (carga, limpieza, joins, verificaciÃ³n) con prints informativos y comentarios. Esto hace que el notebook sea **reproducible** y fÃ¡cil de auditar.

### Â¿QuÃ© mejorarÃ­a?

1. **Pipeline con Prefect**: La prÃ¡ctica incluÃ­a implementar un pipeline orquestado con Prefect (que quedÃ³ pendiente). Esto automatizarÃ­a el flujo y permitirÃ­a scheduling para datasets que se actualizan periÃ³dicamente.

2. **ValidaciÃ³n de calidad automatizada**: Implementar tests automÃ¡ticos (ej.: "borough no puede ser nulo", "trip_distance >= 0") con bibliotecas como `great_expectations`.

3. **AnÃ¡lisis temporal mÃ¡s profundo**: El dataset de calendar no tenÃ­a dÃ­as especiales en el perÃ­odo analizado. Con datos reales (ej.: impacto de Navidad, Super Bowl), podrÃ­a medir efectos en demanda y tarifas.

4. **Visualizaciones interactivas**: Los grÃ¡ficos estÃ¡ticos (matplotlib/seaborn) son buenos para reportes, pero para exploraciÃ³n serÃ­a Ãºtil usar **Plotly** (zoom, hover, filtros dinÃ¡micos).

---

## Â¿En quÃ© tipo de proyecto real usarÃ­a esto?

Estos conceptos de EDA & Fuentes son **transversales** a casi cualquier proyecto de datos. Algunos ejemplos concretos donde lo aplicarÃ­a:

### 1. **E-commerce: OptimizaciÃ³n de inventario**

**Problema**: Una tienda online quiere entender quÃ© productos vender mÃ¡s y cuÃ¡ndo reponerlos.

**AplicaciÃ³n UT1**:

- **Fuentes a integrar**: Ventas (transacciones), Inventario (stock actual), Clima (API externa), Calendario (temporadas/feriados).
- **EDA clave**: Detectar productos con ventas estacionales, identificar outliers (promociones puntuales), correlacionar clima con categorÃ­as (ej.: paraguas en dÃ­as lluviosos).
- **Joins**: `ventas + productos` (LEFT JOIN para mantener todos los productos, incluso los sin ventas) + `ventas + calendario` (marcar feriados/eventos).
- **Insights accionables**: "Restock de ropa de invierno 2 semanas antes del frÃ­o", "Promociones en productos de baja rotaciÃ³n".

---

### 2. **Salud pÃºblica: AnÃ¡lisis de mortalidad infantil**

**Problema**: Entender factores asociados a mortalidad infantil en distintos barrios de una ciudad.

**AplicaciÃ³n UT1**:

- **Fuentes a integrar**: Registros de nacimientos/defunciones (CSV), Datos socioeconÃ³micos por barrio (censo), Infraestructura sanitaria (geolocalizaciÃ³n de hospitales).
- **EDA clave**: Identificar barrios con mayor tasa de mortalidad, detectar missing data en variables crÃ­ticas (peso al nacer, atenciÃ³n prenatal), explorar correlaciones con Ã­ndices de pobreza.
- **Joins**: `defunciones + censo` (por cÃ³digo postal o barrio) + `defunciones + hospitales` (distancia al centro de salud mÃ¡s cercano).
- **Insights accionables**: "Barrios X e Y requieren campaÃ±as de salud preventiva", "Falta de atenciÃ³n prenatal es el factor mÃ¡s correlacionado".

---

### 3. **LogÃ­stica: OptimizaciÃ³n de rutas de delivery**

**Problema**: Una empresa de delivery quiere reducir tiempos de entrega y costos operativos.

**AplicaciÃ³n UT1**:

- **Fuentes a integrar**: Ã“rdenes (timestamps, ubicaciones), TrÃ¡fico (API de Google Maps), Clima (lluvia afecta tiempos), Zonas (barrios, distritos).
- **EDA clave**: Detectar horas pico por zona, identificar rutas problemÃ¡ticas (outliers en tiempo), explorar correlaciÃ³n entre clima y demoras.
- **Joins**: `ordenes + zonas` (enriquecer con info geogrÃ¡fica) + `ordenes + trafico` (por timestamp y ubicaciÃ³n).
- **Insights accionables**: "Evitar ruta X entre 18-19h", "Asignar mÃ¡s repartidores en zona Y los viernes".

---

### 4. **Finanzas: DetecciÃ³n de fraude en transacciones**

**Problema**: Un banco quiere identificar transacciones sospechosas en tiempo real.

**AplicaciÃ³n UT1**:

- **Fuentes a integrar**: Transacciones (monto, ubicaciÃ³n, timestamp), Perfil de cliente (historial, lÃ­mites), GeolocalizaciÃ³n (paÃ­s, ciudad).
- **EDA clave**: Detectar outliers en montos/frecuencia, identificar patrones inusuales (ej.: compras en mÃºltiples paÃ­ses en pocas horas), explorar correlaciones entre variables (monto vs distancia del domicilio).
- **Joins**: `transacciones + clientes` (historial de comportamiento) + `transacciones + geolocalizaciÃ³n` (verificar coherencia).
- **Insights accionables**: "Bloquear automÃ¡ticamente transacciones > 3 desviaciones estÃ¡ndar del promedio del cliente", "Alertas para compras en paÃ­ses distintos en < 2 horas".

---

## ConclusiÃ³n

La **UT1** me dio las herramientas fundamentales para trabajar con datos reales: cargar, explorar, limpiar, integrar y visualizar. Pero mÃ¡s allÃ¡ de las tÃ©cnicas especÃ­ficas, me enseÃ±Ã³ una **mentalidad crÃ­tica**: nunca confiar ciegamente en los datos, siempre documentar decisiones, y entender que el EDA no es un paso previo al "trabajo real", *es parte esencial del trabajo*.

Los ejercicios de Iris, Netflix y NYC Taxis me permitieron practicar desde lo bÃ¡sico (chequeos de calidad) hasta lo complejo (integraciÃ³n de 3M de registros con mÃºltiples fuentes). Ahora me siento cÃ³modo enfrentando datasets nuevos, y sÃ© que lo primero que harÃ© serÃ¡:

1. âœ… **Cargar y explorar**: `.shape`, `.info()`, `.describe()`
2. âœ… **Chequear calidad**: missing, duplicados, outliers
3. âœ… **Integrar fuentes**: elegir joins inteligentes
4. âœ… **Visualizar con propÃ³sito**: grÃ¡ficos que cuenten historias
5. âœ… **Documentar todo**: para que sea reproducible

Este es el **cimiento** sobre el que construirÃ© las prÃ³ximas unidades (limpieza avanzada, feature engineering, modelado). Y estoy seguro de que estos fundamentos los aplicarÃ© en cualquier proyecto de datos que enfrente, dentro o fuera de la universidad.

---

## ğŸ“š Referencias

- Brust, A. V. (2023). *Ciencia de Datos para Gente Sociable â€“ CapÃ­tulos 1â€“4*. https://bitsandbricks.github.io/ciencia_de_datos_gente_sociable/
- Google. *Good Data Analysis (IntroducciÃ³n y Mindset; Technical)*. https://developers.google.com/machine-learning/guides/good-data-analysis
- Kaggle. *Pandas: Creating, Reading and Writing; Indexing, Selecting & Assigning; Summary Functions and Maps; Grouping and Sorting*
- DocumentaciÃ³n oficial: `pandas`, `matplotlib`, `seaborn`, `MkDocs`

---

