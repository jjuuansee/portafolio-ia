---
title: "Resumen Reflexivo UT3 ‚Äî Feature Engineering"
date: 2025-12-02
author: "Juan Paroli"
---

# ‚öôÔ∏è Reflexi√≥n sobre UT3: Feature Engineering

## ¬øDe qu√© trat√≥ esta unidad y qu√© problemas buscaba resolver?

La **UT3** abord√≥ uno de los aspectos m√°s creativos e impactantes de la ciencia de datos: **crear informaci√≥n a partir de informaci√≥n**. El Feature Engineering es el proceso de transformar variables crudas en representaciones que capturan mejor los patrones subyacentes del problema.

El problema central fue: **¬øc√≥mo maximizar el poder predictivo de los datos sin agregar m√°s datos?** La respuesta: crear features derivadas, aplicar transformaciones inteligentes, reducir dimensionalidad cuando hay redundancia, y extraer patrones temporales cuando el tiempo importa.

En mis palabras, esta unidad me ense√±√≥ que **los datos cuentan historias ocultas** y que un buen cient√≠fico de datos sabe **hacer las preguntas correctas** para revelarlas. No se trata de aplicar todas las transformaciones posibles, sino de entender el dominio y crear features que **tengan sentido**.

---

## Conceptos y t√©cnicas clave que incorpor√©

### 1. **Features Derivadas con Sentido de Negocio**

El Feature Engineering m√°s efectivo combina **t√©cnicas matem√°ticas** con **conocimiento del dominio**. Las features derivadas deben **contar una historia** sobre los datos.

**Ejemplo del portafolio**: En la [pr√°ctica de Feature Engineering](../ejercicios/ut3-feature-engineering/feature_engineering.md), cre√© features para precios de viviendas:

**Features Ratio:**
```python
# Precio por pie cuadrado: valor relativo del espacio
df['price_per_sqft'] = df['price'] / df['sqft']

# Densidad de construcci√≥n: proporci√≥n construida del terreno
df['construction_density'] = df['sqft'] / df['lot_size']

# Equilibrio ubicaci√≥n-educaci√≥n
df['city_school_ratio'] = df['distance_to_city'] / df['school_rating']
```

**Features Compuestas:**
```python
# √çndice de lujo
df['luxury_score'] = df['price'] + df['sqft'] + df['garage_spaces']

# Score de ubicaci√≥n
df['location_score'] = df['school_rating'] - df['distance_to_city']
```

**Resultado en datos reales (Ames Housing):**

| Feature | Correlaci√≥n con precio |
|---------|------------------------|
| `space_efficiency` (GrLivArea/LotArea) | **0.875** |
| `property_age` (2025 - YearBuilt) | **-0.822** |
| `bath_per_bedroom` | **0.658** |

Las features con **mayor correlaci√≥n** fueron las que combinan estructura f√≠sica con temporalidad, confirmando que el Feature Engineering efectivo requiere entender el dominio.

---

### 2. **Encoding de Variables Categ√≥ricas seg√∫n Cardinalidad**

No todas las variables categ√≥ricas deben tratarse igual. La **cardinalidad** (n√∫mero de categor√≠as √∫nicas) determina la mejor estrategia:

**Ejemplo del portafolio**: En la [pr√°ctica de Target Encoding](../ejercicios/ut3-encoding/encoding.md), compar√© t√©cnicas con el dataset Adult Income:

| Variable | Cardinalidad | Estrategia recomendada |
|----------|--------------|------------------------|
| `sex` | 2 | One-Hot Encoding |
| `workclass` | 9 | One-Hot Encoding |
| `native-country` | 42 | **Target Encoding** |

**Problema de dimensionalidad con One-Hot:**
```
8 variables categ√≥ricas ‚Üí 94 columnas adicionales
```

**Resultados comparativos:**

| M√©todo | Accuracy | AUC-ROC | F1-Score | Tiempo |
|--------|----------|---------|----------|--------|
| Label Encoding | **0.8632** | **0.9101** | 0.6931 | 0.77s |
| One-Hot (baja cardinalidad) | 0.8483 | 0.8995 | 0.6633 | 0.67s |
| Target Encoding (alta cardinalidad) | 0.8092 | 0.8318 | 0.5658 | 1.63s |
| Pipeline con branching | 0.8488 | 0.9021 | 0.6671 | 2.08s |

**Hallazgo clave**: Label Encoding fue el mejor porque Random Forest **no asume relaciones lineales** entre categor√≠as. Para modelos lineales, Target Encoding ser√≠a m√°s importante.

---

### 3. **PCA y Feature Selection: Interpretabilidad vs Compresi√≥n**

La reducci√≥n de dimensionalidad tiene dos enfoques muy diferentes:

- **PCA**: Comprime informaci√≥n pero **pierde interpretabilidad** (los componentes son combinaciones lineales abstractas).
- **Feature Selection**: Mantiene features originales con **interpretabilidad completa**.

**Ejemplo del portafolio**: En la [pr√°ctica de PCA](../ejercicios/ut3-pca/pca.md), compar√© ambos enfoques en Ames Housing (81 features):

**Varianza explicada por PCA:**
- 38 componentes ‚Üí 80% varianza (53% reducci√≥n)
- 51 componentes ‚Üí 90% varianza
- 59 componentes ‚Üí 95% varianza

**Comparaci√≥n de m√©todos (38 features seleccionadas):**

| M√©todo | RMSE | R¬≤ | ¬øInterpretable? |
|--------|------|-----|-----------------|
| **Original** | $26,342 | 0.8885 | S√≠ |
| **Lasso** | **$26,090** | **0.8908** | S√≠ |
| **RF Importance** | $26,238 | 0.8894 | S√≠ |
| **Mutual Information** | $26,279 | 0.8891 | S√≠ |
| **PCA Componentes** | $26,620 | 0.8859 | **No** |
| **PCA Loadings** | $27,020 | 0.8830 | S√≠ |

**Conclusi√≥n**: Feature Selection (especialmente Lasso) obtuvo **mejor performance** que PCA manteniendo **interpretabilidad**. En bienes ra√≠ces, poder decir "GrLivArea es importante" es m√°s valioso que "PC1 influye".

---

### 4. **Temporal Feature Engineering: Capturando el Ritmo del Usuario**

Cuando los datos tienen componente temporal, las features cl√°sicas no capturan la **din√°mica del comportamiento**. Necesitamos features que representen **cu√°ndo** y **con qu√© frecuencia**.

**Ejemplo del portafolio**: En la [pr√°ctica de Temporal FE](../ejercicios/ut3-temporal-feature-engineering/temporal_fe.md), cre√© 37 features temporales para predecir recompra en e-commerce:

**Categor√≠as de features temporales:**

1. **Lag Features** (comportamiento reciente):
```python
orders_df['days_since_prior_lag_1'] = (
    orders_df.groupby('user_id')['days_since_prior_order']
    .shift(1)  # CR√çTICO: shift(1) previene leakage
)
```

2. **Rolling Windows** (tendencia):
```python
orders_df['rolling_cart_mean_3'] = (
    orders_df.groupby('user_id')['cart_size']
    .shift(1).rolling(window=3, min_periods=1).mean()
)
```

3. **RFM Analysis** (Recency, Frequency, Monetary):
```python
# Recency: d√≠as desde √∫ltima compra
# Frequency: total de √≥rdenes
# Monetary: gasto promedio y total
```

4. **Calendar Features** con encoding c√≠clico:
```python
# hour_sin, hour_cos: las 23h est√°n "cerca" de las 0h
df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)
df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)
```

**Impacto en performance:**

| Modelo | Features | AUC |
|--------|----------|-----|
| Base (sin temporal) | 7 | 0.6615 |
| **Full (con temporal)** | 37 | **0.7276** |
| **Mejora** | | **+10%** |

**Top features por importancia:**

| Rank | Feature | Importancia | Categor√≠a |
|------|---------|-------------|-----------|
| 1 | `product_diversity_ratio` | 0.1039 | Diversity |
| 2 | `recency_days` | 0.0833 | RFM |
| 3 | `unique_products` | 0.0635 | Diversity |
| 4 | `spend_90d` | 0.0564 | Time Window |
| 5 | `days_since_prior_lag_3` | 0.0483 | Lag/Window |

**Insight clave**: Las features de **Lag/Window** concentraron 29% de la importancia total, confirmando que el comportamiento hist√≥rico es el mejor predictor de comportamiento futuro.

---

## ¬øQu√© fue lo que m√°s me cost√≥ y c√≥mo lo destrab√©?

Lo que m√°s me cost√≥ fue **prevenir data leakage en features temporales**. Es muy f√°cil crear features que "miren al futuro" sin darte cuenta.

### El problema

Cuando calculas una media m√≥vil o un promedio por usuario, la operaci√≥n por defecto **incluye el registro actual**. Pero si usas esa feature para predecir algo sobre ese mismo registro, est√°s usando informaci√≥n que no tendr√≠as en producci√≥n.

```python
# ‚ùå LEAKAGE: incluye el valor actual
df['avg_cart'] = df.groupby('user_id')['cart_size'].transform('mean')

# ‚ùå LEAKAGE: rolling sin shift incluye el valor actual
df['rolling_mean'] = df.groupby('user_id')['cart_size'].rolling(3).mean()
```

### C√≥mo lo destrab√©

1. **Regla de oro**: Siempre usar `.shift(1)` antes de cualquier agregaci√≥n:

```python
# ‚úÖ CORRECTO: excluye el valor actual
df['rolling_mean'] = (
    df.groupby('user_id')['cart_size']
    .shift(1)  # PRIMERO shift
    .rolling(3, min_periods=1).mean()  # LUEGO rolling
)
```

2. **TimeSeriesSplit en validaci√≥n**: Nunca usar KFold regular para datos temporales:

```python
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=3)
for train_idx, val_idx in tscv.split(X):
    # train siempre antes de val temporalmente
```

3. **Checklist de verificaci√≥n**:
   - ‚úÖ Todas las aggregations usan `shift(1)` antes
   - ‚úÖ Rolling windows con `closed='left'`
   - ‚úÖ Solo `ffill()`, nunca `bfill()` (forward fill, no backward)
   - ‚úÖ Val dates > Train dates en todos los folds

**Lecci√≥n clave**: El data leakage temporal es **silencioso**. El modelo funciona "bien" en validaci√≥n pero falla en producci√≥n. La √∫nica forma de detectarlo es ser **paranoico** con cada feature.

---

## Una tarea en detalle: PCA y Feature Selection en Ames Housing

### ¬øQu√© hice?

En la [pr√°ctica de PCA](../ejercicios/ut3-pca/pca.md), implement√© y compar√© m√∫ltiples t√©cnicas de reducci√≥n de dimensionalidad:

1. **PCA completo**: An√°lisis de varianza explicada por componentes
2. **Feature Selection por PCA Loadings**: Seleccionar features originales con mayor peso en PCs
3. **M√©todos Filter**: F-test (ANOVA) y Mutual Information
4. **M√©todos Wrapper**: Forward Selection, Backward Elimination, RFE
5. **M√©todos Embedded**: Random Forest Importance y Lasso

### ¬øQu√© aprend√≠?

1. **PCA no es siempre la respuesta**: PC1 explica solo 13.4% de la varianza. En datos tabulares con muchas features heterog√©neas, PCA pierde mucha informaci√≥n.

2. **Los m√©todos se complementan**: Mutual Information captura relaciones no lineales que F-test ignora. Random Forest captura interacciones que los m√©todos univariados no ven.

3. **Consistencia es se√±al**: 15 features aparecieron en m√∫ltiples m√©todos (`Overall Qual`, `Gr Liv Area`, `Total Bsmt SF`...). Estas son las m√°s robustas.

4. **Lasso es poderoso para feature selection**: Al forzar coeficientes a cero, identifica autom√°ticamente features redundantes. De 81 features, Lasso mantuvo solo 41 con coeficiente no-cero.

5. **Trade-off tiempo-performance**: Forward/Backward Selection tomaron 163-251 segundos vs 7 segundos de RFE, con performance similar. RFE es m√°s pr√°ctico.

### ¬øQu√© mejorar√≠a?

1. **An√°lisis de estabilidad**: Correr feature selection con diferentes seeds/folds para ver qu√© features son consistentes.

2. **Interacciones autom√°ticas**: Usar `PolynomialFeatures` para generar interacciones y luego seleccionar las mejores con Lasso.

3. **Visualizaci√≥n de componentes**: Crear biplots de PCA para entender qu√© features contribuyen a cada componente.

4. **Comparaci√≥n con modelos no lineales**: Ver si las features seleccionadas funcionan igual de bien con XGBoost o LightGBM.

---

## ¬øEn qu√© tipo de proyecto real usar√≠a esto?

### 1. **E-commerce: Sistema de recomendaci√≥n**

**Problema**: Recomendar productos a usuarios bas√°ndose en historial de compras.

**Aplicaci√≥n UT3**:

- **Temporal FE**: Lag features de productos vistos/comprados, rolling windows de categor√≠as favoritas, RFM por usuario.
- **Encoding**: Target Encoding para productos de alta cardinalidad (miles de SKUs).
- **Feature Selection**: Reducir dimensionalidad de embeddings de productos.

**Features clave**: `days_since_last_purchase`, `category_diversity`, `recency_score`, `avg_basket_value`.

---

### 2. **Finanzas: Detecci√≥n de fraude en tiempo real**

**Problema**: Identificar transacciones fraudulentas en milisegundos.

**Aplicaci√≥n UT3**:

- **Temporal FE**: Velocidad de transacciones (transacciones/hora), distancia temporal desde √∫ltima transacci√≥n, rolling sum de montos.
- **Feature Engineering**: Ratios como `monto_actual / monto_promedio_usuario`, `distancia_desde_ubicaci√≥n_habitual`.
- **PCA**: Reducir features para inferencia r√°pida.

**Features clave**: `tx_per_hour`, `amount_vs_avg_ratio`, `distance_from_home`, `time_since_last_tx`.

---

### 3. **Marketing: Predicci√≥n de churn**

**Problema**: Identificar clientes en riesgo de abandonar antes de que lo hagan.

**Aplicaci√≥n UT3**:

- **RFM**: Recency (cu√°ndo fue su √∫ltima interacci√≥n), Frequency (cu√°ntas veces interact√∫a), Monetary (cu√°nto gasta).
- **Temporal FE**: Trend features (¬øest√° disminuyendo su engagement?), estacionalidad.
- **Encoding**: One-Hot para plan/producto, Target Encoding para regi√≥n.

**Features clave**: `days_since_last_login`, `engagement_trend_30d`, `support_tickets_90d`, `renewal_probability`.

---

### 4. **Salud: Predicci√≥n de riesgo cardiovascular**

**Problema**: Calcular probabilidad de evento cardiovascular en 10 a√±os.

**Aplicaci√≥n UT3**:

- **Feature Engineering**: Ratios como `colesterol_total / HDL`, `BMI = peso / altura¬≤`, `presi√≥n_pulse = sist√≥lica - diast√≥lica`.
- **Transformaciones**: Log de triglic√©ridos (distribuci√≥n sesgada), bins de edad.
- **Feature Selection**: Identificar factores de riesgo m√°s predictivos con Lasso.

**Features clave**: `cholesterol_ratio`, `pulse_pressure`, `metabolic_syndrome_score`, `family_history_flag`.

---

## Conclusi√≥n

La **UT3** me ense√±√≥ que el Feature Engineering es donde la **creatividad** se encuentra con la **ciencia**. No se trata de aplicar transformaciones mec√°nicamente, sino de **entender el problema** y **crear representaciones que capturen su esencia**.

Los cuatro pilares de esta unidad fueron:

1. ‚úÖ **Features con sentido de negocio**: Ratios, scores compuestos, interacciones que reflejan relaciones reales.
2. ‚úÖ **Encoding seg√∫n cardinalidad**: One-Hot para pocas categor√≠as, Target Encoding para muchas.
3. ‚úÖ **Reducci√≥n inteligente**: Preferir Feature Selection sobre PCA cuando la interpretabilidad importa.
4. ‚úÖ **Temporal FE sin leakage**: Shift antes de rolling, TimeSeriesSplit en validaci√≥n.

El impacto fue tangible: las temporal features mejoraron el modelo de predicci√≥n de recompra en **+10% AUC**. Eso es la diferencia entre un modelo mediocre y uno √∫til.

Ahora, cada vez que enfrento un problema predictivo, mi proceso incluye:

1. ‚úÖ Entender el dominio y qu√© relaciones tienen sentido
2. ‚úÖ Crear features derivadas (ratios, transformaciones, scores)
3. ‚úÖ Elegir encoding seg√∫n cardinalidad
4. ‚úÖ Si hay tiempo: extraer features temporales con cuidado de leakage
5. ‚úÖ Seleccionar features con m√©todos complementarios

---

## üìö Referencias

- Kaggle Course: *Feature Engineering* - https://www.kaggle.com/learn/feature-engineering
- Scikit-learn: *Feature Selection* - https://scikit-learn.org/stable/modules/feature_selection.html
- Category Encoders Documentation - https://contrib.scikit-learn.org/category_encoders/
- Pandas: *Time Series / Shift / Diff* - https://pandas.pydata.org/docs/user_guide/timeseries.html
- Feature Engineering for Machine Learning - O'Reilly

---

