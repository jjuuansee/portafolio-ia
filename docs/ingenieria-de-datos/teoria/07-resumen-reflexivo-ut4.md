---
title: "Resumen Reflexivo UT4 ‚Äî Datos No Estructurados"
date: 2025-12-02
author: "Juan Paroli"
---

# üéµ Reflexi√≥n sobre UT4: Datos No Estructurados (Audio)

## ¬øDe qu√© trat√≥ esta unidad y qu√© problemas buscaba resolver?

La **UT4** expandi√≥ el concepto de "datos" m√°s all√° de las tablas estructuradas. Los datos del mundo real vienen en muchas formas: **im√°genes**, **audio**, **texto**, **video**. Cada modalidad requiere t√©cnicas espec√≠ficas de preprocesamiento y extracci√≥n de features antes de poder aplicar modelos de ML.

El problema central fue: **¬øc√≥mo convertir se√±ales crudas (ondas de audio, p√≠xeles de imagen) en representaciones num√©ricas que un modelo pueda entender?** Esto implica entender la f√≠sica de la se√±al, aplicar transformaciones matem√°ticas (Fourier, Mel), y extraer features que capturen las caracter√≠sticas relevantes para la tarea.

En mis palabras, esta unidad me ense√±√≥ que **los datos no estructurados son estructurados de otra forma**. Una onda de audio no es "caos"; es una se√±al con frecuencias, amplitudes y patrones temporales. El trabajo del cient√≠fico de datos es **revelar esa estructura oculta**.

---

## Conceptos y t√©cnicas clave que incorpor√©

### 1. **Preprocesamiento de Audio: Estandarizaci√≥n de Se√±ales**

Antes de extraer features, los audios deben tener **formato consistente**. Esto implica normalizar sample rate, duraci√≥n, canales y amplitud.

**Ejemplo del portafolio**: En la [pr√°ctica de Audio](../ejercicios/ut4-audio_as_data/14-audio.ipynb), trabaj√© con el dataset **UrbanSound8K** (8,732 clips de sonidos urbanos):

**Pipeline de preprocesamiento:**

```python
TARGET_SR = 16000          # Hz (suficiente para voz/sonidos urbanos)
TARGET_DURATION = 3.0      # segundos
TARGET_AMPLITUDE = 0.99    # normalizaci√≥n de pico

def preprocess_audio(path, target_sr, target_duration, top_db=30):
    # 1. Cargar audio
    y, sr = librosa.load(path, sr=None, mono=False)
    
    # 2. Convertir a mono (si es est√©reo)
    if y.ndim > 1:
        y = np.mean(y, axis=0)
    
    # 3. Recortar silencios
    y_trim, _ = librosa.effects.trim(y, top_db=top_db)
    
    # 4. Resamplear a sample rate objetivo
    y_rs = librosa.resample(y_trim, orig_sr=sr, target_sr=target_sr)
    
    # 5. Ajustar duraci√≥n (pad o truncate)
    target_len = int(target_sr * target_duration)
    if len(y_rs) > target_len:
        y_rs = y_rs[:target_len]
    else:
        y_rs = np.pad(y_rs, (0, target_len - len(y_rs)))
    
    # 6. Normalizar amplitud
    max_abs = np.max(np.abs(y_rs)) or 1.0
    y_norm = (TARGET_AMPLITUDE * y_rs) / max_abs
    
    return y_norm.astype(np.float32), target_sr
```

**Justificaci√≥n de decisiones:**

| Par√°metro | Valor | Raz√≥n |
|-----------|-------|-------|
| `TARGET_SR = 16000` | 16 kHz | Suficiente para voz/sonidos urbanos (Nyquist: hasta 8 kHz). Reduce tama√±o vs 44.1 kHz. |
| `TARGET_DURATION = 3.0` | 3 segundos | Captura eventos urbanos t√≠picos sin exceso de padding. |
| `top_db = 30` | 30 dB | Elimina silencios/ruido de fondo sin perder se√±al √∫til. |
| `mono = True` | Mono | Reduce complejidad; informaci√≥n est√©reo no es relevante para clasificaci√≥n. |

**Resultado:**
- **Antes**: 192,000 muestras @ 48 kHz, 4.0s, amplitud [-0.85, 0.81]
- **Despu√©s**: 48,000 muestras @ 16 kHz, 3.0s, amplitud [-0.99, 0.93]

---

### 2. **Representaciones Espectrales: De Tiempo a Frecuencia**

El waveform (amplitud vs tiempo) es √∫til pero no captura la **composici√≥n frecuencial** del sonido. Para eso, usamos transformaciones espectrales.

**Short-Time Fourier Transform (STFT):**

La STFT divide la se√±al en ventanas y aplica la transformada de Fourier a cada una, generando un **espectrograma** (frecuencia vs tiempo).

```python
n_fft = 2048      # tama√±o de ventana FFT
hop_length = 512  # avance entre ventanas

D = librosa.stft(y, n_fft=n_fft, hop_length=hop_length)
S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)
```

**Trade-off resoluci√≥n:**
- `n_fft` grande ‚Üí mejor resoluci√≥n en frecuencia, peor en tiempo
- `n_fft` peque√±o ‚Üí mejor resoluci√≥n en tiempo, peor en frecuencia

**Espectrograma de Mel:**

El o√≠do humano no percibe frecuencias linealmente; es m√°s sensible a cambios en bajas frecuencias. La escala Mel ajusta las bandas de frecuencia para **imitar la percepci√≥n humana**.

```python
mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)
mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)
```

**Aplicaci√≥n**: Los espectrogramas Mel son el input est√°ndar para redes neuronales de audio (CNN para clasificaci√≥n de sonidos).

---

### 3. **MFCCs: Features Compactas para ML Cl√°sico**

Los **Mel-Frequency Cepstral Coefficients (MFCCs)** comprimen la informaci√≥n espectral en ~13-20 coeficientes que representan la "forma" del espectro.

**Extracci√≥n de features:**

```python
def extract_mfcc_features(y, sr, n_mfcc=13):
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    feats = {}
    
    # Estad√≠sticas por coeficiente
    for i in range(n_mfcc):
        feats[f"mfcc_{i+1}_mean"] = float(np.mean(mfcc[i, :]))
        feats[f"mfcc_{i+1}_std"] = float(np.std(mfcc[i, :]))
    
    # Features adicionales
    feats["rms_mean"] = float(np.mean(librosa.feature.rms(y=y)))
    feats["zcr_mean"] = float(np.mean(librosa.feature.zero_crossing_rate(y=y)))
    
    return feats  # 28 features total
```

**Interpretaci√≥n de MFCCs:**

| Coeficiente | Qu√© captura |
|-------------|-------------|
| `mfcc_1` | Energ√≠a global (brillo/volumen) |
| `mfcc_2-4` | Forma general del espectro (timbre) |
| `mfcc_5+` | Detalles finos (textura) |

**Features extra√≠das para 100 clips:**
- Shape final: **(100, 31)** ‚Üí 28 features MFCC + 3 metadatos (filename, sr, duration)
- Listas para entrenar modelos cl√°sicos (RandomForest, SVM, etc.)

---

### 4. **M√©tricas Espectrales Din√°micas**

M√°s all√° de los MFCCs, existen features que describen **c√≥mo cambia el espectro en el tiempo**:

```python
# Centroide espectral: "centro de masa" de las frecuencias
cent = librosa.feature.spectral_centroid(y=y, sr=sr)[0]

# Rolloff: frecuencia bajo la cual est√° el 85% de la energ√≠a
rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr, roll_percent=0.85)[0]

# Bandwidth: dispersi√≥n del espectro alrededor del centroide
bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]
```

**Aplicaci√≥n**: Estas m√©tricas permiten distinguir entre sonidos "brillantes" (alto centroide: sirenas) y "oscuros" (bajo centroide: motor).

---

## ¬øQu√© fue lo que m√°s me cost√≥ y c√≥mo lo destrab√©?

Lo que m√°s me cost√≥ fue **entender la relaci√≥n entre sample rate, frecuencias y el teorema de Nyquist**.

### El problema

Al principio no entend√≠a por qu√© 16 kHz era "suficiente" para sonidos urbanos. ¬øNo estamos perdiendo informaci√≥n al reducir desde 44.1 kHz o 48 kHz?

### C√≥mo lo destrab√©

1. **Teorema de Nyquist**: Para capturar una frecuencia `f`, necesitas un sample rate de al menos `2f`. Con 16 kHz, puedo capturar hasta 8 kHz.

2. **Rangos de frecuencia relevantes**:
   - Voz humana: 85 Hz - 8 kHz
   - Sirenas: 500 Hz - 3 kHz
   - Ladridos: 200 Hz - 4 kHz
   - Motor de auto: 20 Hz - 1 kHz

3. **Conclusi√≥n**: La mayor√≠a de sonidos urbanos est√°n **por debajo de 8 kHz**. Reducir a 16 kHz:
   - ‚úÖ Mantiene toda la informaci√≥n relevante
   - ‚úÖ Reduce tama√±o de datos en ~3x (vs 44.1 kHz)
   - ‚úÖ Reduce tiempo de procesamiento

4. **Verificaci√≥n pr√°ctica**: Compar√© espectrogramas a 48 kHz y 16 kHz. Las frecuencias por encima de 8 kHz eran mayormente ruido de fondo, no informaci√≥n √∫til.

**Lecci√≥n clave**: Entender la f√≠sica de la se√±al permite tomar decisiones informadas sobre preprocesamiento, en lugar de usar valores "por defecto".

---

## Una tarea en detalle: Pipeline de Audio para UrbanSound8K

### ¬øQu√© hice?

Desarroll√© un pipeline completo para procesar audio y extraer features:

1. **Descarga del dataset** desde Kaggle (5.6 GB, 8,732 clips)
2. **Exploraci√≥n inicial**: An√°lisis de waveforms, espectrogramas, caracter√≠sticas
3. **Pipeline de preprocesamiento**: Estandarizaci√≥n de formato
4. **Extracci√≥n de features**: MFCCs + m√©tricas espectrales
5. **Exportaci√≥n**: CSV con 28 features por clip

### ¬øQu√© aprend√≠?

1. **El preprocesamiento es cr√≠tico**: Sin estandarizaci√≥n, los clips tienen duraciones de 0.5s a 4s, sample rates de 22 kHz a 96 kHz. Esto har√≠a imposible entrenar un modelo consistente.

2. **El recorte de silencios (`trim`) mejora la calidad**: Eliminar silencios al inicio/final concentra la informaci√≥n √∫til y mejora el ratio se√±al-ruido efectivo.

3. **MFCCs son poderosos pero no suficientes**: Para tareas complejas, combinar MFCCs con espectrogramas Mel (para CNNs) da mejores resultados.

4. **Augmentation es factible y √∫til**: `pitch_shift` y `time_stretch` generan variantes del audio para aumentar datos de entrenamiento:

```python
# Pitch shift: subir 2 semitonos
y_pitch = librosa.effects.pitch_shift(y, sr=sr, n_steps=2)

# Time stretch: hacer 10% m√°s lento
y_slow = librosa.effects.time_stretch(y, rate=0.9)
```

5. **Los metadatos del dataset son valiosos**: UrbanSound8K incluye folds predefinidos para cross-validation, evitando que clips del mismo evento (grabados segundos despu√©s) aparezcan en train y test.

### ¬øQu√© mejorar√≠a?

1. **Extraer m√°s features**: A√±adir spectral contrast, spectral flatness, tonnetz (para audio musical), chroma features.

2. **Clasificaci√≥n end-to-end**: Entrenar una CNN sobre espectrogramas Mel en lugar de features manuales.

3. **Data augmentation sistem√°tica**: Crear versiones augmentadas de todos los clips (pitch shift, time stretch, a√±adir ruido) y evaluar impacto en accuracy.

4. **An√°lisis por clase**: Ver si diferentes clases de sonido (sirena vs ladrido) tienen diferentes distribuciones de features.

5. **Pipeline en producci√≥n**: Crear una funci√≥n que tome un archivo de audio crudo y retorne la predicci√≥n de clase en tiempo real.

---

## ¬øEn qu√© tipo de proyecto real usar√≠a esto?

### 1. **Seguridad: Detecci√≥n de disparos/vidrios rotos**

**Problema**: Sistema de vigilancia que detecta sonidos de emergencia en tiempo real.

**Aplicaci√≥n UT4**:

- **Preprocesamiento**: Stream de audio continuo, ventanas de 2-3 segundos con overlap.
- **Features**: MFCCs + zero crossing rate (disparos tienen ZCR alto).
- **Modelo**: CNN sobre espectrogramas Mel para detecci√≥n robusta.
- **Latencia cr√≠tica**: Procesamiento en menos de 100ms.

---

### 2. **Salud: An√°lisis de tos para detecci√≥n de COVID**

**Problema**: Clasificar grabaciones de tos para screening de enfermedades respiratorias.

**Aplicaci√≥n UT4**:

- **Preprocesamiento**: Normalizaci√≥n, detecci√≥n de eventos de tos en grabaciones largas.
- **Features**: MFCCs (capturan timbre de la tos), spectral centroid, duraci√≥n del evento.
- **Modelo**: Random Forest o CNN sobre espectrogramas.
- **Consideraciones √©ticas**: Privacidad de audio m√©dico, sesgo por dispositivo de grabaci√≥n.

---

### 3. **Automotive: Diagn√≥stico de motor por sonido**

**Problema**: Detectar fallas mec√°nicas bas√°ndose en el sonido del motor.

**Aplicaci√≥n UT4**:

- **Preprocesamiento**: Filtrado de ruido de viento/camino, normalizaci√≥n por RPM.
- **Features**: Spectral centroid (frecuencia dominante), harmonic-to-noise ratio, anomal√≠as en patrones peri√≥dicos.
- **Modelo**: Autoencoder para detecci√≥n de anomal√≠as (sonidos "anormales" vs "normales").
- **Deployment**: Edge computing en el veh√≠culo.

---

### 4. **Medio ambiente: Monitoreo de biodiversidad**

**Problema**: Identificar especies de aves/mam√≠feros en grabaciones de campo.

**Aplicaci√≥n UT4**:

- **Preprocesamiento**: Segmentaci√≥n de vocalizaciones, separaci√≥n de fuentes.
- **Features**: MFCCs + chroma (para cantos de aves con estructura mel√≥dica).
- **Modelo**: CNN + attention para secuencias largas con m√∫ltiples eventos.
- **Escala**: Miles de horas de grabaci√≥n, procesamiento batch.

---

### 5. **Accesibilidad: Transcripci√≥n de audio para sordos**

**Problema**: Generar subt√≠tulos en tiempo real de sonidos no verbales (aplausos, m√∫sica, sirenas).

**Aplicaci√≥n UT4**:

- **Preprocesamiento**: VAD (Voice Activity Detection) para separar voz de otros sonidos.
- **Features**: MFCCs para clasificaci√≥n de sonidos ambientales.
- **Modelo**: Multi-label classification (puede haber m√∫ltiples sonidos simult√°neos).
- **Output**: "[aplausos]", "[m√∫sica dram√°tica]", "[sirena de ambulancia]".

---

## Conclusi√≥n

La **UT4** expandi√≥ mi perspectiva sobre qu√© significa "datos". M√°s all√° de las tablas CSV, el mundo genera **se√±ales continuas** que pueden analizarse con las t√©cnicas correctas.

Los pilares de esta unidad fueron:

1. ‚úÖ **Estandarizaci√≥n de se√±ales**: Sample rate, duraci√≥n, canales, amplitud consistentes.
2. ‚úÖ **Representaciones espectrales**: Transformar de tiempo a frecuencia (STFT, Mel).
3. ‚úÖ **Extracci√≥n de features**: MFCCs y m√©tricas espectrales para ML cl√°sico.
4. ‚úÖ **Entender la f√≠sica**: Nyquist, escalas de frecuencia, percepci√≥n humana.

Aunque esta unidad se enfoc√≥ en audio, los principios aplican a cualquier dato no estructurado:

- **Im√°genes**: Estandarizar resoluci√≥n, extraer features (HOG, SIFT) o usar CNNs.
- **Texto**: Tokenizaci√≥n, embeddings (Word2Vec, BERT).
- **Video**: Combinar t√©cnicas de imagen + audio + series temporales.

El mensaje central es que **los datos no estructurados son estructurados de otra forma**. Nuestro trabajo es encontrar esa estructura y representarla de manera que los modelos puedan aprenderla.

---

## üìö Referencias

- Librosa Documentation: https://librosa.org/doc/latest/
- UrbanSound8K Dataset: https://www.kaggle.com/datasets/chrisfilo/urbansound8k
- McFee, B., et al. (2015). *librosa: Audio and Music Signal Analysis in Python*.
- Stevens, S. S., Volkmann, J., & Newman, E. B. (1937). *A Scale for the Measurement of the Psychological Magnitude Pitch*. (Escala Mel)
- Scikit-learn: Audio feature extraction

---

