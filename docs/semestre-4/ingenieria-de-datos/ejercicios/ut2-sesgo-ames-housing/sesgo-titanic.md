---
title: "Pr√°ctica 7 ‚Äî Detecci√≥n y mitigaci√≥n de sesgo con Fairlearn"
date: 2025-10-12
author: "Juan Paroli"
---

# ‚öñÔ∏è Detecci√≥n y mitigaci√≥n de sesgo: construyendo modelos √©ticos con Fairlearn

## Contexto

Esta pr√°ctica aborda c√≥mo **detectar** y **mitigar** sesgos en modelos de *ML* usando `fairlearn`. Se trabajaron dos casos complementarios:

1. **Boston Housing (regresi√≥n)** ‚Äî ejemplo hist√≥rico con una **variable racial** (`B`) problem√°tica que codifica indirectamente la proporci√≥n de poblaci√≥n afroamericana.
2. **Titanic (clasificaci√≥n)** ‚Äî sesgos por **g√©nero** y **clase** (paridad demogr√°fica).

> **Objetivo**: evaluar m√©tricas de equidad, visualizar brechas y aplicar mitigaci√≥n con ExponentiatedGradient bajo la restricci√≥n de Demographic Parity, discutiendo los trade-offs √©ticos y de performance.

Esta pr√°ctica fue desarrollada en un notebook de jupyter que puedes encontrar [aqu√≠](practica7.ipynb)

---

## Objetivos

- [x] Identificar variables sensibles y cuantificar su relaci√≥n con el target.
- [x] Medir **brechas** de resultados entre grupos y evaluar **paridad demogr√°fica**.
- [x] Aplicar **mitigaci√≥n** (ExponentiatedGradient + DemographicParity) y comparar con baseline.
- [x] Elaborar un **marco de decisi√≥n √©tica** para uso responsable en producci√≥n.

---

## Desarrollo

### 1. Boston Housing ‚Äî Sesgo racial hist√≥rico (Regresi√≥n)

**Setup**
- Dataset: CMU `boston` (506 filas).
- Variable problem√°tica: `B` (1978) codifica de forma indirecta la **proporci√≥n de poblaci√≥n afroamericana**.
- Librer√≠as: `pandas`, `numpy`, `scikit-learn`, `fairlearn`, `matplotlib`, `seaborn`.

**Proceso**
- An√°lisis de correlaci√≥n entre `B` y el target (`MEDV`).
- Modelado con y sin la variable `B`.
- Medici√≥n de brechas de precios por grupo.
- Visualizaci√≥n de distribuciones.

**Resultados clave**

- Correlaci√≥n entre `B` y `MEDV`: **0.333**.
- Modelo lineal **con** `B`: **R¬≤ = 0.7112** (mejora predictiva **pero** con riesgo de perpetuar sesgo).
- **Brecha de precios** por grupo (media):
  - Alta_prop_afroam: **$22.81k**
  - Baja_prop_afroam: **$22.25k**
  - Diferencia: **‚àí$0.56k (‚àí2.4%)**

*(En este corte particular no hay brecha a favor del grupo hist√≥ricamente desfavorecido, pero el uso de `B` sigue siendo √©ticamente problem√°tico)*

![](results/distribucion_de_precios_segun_raza.png)

**An√°lisis √©tico**

- `B` es una variable **hist√≥ricamente sesgada**.
- **NO** se debe usar en producci√≥n; **s√≠** en **√°mbitos educativos** para estudiar sesgo.
- **Alternativas**: retirar `B`, documentar limitaciones, buscar **features menos problem√°ticas** (`LSTAT`, `RM`, `CRIM`, `TAX`, `PTRATIO` mostraron correlaciones relevantes sin codificaci√≥n racial expl√≠cita).

### 2. Titanic ‚Äî Paridad demogr√°fica (Clasificaci√≥n)

**Setup**
- Dataset: Titanic (pasajeros del hundimiento).
- Features: `pclass`, `age`, `sibsp`, `parch`, `fare`; target `survived`; atributo sensible `sex`.
- Librer√≠as: `pandas`, `scikit-learn`, `fairlearn`, `seaborn`.

**Proceso**
- An√°lisis de sesgos en el dataset.
- Entrenamiento de baseline (RandomForest).
- Aplicaci√≥n de mitigaci√≥n con ExponentiatedGradient.
- Comparaci√≥n de m√©tricas de equidad y performance.

**Resultados clave**

**Detecci√≥n de sesgo (dataset)**
- **Gender gap** (tasa de supervivencia): **+54.8%** a favor de mujeres.
- **Class gap**: **+41.3%** a favor de pasajeros de 1ra vs 3ra.

**Baseline (RandomForest)**
- **Accuracy**: **0.673**
- **Demographic Parity Difference (DPD)**: **0.113**

**Mitigaci√≥n (ExponentiatedGradient + DemographicParity)**
- **Accuracy**: **0.617**
- **DPD**: **0.035**
- **Trade-off**:
  - *Performance loss*: **8.3%**
  - *Fairness gain*: **0.079**

**Recomendaci√≥n**

- **Evaluar caso por caso.** La mejora de equidad es clara, pero la ca√≠da en accuracy supera el 5% en este setting. En dominios sensibles (p.ej., salud/finanzas) podr√≠a justificarse; en otros, debe manejarse con criterios de **riesgo**, **impacto** y **aceptaci√≥n regulatoria**.

---

## üìÅ Evidencias

### Boston Housing: Distribuci√≥n de precios seg√∫n raza

**Visualizaci√≥n de brechas de precios**

![](results/distribucion_de_precios_segun_raza.png)

### C√≥digo de ejemplo: Mitigaci√≥n de sesgo

```python
from fairlearn.reductions import ExponentiatedGradient, DemographicParity
from sklearn.ensemble import RandomForestClassifier

# Baseline
model_baseline = RandomForestClassifier()
model_baseline.fit(X_train, y_train)

# Mitigaci√≥n con ExponentiatedGradient
constraint = DemographicParity()
mitigator = ExponentiatedGradient(
    model_baseline,
    constraint,
    eps=0.01
)
mitigator.fit(X_train, y_train, sensitive_features=sensitive_features_train)

# Evaluaci√≥n
predictions_mitigated = mitigator.predict(X_test)
dpd = demographic_parity_difference(y_test, predictions_mitigated, 
                                   sensitive_features=sensitive_features_test)
```

---

## üí° Reflexi√≥n

### Aprendizajes clave

- **Detecci√≥n vs. Correcci√≥n**: Detectar es crucial cuando el sesgo es **hist√≥rico/estructural** (Boston): hace falta **transparencia** y **trazabilidad** antes de cualquier correcci√≥n. Corregir (Titanic) conlleva **trade-offs**: se gana en equidad (baja DPD) bajando la exactitud.
- **Transparencia vs. Utilidad**: Documentar **qu√© m√©trica de equidad** se impone y **qu√© se sacrifica** (accuracy, recall, etc.). Preferir **modelos explicables**.
- **Variables sensibles**: Variables como `B` en Boston Housing son **hist√≥ricamente problem√°ticas** y no deben usarse en producci√≥n, aunque mejoren la performance del modelo.

### Limitaciones y desaf√≠os

- **Trade-offs entre equidad y performance**: La mitigaci√≥n de sesgo siempre implica sacrificar algo de performance. El umbral aceptable depende del **dominio** y del **da√±o potencial**.
- **M√©tricas de equidad m√∫ltiples**: No existe una √∫nica m√©trica de equidad. Demographic Parity, Equalized Odds, y otras m√©tricas pueden entrar en conflicto.
- **Contexto espec√≠fico**: Las decisiones √©ticas deben evaluarse **caso por caso**, considerando el dominio de aplicaci√≥n y el impacto social.

### Pr√≥ximos pasos

- Explorar otras m√©tricas de equidad (Equalized Odds, Calibrated Equalized Odds).
- Implementar monitoreo continuo de sesgos en producci√≥n.
- Desarrollar procesos de auditor√≠a √©tica para modelos en producci√≥n.

!!! warning "Atenci√≥n"
    La responsabilidad √©tica en ML es fundamental. **Reconocer** sesgos no corregibles y **no reforzarlos**. **Evitar** variables sensibles salvo fines educativos o investigaci√≥n controlada. **Reportar** sistem√°ticamente las m√©tricas de equidad junto con las de performance.

---

## üìö Referencias

- **Fairlearn Documentation**: *Metrics (DP, EO) & Reductions (ExponentiatedGradient, DemographicParity)*.
  [https://fairlearn.org/](https://fairlearn.org/)

- **Lineamientos de Responsible/Trustworthy AI**: Auditor√≠a, documentaci√≥n, monitoring.
  [https://www.microsoft.com/en-us/ai/responsible-ai](https://www.microsoft.com/en-us/ai/responsible-ai)

- **Notas hist√≥ricas sobre Boston Housing**: Controversias del feature `B` y su contexto hist√≥rico.

- **Kaggle**: *Titanic Dataset* (Seaborn/Kaggle): an√°lisis cl√°sico de sesgos por g√©nero/clase.
  [https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic)

- **Notebook completo**: [practica7.ipynb](practica7.ipynb)

---
