---
title: "Pr√°ctica 15 ‚Äî Pipelines ETL, DataOps y Orquestaci√≥n con Prefect"
date: 2025-12-08
author: "Juan Paroli"
---

# üîÑ Orquestando pipelines de datos: del c√≥digo al flujo con Prefect

## Contexto

En esta pr√°ctica trabajamos con **Prefect**, una herramienta moderna de orquestaci√≥n de pipelines de datos. Dise√±amos e implementamos un **pipeline ETL completo** para procesar datos de ventas de un e-commerce, aplicando principios de **DataOps** como observabilidad, reproducibilidad y CI/CD para datos.

Prefect permite definir pipelines como c√≥digo Python puro, detectando autom√°ticamente dependencias entre tasks y construyendo DAGs impl√≠citos. Esto simplifica significativamente la orquestaci√≥n comparado con alternativas como Apache Airflow.

> Este ejercicio fue desarrollado en un notebook de Jupyter que puedes encontrar [aqu√≠](./quince.ipynb).

## üéØ Objetivos

- [x] Comprender los conceptos fundamentales de Prefect: Tasks, Flows, estados, y caching
- [x] Investigar y aplicar funcionalidades avanzadas: retries, logging estructurado, validaci√≥n
- [x] Dise√±ar e implementar un pipeline ETL completo con Prefect
- [x] Explorar deployments y scheduling para ejecuci√≥n programada
- [x] Conectar Prefect con principios de DataOps: observabilidad, reproducibilidad, CI/CD
- [x] Comparar Prefect con alternativas como Apache Airflow y Dagster

## Desarrollo

### 1. Investigaci√≥n: Conceptos Fundamentales

#### Tasks en Prefect

Una **Task** es una unidad de trabajo individual decorada con `@task`. Seg√∫n la [documentaci√≥n oficial](https://docs.prefect.io/latest/concepts/tasks/), es *"a discrete piece of work that can be tracked and retried independently"*.

**Evaluaci√≥n diferida ("lazy evaluation")**: Las tasks no se ejecutan inmediatamente cuando se llaman. Prefect construye primero el grafo de dependencias y luego ejecuta en el orden correcto.

**Estados de Tasks**:
| Estado | Descripci√≥n |
|--------|-------------|
| **PENDING** | Esperando ejecuci√≥n |
| **RUNNING** | En ejecuci√≥n activa |
| **COMPLETED** | Ejecutada exitosamente |
| **FAILED** | Fall√≥ durante ejecuci√≥n |
| **RETRYING** | Siendo reintentada |

**Par√°metros importantes del decorador `@task`**:
- `retries`: N√∫mero de reintentos si falla
- `retry_delay_seconds`: Tiempo entre reintentos
- `cache_expiration`: Duraci√≥n de validez del cach√©
- `tags`: Etiquetas para organizaci√≥n
- `log_prints`: Captura prints como logs

#### Flows en Prefect

Un **Flow** es un contenedor que orquesta m√∫ltiples Tasks. La diferencia clave:
- **Task**: Unidad b√°sica de trabajo
- **Flow**: Orquesta y coordina tasks, maneja dependencias

**DAGs impl√≠citos**: Prefect detecta autom√°ticamente las dependencias cuando pasas el resultado de una task como par√°metro de otra, construyendo el grafo sin configuraci√≥n expl√≠cita.

### 2. Dise√±o del Pipeline

**Escenario**: Ventas de un e-commerce con datos de transacciones diarias

| Rol | Descripci√≥n |
|-----|-------------|
| **Business data owner** | Equipo de ventas que genera transacciones |
| **Data engineers** | Equipo que construye y mantiene el pipeline ETL |
| **Data consumers** | Analistas y dashboards que consumen datos procesados |

**Tipo de pipeline**: **Batch** ‚Äî Las ventas se procesan diariamente en lotes, facilitando validaci√≥n y manejo de errores.

### 3. Implementaci√≥n del Pipeline ETL

Implementamos tres tasks principales con el decorador `@task`:

**Task 1: Extract** ‚Äî Extrae 100 registros de ventas simulados
```python
@task(tags=["extract", "data-source"], log_prints=True)
def extract_data():
    # Simula extracci√≥n de datos de ventas
    data = {
        'fecha': pd.date_range(start='2024-01-01', periods=100, freq='D'),
        'producto': np.random.choice(['A', 'B', 'C', 'D'], 100),
        'cantidad': np.random.randint(1, 50, 100),
        'precio_unitario': np.random.uniform(10, 100, 100).round(2),
        'region': np.random.choice(['Norte', 'Sur', 'Este', 'Oeste'], 100)
    }
    return pd.DataFrame(data)
```

**Task 2: Transform** ‚Äî Calcula totales y categoriza tickets
```python
@task(tags=["transform", "data-processing"], log_prints=True)
def transform_data(df: pd.DataFrame) -> pd.DataFrame:
    df['total'] = df['cantidad'] * df['precio_unitario']
    df['ticket_size'] = pd.cut(df['total'], 
        bins=[0, 100, 500, 1000, float('inf')],
        labels=['Bajo', 'Medio', 'Alto', 'Muy Alto'])
    return df
```

**Task 3: Load** ‚Äî Guarda en CSV con retries autom√°ticos
```python
@task(tags=["load", "data-output"], log_prints=True, retries=2, retry_delay_seconds=3)
def load_data(df: pd.DataFrame, output_path: str = "ventas_procesadas.csv") -> str:
    df.to_csv(output_path, index=False)
    return output_path
```

**Flow Principal**:
```python
@flow(name="ETL Pipeline Ventas", log_prints=True)
def etl_flow():
    df_raw = extract_data()
    df_transformed = transform_data(df_raw)
    output_file = load_data(df_transformed)
    return output_file
```

### 4. Funcionalidades Avanzadas

| Funcionalidad | Descripci√≥n | Implementaci√≥n |
|---------------|-------------|----------------|
| **Retries** | Reintentos autom√°ticos ante fallos | `@task(retries=2, retry_delay_seconds=3)` |
| **Caching** | Evita re-ejecuci√≥n con mismos par√°metros | `@task(cache_expiration=timedelta(hours=1))` |
| **Logging** | Captura autom√°tica de prints | `@task(log_prints=True)` |
| **Concurrencia** | Ejecuci√≥n paralela de tasks independientes | `ConcurrentTaskRunner()` |

### 5. Deployments y Scheduling

**Deployment**: Configuraci√≥n que permite ejecutar un Flow de manera programada o bajo demanda.

**Tipos de schedules**:
- **CronSchedule**: `cron="0 6 * * *"` ‚Äî Todos los d√≠as a las 6 AM
- **IntervalSchedule**: `interval=timedelta(hours=1)` ‚Äî Cada hora
- **RRuleSchedule**: Reglas RFC 5545 para horarios complejos

### 6. Extensi√≥n DataOps: Validaci√≥n con Logging

Implementamos una task de validaci√≥n que verifica calidad de datos:

```python
@task(retries=1, retry_delay_seconds=2, tags=["validation", "data-quality"])
def validate_data(df: pd.DataFrame) -> pd.DataFrame:
    logger = get_run_logger()
    errors = []
    
    # Validaci√≥n 1: DataFrame no vac√≠o
    if len(df) <= 0:
        errors.append("DataFrame vac√≠o")
    
    # Validaci√≥n 2: Columnas requeridas
    required = ['fecha', 'producto', 'cantidad', 'precio_unitario', 'total']
    missing = [col for col in required if col not in df.columns]
    if missing:
        errors.append(f"Columnas faltantes: {missing}")
    
    if errors:
        raise ValueError(f"Validaci√≥n fallida: {errors}")
    
    logger.info("‚úÖ Validaci√≥n exitosa")
    return df
```

## üìÅ Evidencias

### Ejecuci√≥n del Pipeline ETL

```
17:19:25.183 | INFO | Flow run 'witty-platypus' - Beginning flow run for flow 'ETL Pipeline Ventas'
17:19:25.189 | INFO | Flow run 'witty-platypus' - üöÄ Iniciando pipeline ETL...
17:19:26.301 | INFO | Task run 'extract_data-f23' - üì• Extra√≠dos 100 registros
17:19:27.578 | INFO | Task run 'extract_data-f23' - Finished in state Completed()
17:19:28.829 | INFO | Task run 'transform_data-87c' - üîÑ Transformados 100 registros
17:19:28.836 | INFO | Task run 'transform_data-87c' -    Total vendido: $136,630.33
17:19:30.236 | INFO | Task run 'transform_data-87c' - Finished in state Completed()
17:19:31.797 | INFO | Task run 'load_data-cd4' - üíæ Datos cargados en: ventas_procesadas.csv
17:19:31.810 | INFO | Task run 'load_data-cd4' -    Registros guardados: 100
17:19:33.019 | INFO | Task run 'load_data-cd4' - Finished in state Completed()
17:19:33.028 | INFO | Flow run 'witty-platypus' - ‚úÖ Pipeline completado. Archivo generado: ventas_procesadas.csv
17:19:33.108 | INFO | Flow run 'witty-platypus' - Finished in state Completed()

üéâ Flow ejecutado exitosamente. Resultado: ventas_procesadas.csv
```

### Ejecuci√≥n con Validaci√≥n (detectando errores)

La validaci√≥n detecta correctamente cuando faltan columnas requeridas:

```
17:21:48.699 | INFO  | Task run 'validate_data-885' - Iniciando validaci√≥n de datos
17:21:48.706 | INFO  | Task run 'validate_data-885' - DataFrame recibido con 100 registros y 5 columnas
17:21:48.712 | INFO  | Task run 'validate_data-885' - ‚úÖ Validaci√≥n de cantidad de registros: OK (100 registros)
17:21:48.727 | INFO  | Task run 'validate_data-885' - ‚úÖ Validaci√≥n de valores nulos: OK (sin nulos)
17:21:48.733 | ERROR | Task run 'validate_data-885' - Columnas requeridas faltantes: ['total']
17:21:48.742 | ERROR | Task run 'validate_data-885' - Validaci√≥n fallida con 1 error(es) cr√≠tico(s)
17:21:48.751 | INFO  | Task run 'validate_data-885' - Task run failed - Retry 1/1 will start 2 second(s) from now
```

> **Nota**: El error de columna faltante `'total'` es esperado, ya que la columna se crea en `transform_data`, pero la validaci√≥n se ejecuta antes de la transformaci√≥n. Esto demuestra que la validaci√≥n funciona correctamente detectando problemas de calidad de datos.

## üí° Reflexi√≥n

### Conexi√≥n con DataOps

**1. Observabilidad**: Prefect proporciona:
- Logging estructurado autom√°tico por cada task
- Estados claros (PENDING, RUNNING, COMPLETED, FAILED)
- UI centralizada para monitoreo en tiempo real
- M√©tricas de tiempo de ejecuci√≥n y tasa de √©xito

**2. Reproducibilidad**: El caching garantiza:
- Resultados consistentes con las mismas entradas
- Historial de ejecuciones para reproducir estados anteriores
- Puntos de recuperaci√≥n si un pipeline falla

**3. CI/CD para datos**: Los Deployments permiten:
- Versionado de flows
- Diferentes ambientes (dev, staging, producci√≥n)
- Rollback r√°pido cambiando versi√≥n activa
- Integraci√≥n con GitHub Actions y GitLab CI

### Comparaci√≥n con Alternativas

| Aspecto | Prefect | Apache Airflow | Dagster |
|---------|---------|----------------|---------|
| **DAGs** | Impl√≠citos (autom√°ticos) | Expl√≠citos (manual) | Impl√≠citos |
| **Curva de aprendizaje** | Suave | Empinada | Moderada |
| **Orientaci√≥n** | Workflow-centric | DAG-based | Asset-centric |
| **Python nativo** | ‚úÖ S√≠ | Parcial | ‚úÖ S√≠ |

### Desaf√≠os Encontrados

1. **DAGs Impl√≠citos**: Al principio confuso, pero luego muy intuitivo ‚Äî solo pasas resultados como par√°metros.
2. **Validaci√≥n con Logging**: Importante usar diferentes niveles (info, warning, error) para logs informativos pero concisos.
3. **Deployments vs Flows**: El Flow es c√≥digo Python, el Deployment es la "instalaci√≥n" con configuraci√≥n espec√≠fica.

## üìö Referencias

- [Documentaci√≥n oficial de Prefect](https://docs.prefect.io/)
- [Prefect Concepts Overview](https://docs.prefect.io/latest/concepts/)
- [Prefect Tasks Documentation](https://docs.prefect.io/latest/concepts/tasks/)
- [Prefect Flows Documentation](https://docs.prefect.io/latest/concepts/flows/)
- [Prefect Caching Documentation](https://docs.prefect.io/latest/concepts/tasks/#caching)
- [Prefect Deployments Documentation](https://docs.prefect.io/latest/concepts/deployments/)

---

*"La mejor forma de aprender una herramienta es leer su documentaci√≥n oficial. Los tutoriales te dan el 'qu√©', la documentaci√≥n te da el 'por qu√©' y el 'c√≥mo'."*
